{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hao-191/hello-hao/blob/master/Lyrics_(nophoneme_Pretrained_Bert2GPT2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tso4V8uOT6ex"
      },
      "source": [
        "#Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPBd3_sfDafs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd9b59c-6cce-472d-d248-fb4a19535018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ019z34Dup2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b30ffc2-a5a7-4daf-86f6-c34d03a5e532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: HOME=/content/drive/My Drive/\n",
            "/content/drive/My Drive\n",
            "/content/drive/My Drive/Courses/nlp_spring2022/final\n"
          ]
        }
      ],
      "source": [
        "%env HOME=/content/drive/My Drive/\n",
        "%cd ~/\n",
        "!mkdir -p Courses/nlp_spring2022/final/checkpoints\n",
        "%cd Courses/nlp_spring2022/final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0_PV7bAGw5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c97c372-155d-4968-c517-4fe11faa5c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 16 14:28:34 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BObK9waB2zWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ef4340-9d2b-41d6-eae6-3cf36ccec439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-2.2.1-py3-none-any.whl (342 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Collecting keybert\n",
            "  Using cached keybert-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Using cached fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Collecting aiohttp\n",
            "  Using cached aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19\n",
            "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Using cached xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.44.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.0.2)\n",
            "Collecting rich>=10.4.0\n",
            "  Using cached rich-12.4.1-py3-none-any.whl (231 kB)\n",
            "Collecting sentence-transformers>=0.3.8\n",
            "  Using cached sentence_transformers-2.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.11.0+cu113)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 29.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.12.0+cu113)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Using cached tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "Collecting pyyaml\n",
            "  Using cached PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2019.12.20)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Using cached frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Using cached yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Using cached multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Using cached asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Installing collected packages: urllib3, pyyaml, multidict, frozenlist, yarl, tokenizers, huggingface-hub, asynctest, async-timeout, aiosignal, transformers, sentencepiece, fsspec, commonmark, aiohttp, xxhash, sentence-transformers, rich, responses, keybert, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 commonmark-0.9.1 datasets-2.2.1 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.6.0 keybert-0.5.1 multidict-6.0.2 pyyaml-6.0 responses-0.18.0 rich-12.4.1 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.19.1 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets nltk tensorboard keybert\n",
        "\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tugNYKn4bQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb134433-63a9-4d95-a5fe-b67cb6e480ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 16 14:29:35 UTC 2022\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import math\n",
        "!date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUgbE7eVgDDH"
      },
      "source": [
        "#GPU State\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hrjEAcxrRIs",
        "outputId": "f44d02df-d2b5-4534-a71e-880993bdd91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |    Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total    {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "printm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYa6VsEUrRnL",
        "outputId": "4749b94e-4bf3-4ce8-cf7d-036f1ebacba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen RAM Free: 12.3 GB  |    Proc size: 331.5 MB\n",
            "GPU RAM Free: 15109MB | Used: 0MB | Util   0% | Total    15109MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1UyZ9u9xUZg"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TBPPgJzaij5"
      },
      "source": [
        "# Data Pre-processing\n",
        "\n",
        "This part is used to pre-process the rap lyric texts that we downloaded from Kaggle. \n",
        "\n",
        "I am going to re-organize the texts into CSV files containing features of (artist_name, this_line,next_line, phonemes, keywords), and split them into train, dev, and test sets with a proportion of (7:1.5:1.5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmA3kRAhTRpN"
      },
      "source": [
        "##Make .csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P14HVwudD6JQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from keybert import KeyBERT\n",
        "\n",
        "nltk.download('cmudict')\n",
        "kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
        "\n",
        "'''\n",
        "    Raw lyrics files downloaded from\n",
        "    https://www.kaggle.com/datasets/rikdifos/rap-lyrics\n",
        "    Shout out to the dataset builder\n",
        "    Removed Tupac1_lyrics.txt and Tupac2_lyrics.txt and The Notorious Big_lyrics.txt\n",
        "'''\n",
        "\n",
        "# use cmudict to get the last 2 vowels of the sentence\n",
        "def get_last_vowels(content,limit=2):\n",
        "    l = []\n",
        "    all_vowels = {\"AA\":0,\"AE\":1,\"AH\":2,\"AO\":3,\"AW\":4,\"AY\":5,\"EH\":6,\"ER\":7,\"EY\":8,\"IH\":9,\"IY\":10,\"OW\":11,\"OY\":12,\"UH\":13,\"UW\":14,\"[PAD]\":15}\n",
        "    dic = nltk.corpus.cmudict.dict()\n",
        "    for line in content:\n",
        "        if line != '[PAD]':\n",
        "            vowels = []\n",
        "            words = line.split()[-5:]\n",
        "            for i in range(len(words)-1,-1,-1):\n",
        "                if len(vowels)>limit-1:\n",
        "                    break\n",
        "                try:\n",
        "                    word = words[i].lower()\n",
        "                    count = 0\n",
        "                    while word not in dic.keys() and count < 2:\n",
        "                        count += 1\n",
        "                        word = word[:-1]\n",
        "                    sounds = dic[word]\n",
        "                    sounds = sounds[0]\n",
        "                    for j in range(len(sounds)-1,-1,-1):\n",
        "                        if len(vowels)>limit-1:\n",
        "                            break\n",
        "                        if sounds[j] in all_vowels.keys():\n",
        "                            vowels.append(sounds[j])\n",
        "                        elif sounds[j][:-1] in all_vowels.keys():\n",
        "                            vowels.append(sounds[j][:-1])\n",
        "                except:\n",
        "                    continue\n",
        "            while len(vowels) < limit:\n",
        "                vowels.append(\"[PAD]\")\n",
        "\n",
        "\n",
        "\n",
        "        else:\n",
        "            vowels = [\"[PAD]\"]*limit\n",
        "\n",
        "\n",
        "        vowels.reverse()\n",
        "        vowels_str = ','.join(vowels)\n",
        "        l.append(vowels_str)\n",
        "\n",
        "    return l\n",
        "\n",
        "##Use KeyBERT model to extract 3 keywords from a verse\n",
        "def get_keywords(content,model,n = 10):\n",
        "    kwds = []\n",
        "    this_verse = ''\n",
        "    new_verse = False\n",
        "    line_count = 0\n",
        "    for i in range(len(content)-1):\n",
        "        this_line = content[i]\n",
        "        next_line = content[i+1]\n",
        "        if (this_line=='[PAD]') and (next_line!=\"[PAD]\"):\n",
        "            new_verse = True\n",
        "            this_verse = ''\n",
        "            line_count = 0\n",
        "            kwds += [\"[PAD]\"]\n",
        "        elif (this_line == '[PAD]') and (next_line=='[PAD]'):\n",
        "            this_verse = ''\n",
        "            line_count = 0\n",
        "            kwds += [\"[PAD]\"]\n",
        "        elif (this_line!='[PAD]') and (next_line=='[PAD]'):\n",
        "            new_verse = False\n",
        "            this_verse += this_line\n",
        "            line_count += 1\n",
        "            kwd = model.extract_keywords(this_verse,keyphrase_ngram_range=(1,1),stop_words='english',highlight=False,top_n=n)\n",
        "            kwd_l = [item[0] for item in kwd if item[0] not in [\"nigga\",\"niggas\",\"bitch\",\"fuck\",\"pussy\",\"bitches\",\"bitchin\"]][:3]\n",
        "            kwd_s = \",\".join(kwd_l)\n",
        "            if len(kwd_s) < 3:\n",
        "                kwd_s = \"[PAD]\"\n",
        "            kwds += [kwd_s for i in range(line_count)]\n",
        "        elif (this_line!='[PAD]') and (next_line != '[PAD]'):\n",
        "            this_verse += this_line\n",
        "            line_count += 1\n",
        "\n",
        "    kwds += [\"[PAD]\"]\n",
        "    return kwds\n",
        "\n",
        "# convert all .txt files to a csv file with columns (this_line, next_line)\n",
        "def create_csv_file(kw_model):\n",
        "    \n",
        "\n",
        "    data = {}\n",
        "    frames = []\n",
        "    phonemes = []\n",
        "    files = []\n",
        "    for (dirpath, dirnames, filenames) in os.walk('./data/lyrics/'):\n",
        "        files.extend(filenames)\n",
        "\n",
        "    for filename in files:\n",
        "        artist_name = filename[:-11]\n",
        "        print(\"Starting with \" + artist_name + \"...\")\n",
        "        if filename[-4:] == '.txt':\n",
        "            with open(\"./data/lyrics/\"+filename,'r') as f:\n",
        "                content = f.read().splitlines()\n",
        "\n",
        "                ##Clean the \"(\" \")\"...\n",
        "                for i in range(len(content)-1,-1,-1):\n",
        "                    no_meaning = False\n",
        "                    seq = content[i]\n",
        "                    seq = seq.split()\n",
        "                    for j in range(len(seq)):\n",
        "                        if (\"(\" in seq[j]) or (\")\" in seq[j]):\n",
        "                            no_meaning = True\n",
        "                    if no_meaning:\n",
        "                        content.pop(i)\n",
        "\n",
        "\n",
        "                for i in range(len(content)-1,-1,-1):\n",
        "                    if len(content[i].split()) <= 5 and content[i]:\n",
        "                        content.pop(i)\n",
        "                \n",
        "                for i in range(len(content)):\n",
        "                    if content[i] == '':\n",
        "                        content[i] = '[PAD]'\n",
        "                content.append('[PAD]')\n",
        "\n",
        "                ##Getting the phonemetic features for last word of each line\n",
        "                phonemes = get_last_vowels(content)\n",
        "            \n",
        "                ##Getting keyword for each verse. All lines in the same verse will receive same keywords\n",
        "                keywords = get_keywords(content,kw_model)\n",
        "\n",
        "\n",
        "                this_line_cleaned = content[:-1]\n",
        "                next_line_cleaned = content[1:]\n",
        "                phonemes_cleaned = phonemes[:-1]\n",
        "                keyword_cleaned = keywords[:-1]\n",
        "\n",
        "                ##Remove the lines that is empty\n",
        "                for i in range(len(this_line_cleaned)-1,-1,-1):\n",
        "                  if this_line_cleaned[i] == '[PAD]' or next_line_cleaned[i] == '[PAD]':\n",
        "                    this_line_cleaned.pop(i)\n",
        "                    next_line_cleaned.pop(i)\n",
        "                    phonemes_cleaned.pop(i)\n",
        "                    keyword_cleaned.pop(i)\n",
        "\n",
        "\n",
        "                ##Clean Repetitive lytics \n",
        "                for i in range(len(this_line_cleaned)-1,-1,-1):\n",
        "                  if this_line_cleaned[i].split()[:5] == next_line_cleaned[i].split()[:5]:\n",
        "                    this_line_cleaned.pop(i)\n",
        "                    next_line_cleaned.pop(i)\n",
        "                    phonemes_cleaned.pop(i)\n",
        "                    keyword_cleaned.pop(i)\n",
        "\n",
        "                data['artist_name'] = artist_name\n",
        "                data['this_line'] = this_line_cleaned\n",
        "                data['next_line'] = next_line_cleaned\n",
        "                data['phonemes'] = phonemes_cleaned\n",
        "                data['keywords'] = keyword_cleaned\n",
        "\n",
        "           \n",
        "\n",
        "                df_temp = pd.DataFrame(data)\n",
        "                frames.append(df_temp)\n",
        "    df = pd.concat(frames)\n",
        "    df.to_csv(\"./data/data_csv/lyrics_ult.csv\")\n",
        "    # print(df.head()['this_line'])\n",
        "    #Splitting the dataframes into train, dev and test (7:1.5:1.5)\n",
        "    train,test_and_dev = train_test_split(df,test_size = 0.3)\n",
        "    dev,test = train_test_split(test_and_dev,test_size = 0.5)\n",
        "\n",
        "    print(train.size,dev.size,test.size)\n",
        "    #Save them to csv\n",
        "    train.to_csv(\"./data/data_csv/train_ult.csv\")\n",
        "    dev.to_csv(\"./data/data_csv/valid_ult.csv\")\n",
        "    test.to_csv(\"./data/data_csv/test_ult.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdJVj8qtEGS4"
      },
      "outputs": [],
      "source": [
        "#create_csv_file(kw_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox-VqvnIy_3x"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XCMMAgSzBLZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "gradient_accumulation_steps = 1\n",
        "learning_rate = 6.25e-5\n",
        "adam_epsilon = 1e-8\n",
        "max_grad_norm = 1.0\n",
        "num_train_epochs = 10\n",
        "warmup_steps = 0\n",
        "\n",
        "seed = 42\n",
        "batch_size = 8\n",
        "max_seq_length = 384\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sentence_encoder_model_name = \"bert-base-uncased\"\n",
        "keyword_encoder_model_name = \"bert-base-uncased\"\n",
        "decoder_model_name = \"distilgpt2\"\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "ph_dic = {\"AA\":0,\"AE\":1,\"AH\":2,\"AO\":3,\"AW\":4,\"AY\":5,\"EH\":6,\"ER\":7,\"EY\":8,\"IH\":9,\"IY\":10,\"OW\":11,\"OY\":12,\"UH\":13,\"UW\":14,\"[PAD]\":15}\n",
        "def ph_decode(l,ph_dic):\n",
        "  l2 = []\n",
        "  for i in l:\n",
        "    for k,v in ph_dic.items():\n",
        "      if v == i:\n",
        "        l2.append(k)\n",
        "        break\n",
        "  return \",\".join(l2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRSdGz_7BfzW"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "Like in Question 1, we will use the GPT2 tokenizer to tokenize text. Any new word not in the GPT2 vocabulary will be split into subwords automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVorlf8P5jki"
      },
      "source": [
        "### Config for Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFGjoqdkIogK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "\n",
        "sentence_encoder_config = AutoConfig.from_pretrained(sentence_encoder_model_name)\n",
        "keyword_encoder_config = AutoConfig.from_pretrained(keyword_encoder_model_name)\n",
        "decoder_config = AutoConfig.from_pretrained(decoder_model_name)\n",
        "\n",
        "# sentence_encoder_config.vocab_size = 50257\n",
        "# keyword_encoder_config.vocab_size = 50257\n",
        "# decoder_config.vocab_size = 50257\n",
        "decoder_config.layer_norm_eps = 1e-12\n",
        "decoder_config.hidden_dropout_prob = 0.1\n",
        "decoder_config.attention_probs_dropout_prob = 0.1\n",
        "decoder_config.intermediate_size = 768\n",
        "decoder_config.hidden_act = \"gelu\"\n",
        "decoder_config.pad_token_id=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVwSyi-F8mDm"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentence_encoder_tokenizer = AutoTokenizer.from_pretrained(sentence_encoder_model_name)\n",
        "keyword_encoder_tokenizer = AutoTokenizer.from_pretrained(keyword_encoder_model_name)\n",
        "decoder_tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n",
        "\n",
        "\n",
        "# sentence_encoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# sentence_encoder_tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n",
        "# #sentence_encoder_tokenizer.add_tokens(['[Empty]'],special_tokens = True)\n",
        "\n",
        "# keyword_encoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# keyword_encoder_tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n",
        "# #keyword_encoder_tokenizer.add_tokens(['[Empty]'],special_tokens = True)\n",
        "\n",
        "# decoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# decoder_tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n",
        "# #decoder_tokenizer.add_tokens(['[Empty]'],special_tokens = True)\n",
        "decoder_tokenizer.pad_token = decoder_tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YQK6shuAXPf"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Let's use the bill summarization corpus for our task. For simplicity, we only concern how to generate the title of a bill article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F5Si_QFAWyV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "05c24692ad814124a4b9e4220d56d282",
            "31d0b3f05091410c8bb9c80398572f09",
            "19477efc6acf47798a38b16a051ea7ad",
            "1a849eb139284f489ccf6ae47c48f04b",
            "0af31a7196bb4f8a86143254e3db4410",
            "cb5c71ada824451d877cfb36dfde1884",
            "d73f93a9c7b64c23a632c9c0693bea95",
            "754b32c5f52c4ef4b371823fe7978ec1",
            "a10b8f339aa74beaa652ecb3b64aa33e",
            "93c25f025e4a495caefcb9ec70fc9537",
            "029436f4ae634ee9a66d5cad6ded90cc",
            "116d9d65860240bb8ee48721894541fc",
            "c57ff0763b9e47f18ed3d8e6d5a2c17d",
            "075d01101b5d4f1a8047b5fcbaddae73",
            "bb4853d1be2e48bf95fb1b9f6249b598",
            "8deee92328654395a460304bb74d1d44",
            "704be0a3ca0847c2a05a5827a0541315",
            "d4d3aaf8e00c40d0a9b41d7a769d7d0f",
            "22f1b1258a66481fa00921a023b8a9b5",
            "d38dc8a855d8476dbf2573d7e89052d8",
            "cb067beed977416e813fdd7b05b54a5b",
            "79804d9d9e2344d58bdbd53ba50f4919",
            "e69348bab9ec4130abaa7eecdea663a8",
            "d62476f229ef4a69bb721f1a0b318a9e",
            "924e24f5da33414ea1f217e2859c3e36",
            "8ec0dfe2e43340e5aa494fbc2f698c2e",
            "1d8aee1a81124f2e9a8de9dbf227ebaf",
            "36d937f2878047d68d8ba6f2050e9d6a",
            "0fd169cbb4124204bb5368eec32d31c6",
            "b9f2515923e6414c872beae3571b202e",
            "32daeea075404702988492fbc054d111",
            "36a517552ff14fd5b0e8fb0f75726278",
            "f27ea7edd3e14660b94153ae5f13deb7"
          ]
        },
        "outputId": "a28f1c77-5e64-4411-a109-f88d82fe9568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default-f6d6e6b9d8a57214\n",
            "Reusing dataset csv (/content/drive/My Drive/.cache/huggingface/datasets/csv/default-f6d6e6b9d8a57214/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05c24692ad814124a4b9e4220d56d282"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default-9e1e0dfc39dccec1\n",
            "Reusing dataset csv (/content/drive/My Drive/.cache/huggingface/datasets/csv/default-9e1e0dfc39dccec1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "116d9d65860240bb8ee48721894541fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default-2b8af1ed0e9e49ca\n",
            "Reusing dataset csv (/content/drive/My Drive/.cache/huggingface/datasets/csv/default-2b8af1ed0e9e49ca/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e69348bab9ec4130abaa7eecdea663a8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "train_dataset = datasets.load_dataset('csv', data_files = \"./data/data_csv/train_ult.csv\")['train']\n",
        "valid_dataset = datasets.load_dataset('csv', data_files = \"./data/data_csv/valid_ult.csv\")['train']\n",
        "test_dataset = datasets.load_dataset('csv', data_files = \"./data/data_csv/test_ult.csv\")['train']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8r0IaFct_TN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d321365e-1c84-4dbb-a605-df269d029e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['Unnamed: 0', 'artist_name', 'this_line', 'next_line', 'phonemes', 'keywords'],\n",
            "    num_rows: 69955\n",
            "})\n",
            "Dataset({\n",
            "    features: ['Unnamed: 0', 'artist_name', 'this_line', 'next_line', 'phonemes', 'keywords'],\n",
            "    num_rows: 14991\n",
            "})\n",
            "Dataset({\n",
            "    features: ['Unnamed: 0', 'artist_name', 'this_line', 'next_line', 'phonemes', 'keywords'],\n",
            "    num_rows: 14991\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset)\n",
        "print(valid_dataset)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL5srwrEBErk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614001ca-59bd-4635-f4ca-4a8aa931a1b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "9\n",
            "10\n",
            "1\n",
            "Pop a shot, kill 'em all, kill 'em all\n",
            "It's just me and my niggas taking over the game\n",
            "AH,AO\n",
            "rappers,shooters,tryna\n"
          ]
        }
      ],
      "source": [
        "x = 200\n",
        "print(len(train_dataset['artist_name'][x].split()))\n",
        "print(len(train_dataset['this_line'][x].split()))\n",
        "print(len(train_dataset['next_line'][x].split()))\n",
        "print(len(train_dataset['phonemes'][x].split()))\n",
        "print(train_dataset['this_line'][x])\n",
        "print(train_dataset['next_line'][x])\n",
        "print(train_dataset['phonemes'][x])\n",
        "print(train_dataset['keywords'][x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pavhkFXCqHk"
      },
      "source": [
        "### Implement `class RapDataset`\n",
        "\n",
        "* Implement a function to prepare input and target pairs\n",
        "* Implement a function to pad a batch of samples that have different lengths.\n",
        "* Implement a function to return a batch of tensors of \n",
        " padded input_ids and lm_labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL8bBbmSNgN0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "class RapDataset(Dataset):\n",
        "  \"\"\"\n",
        "  This class will prepare a batch of sequences that are properly padded\n",
        "  due to different lengths in each sentence in a batch.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, datasets,se_tokenizer, ke_tokenizer,d_tokenizer, keywords_column_name = \"keywords\",phonemes_column_name=\"phonemes\",this_line_column_name=\"this_line\", next_line_column_name='next_line', artist_column_name = 'artist_name', max_length=384):\n",
        "    self._this_line_column_name = this_line_column_name\n",
        "    self._next_line_column_name = next_line_column_name\n",
        "    self._artist_column_name = artist_column_name\n",
        "    self._phonemes_column_name = phonemes_column_name\n",
        "    self._keywords_column_name = keywords_column_name\n",
        "    self._preprocessing_num_workers = 4\n",
        "    self._overwrite_cache = False\n",
        "    self._padding = False\n",
        "    self._datasets = datasets\n",
        "    self._se_tokenizer = se_tokenizer\n",
        "    self._ke_tokenizer = ke_tokenizer\n",
        "    self._d_tokenizer = d_tokenizer\n",
        "    self._ph_dic = {\"AA\":0,\"AE\":1,\"AH\":2,\"AO\":3,\"AW\":4,\"AY\":5,\"EH\":6,\"ER\":7,\"EY\":8,\"IH\":9,\"IY\":10,\"OW\":11,\"OY\":12,\"UH\":13,\"UW\":14,\"[PAD]\":15}\n",
        "\n",
        "    self._d_bos = d_tokenizer.bos_token_id\n",
        "    self._d_eos = d_tokenizer.eos_token_id\n",
        "\n",
        "    self._se_sep = se_tokenizer.sep_token_id\n",
        "    self._se_cls = se_tokenizer.cls_token_id\n",
        "    self._ke_sep = ke_tokenizer.sep_token_id\n",
        "    self._ke_cls = ke_tokenizer.cls_token_id\n",
        "\n",
        "    self._max_length = max_length\n",
        "\n",
        "    # tokenize the raw text, creating a new field \"input_ids\"\n",
        "    self._tokenized_datasets = datasets.map(\n",
        "      self._tokenize,\n",
        "      batched=True,\n",
        "      num_proc=self._preprocessing_num_workers,\n",
        "      remove_columns=[],\n",
        "      load_from_cache_file=not self._overwrite_cache,\n",
        "    )\n",
        "\n",
        "  def _clean(self, text):\n",
        "    if isinstance(text, list):\n",
        "      result = []\n",
        "      for x in text:\n",
        "        xx = re.sub(r'\\\\n', ' ', x)\n",
        "        xx = re.sub(r'\\s+', ' ', xx)\n",
        "        result.append(xx)\n",
        "      return result\n",
        "    else:\n",
        "      xx = re.sub(r'\\\\n', ' ', text)\n",
        "      xx = re.sub(r'\\s+', ' ', xx)\n",
        "      return xx\n",
        "\n",
        "  def _tokenize(self, example):\n",
        "    this_line_output = self._se_tokenizer(self._clean(example[self._this_line_column_name]), padding=self._padding, truncation=True, max_length=self._max_length)\n",
        "    next_line_output = self._d_tokenizer(self._clean(example[self._next_line_column_name]), padding=self._padding, truncation=True, max_length=self._max_length)\n",
        "    keyword_output = self._ke_tokenizer(self._clean(example[self._keywords_column_name]), padding=self._padding, truncation=True, max_length=self._max_length)\n",
        "    keyword_ids = keyword_output['input_ids']\n",
        "\n",
        "    phonemes_l = example[self._phonemes_column_name]\n",
        "    phoneme_ids = []\n",
        "    for string in phonemes_l:\n",
        "      if string:\n",
        "        l = string.split(\",\")\n",
        "        for i in range(len(l)):\n",
        "          l[i] = self._ph_dic[l[i]]\n",
        "        phoneme_ids.append(l)\n",
        "      \n",
        "\n",
        "    return {'this_ids': this_line_output['input_ids'],\n",
        "            'next_ids': next_line_output['input_ids'],\n",
        "            'artist_names': example[self._artist_column_name],\n",
        "            'phoneme_ids': phoneme_ids,\n",
        "            'keyword_ids':keyword_ids\n",
        "           }\n",
        "  \n",
        "  def _build_input_from_segments(self, this_line, next_line,artist_name,phoneme_ids,keyword_ids):\n",
        "    instance = {}\n",
        "    instance['input_ids'] = [self._d_bos] + next_line \n",
        "    instance['lm_labels'] = next_line + [self._d_eos]\n",
        "    instance['phoneme_ids'] = phoneme_ids\n",
        "    instance['this_line'] = this_line\n",
        "    instance['next_line'] = next_line\n",
        "    instance['artist_names'] = artist_name\n",
        "    instance['keyword_ids'] = keyword_ids\n",
        "\n",
        "    return instance\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self._tokenized_datasets)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = self._tokenized_datasets[index]\n",
        "    return self._build_input_from_segments(item['this_ids'], item['next_ids'],item['artist_names'],item['phoneme_ids'],item['keyword_ids'])\n",
        "\n",
        "  def _pad_ids(self, arrays, padding, max_length=-1):\n",
        "    if max_length == -1:\n",
        "      length = max([len(i) for i in arrays]) \n",
        "    else:\n",
        "      length = max([len(i) for i in arrays] + [max_length])\n",
        "\n",
        "    for i in range(len(arrays)):\n",
        "      arrays[i] = arrays[i] + [padding for i in range(length - len(arrays[i]))]\n",
        "\n",
        "\n",
        "    return arrays\n",
        "\n",
        "\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    \"\"\"\n",
        "    batch: It should contain a list of Python dictionary returned from __getitem__\n",
        "    \"\"\"\n",
        "    input_ids = [seq['input_ids'] for seq in batch]\n",
        "    lm_labels = [seq['lm_labels'] for seq in batch]\n",
        "    phoneme_ids = [seq['phoneme_ids'] for seq in batch]\n",
        "    artist_name_ids = [seq['artist_names'] for seq in batch]\n",
        "    keyword_ids = [seq['keyword_ids'] for seq in batch]\n",
        "    this_line_ids = [seq['this_line'] for seq in batch]\n",
        "\n",
        "    imput_ids = self._pad_ids(input_ids,self._d_tokenizer.pad_token_id)\n",
        "    lm_labels = self._pad_ids(lm_labels,self._d_tokenizer.pad_token_id)\n",
        "    keywords_ids = self._pad_ids(keyword_ids,self._ke_tokenizer.pad_token_id)\n",
        "    this_line_ids = self._pad_ids(this_line_ids,self._se_tokenizer.pad_token_id)\n",
        "\n",
        "\n",
        "    input_ids = torch.from_numpy(np.asarray(input_ids))\n",
        "    lm_labels = torch.from_numpy(np.asarray(lm_labels))\n",
        "    phoneme_ids =  torch.from_numpy(np.asarray(phoneme_ids))\n",
        "    keyword_ids = torch.from_numpy(np.asarray(keyword_ids))\n",
        "    this_line_ids = torch.from_numpy(np.asarray(this_line_ids))\n",
        "    # artist_name_ids = torch.from_numpy(np.asarray(artist_name_ids))\n",
        "    return input_ids.to(device), lm_labels.to(device), phoneme_ids.to(device), keyword_ids.to(device), this_line_ids.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5xl8l-3UlXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a69cb9-1394-4317-fa20-7e9b4ec00e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-f6d6e6b9d8a57214/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-91591516abba3625.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-f6d6e6b9d8a57214/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-46f245440b1904e0.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-f6d6e6b9d8a57214/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-77ea6b7433f5792d.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-f6d6e6b9d8a57214/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-f89601c3084be338.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-9e1e0dfc39dccec1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-4a6905e854ac3f84.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-9e1e0dfc39dccec1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-b00e2d51113e59e3.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-9e1e0dfc39dccec1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-54d14eba03d19014.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-9e1e0dfc39dccec1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-5d127167312f752a.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-2b8af1ed0e9e49ca/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-ac5027c302778d55.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-2b8af1ed0e9e49ca/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-300599d5f2fb268f.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-2b8af1ed0e9e49ca/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-0cb59834b17ba4eb.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /content/drive/My Drive/.cache/huggingface/datasets/csv/default-2b8af1ed0e9e49ca/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-1498ae669fd9c9f4.arrow\n"
          ]
        }
      ],
      "source": [
        "train_rap_dataset = RapDataset(train_dataset,sentence_encoder_tokenizer,keyword_encoder_tokenizer,decoder_tokenizer)\n",
        "valid_rap_dataset = RapDataset(valid_dataset,sentence_encoder_tokenizer,keyword_encoder_tokenizer,decoder_tokenizer)\n",
        "test_rap_dataset = RapDataset(test_dataset,sentence_encoder_tokenizer,keyword_encoder_tokenizer,decoder_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHyodlLDV0aU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ae0ca0-1178-4b7a-b06b-e71179b2c5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs for Sentence Encoder:\n",
            "\t [101, 2330, 2039, 1996, 11002, 1010, 2016, 1005, 1055, 2074, 3564, 2045, 1010, 7592, 102]\n",
            "Input IDs for Keyword Encoder:\n",
            " \t [101, 7133, 1010, 10181, 1010, 3566, 102]\n",
            "Input IDs for Phoneme Encoder:\n",
            "\t [2, 11]\n",
            "Input IDs for Decoder:\n",
            "\t [50256, 5122, 5273, 338, 4506, 11, 3521, 470, 772, 787, 4151, 2800, 618, 356, 2740]\n",
            "LM Labels for Decoder:\n",
            "\t [5122, 5273, 338, 4506, 11, 3521, 470, 772, 787, 4151, 2800, 618, 356, 2740, 50256]\n",
            "Input sentence for Sentence Encoder:\n",
            "\t [CLS] open up the curtain, she's just sitting there, hello [SEP]\n",
            "Input keywords for Keyword Encoder:\n",
            "\t [CLS] grandmother, deceased, mom [SEP]\n",
            "Input keywords for Phoneme Encoder:\n",
            "\t AH,OW\n",
            "Input sentence for Decoder:\n",
            "\t <|endoftext|>Our conversation's brief, couldn't even make eye contact when we speak\n",
            "LM Ground Truth Sentence for Decoder:\n",
            "\t Our conversation's brief, couldn't even make eye contact when we speak<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "x = 20\n",
        "\n",
        "print(\"Input IDs for Sentence Encoder:\\n\\t\", train_rap_dataset[x]['this_line'])\n",
        "print(\"Input IDs for Keyword Encoder:\\n \\t\",train_rap_dataset[x]['keyword_ids'])\n",
        "print(\"Input IDs for Phoneme Encoder:\\n\\t\",train_rap_dataset[x]['phoneme_ids'])\n",
        "print(\"Input IDs for Decoder:\\n\\t\", train_rap_dataset[x]['input_ids'])\n",
        "print(\"LM Labels for Decoder:\\n\\t\",train_rap_dataset[x]['lm_labels'])\n",
        "\n",
        "print(\"Input sentence for Sentence Encoder:\\n\\t\",sentence_encoder_tokenizer.decode(train_rap_dataset[x]['this_line']))\n",
        "print(\"Input keywords for Keyword Encoder:\\n\\t\",keyword_encoder_tokenizer.decode(train_rap_dataset[x]['keyword_ids']))\n",
        "print(\"Input keywords for Phoneme Encoder:\\n\\t\",ph_decode(train_rap_dataset[x]['phoneme_ids'],ph_dic))\n",
        "print(\"Input sentence for Decoder:\\n\\t\", decoder_tokenizer.decode(train_rap_dataset[x]['input_ids']))\n",
        "print(\"LM Ground Truth Sentence for Decoder:\\n\\t\",decoder_tokenizer.decode(train_rap_dataset[x]['lm_labels']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0hrtRczytOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae67f83f-98b9-422f-f2a8-a9908d18ab37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "19 14 14 2 9\n",
            "Padded Input IDs of Sentence Encoder: \n",
            "\t tensor([ 101, 2057, 7110, 1005, 1056, 1999, 1037, 9241, 1010, 5481, 2378, 1005,\n",
            "        2046, 2293,  102,    0,    0,    0,    0], device='cuda:0')\n",
            "Padded Input IDs of Phoneme Encoder: \n",
            "\t tensor([14,  2], device='cuda:0')\n",
            "Padded Input IDs of Keyword Encoder: \n",
            "\t tensor([  101,  4184,  4402,  1010, 12925, 28345,  1010, 24438,   102],\n",
            "       device='cuda:0')\n",
            "Padded Input IDs of Decoder: \n",
            "\t tensor([50256,  8061, 11892,   777, 40207,    11,   705, 47163,   484,  1282,\n",
            "          287,  2800,   351, 15391], device='cuda:0')\n",
            "Padded Ground Truth IDs of Decoder: \n",
            "\t tensor([ 8061, 11892,   777, 40207,    11,   705, 47163,   484,  1282,   287,\n",
            "         2800,   351, 15391, 50256], device='cuda:0')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create dataloader for traininz\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "print(batch_size)\n",
        "train_dataloader = DataLoader(\n",
        "  train_rap_dataset,\n",
        "  sampler=RandomSampler(train_rap_dataset),\n",
        "  batch_size=batch_size,\n",
        "  collate_fn=train_rap_dataset.collate_fn\n",
        ")\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "  valid_rap_dataset,\n",
        "  sampler=SequentialSampler(valid_rap_dataset),\n",
        "  batch_size=batch_size,\n",
        "  collate_fn=valid_rap_dataset.collate_fn\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "  test_rap_dataset,\n",
        "  sampler=SequentialSampler(test_rap_dataset),\n",
        "  batch_size=batch_size,\n",
        "  collate_fn=test_rap_dataset.collate_fn\n",
        ")\n",
        "\n",
        "\n",
        "x = 2\n",
        "for i, batch in enumerate(train_dataloader):\n",
        "  input_ids, lm_labels,phoneme_ids,keyword_ids,this_line_ids = batch\n",
        "  print(len(this_line_ids[x]),len(input_ids[x]), len(lm_labels[x]),len(phoneme_ids[x]),len(keyword_ids[x]))\n",
        "  print(\"Padded Input IDs of Sentence Encoder: \\n\\t\",this_line_ids[x])\n",
        "  print(\"Padded Input IDs of Phoneme Encoder: \\n\\t\",phoneme_ids[x])\n",
        "  print(\"Padded Input IDs of Keyword Encoder: \\n\\t\",keyword_ids[x])\n",
        "  print(\"Padded Input IDs of Decoder: \\n\\t\",input_ids[x])\n",
        "  print(\"Padded Ground Truth IDs of Decoder: \\n\\t\",lm_labels[x])\n",
        "  print()\n",
        "\n",
        "\n",
        "  if i >= 0:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPNKdK0N1mj4"
      },
      "source": [
        "Data is completely pre-processed above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2SLtjGU9-mQ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs8T79i_VaXg"
      },
      "source": [
        "##Initialize sub pretrained models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBZUBaJUcAbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9da84ba-cc65-4fb7-c6dd-00ffaa1a4300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['transformer.h.2.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.0.crossattention.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.1.crossattention.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.3.crossattention.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.3.crossattention.masked_bias', 'transformer.h.2.crossattention.masked_bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.5.crossattention.bias', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.5.crossattention.masked_bias', 'transformer.h.4.crossattention.bias', 'transformer.h.2.crossattention.bias', 'transformer.h.0.crossattention.masked_bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.4.crossattention.masked_bias', 'transformer.h.1.crossattention.masked_bias', 'transformer.h.0.ln_cross_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['transformer/embeddings', 'transformer/pooler', 'transformer/encoder']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoderModel(\n",
              "  (encoder): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0): GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (crossattention): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (q_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (crossattention): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (q_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (crossattention): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (q_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (crossattention): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (q_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (crossattention): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (q_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (crossattention): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (q_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from transformers import EncoderDecoderModel, AutoModel\n",
        "k_encoder_name = 'bert-base-uncased'\n",
        "k_encoder = AutoModel.from_pretrained(k_encoder_name)\n",
        "k_encoder.to(device)\n",
        "encoder_name = 'bert-base-uncased'\n",
        "decoder_name = 'distilgpt2'\n",
        "bert2gpt2 = EncoderDecoderModel.from_encoder_decoder_pretrained(encoder_name, decoder_name,  tie_encoder_decoder=True)\n",
        "bert2gpt2.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Design Encoder Decoder Model with all features\n",
        "\n"
      ],
      "metadata": {
        "id": "6RjrIHDww0WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class Encoder_Decoder_LM(nn.Module):\n",
        "  def __init__(self,k_encoder,bert2gpt2,s_tokenizer,k_tokenizer,d_tokenizer):\n",
        "    super().__init__()\n",
        "\n",
        "    # self.p_emb = nn.Embedding(16,768)\n",
        "    # self.p_layer = nn.Linear(768,768)\n",
        "    self.w_k = nn.Linear(768,768)\n",
        "\n",
        "\n",
        "    self.k_encoder = k_encoder\n",
        "    self.bert2gpt2 = bert2gpt2\n",
        "    self.s_tok = s_tokenizer\n",
        "    self.k_tok = k_tokenizer\n",
        "    self.d_tok = d_tokenizer\n",
        "\n",
        "    \n",
        "\n",
        "  def make_attention_mask(self,ids,tok):\n",
        "    return torch.log(torch.ne(ids,tok.pad_token_id))\n",
        "\n",
        "  def forward(self,this_line_ids, keyword_ids, input_ids,phoneme_ids):\n",
        "    #getting tensors' sizes\n",
        "    B = this_line_ids.size()[0]\n",
        "    M = this_line_ids.size()[1]\n",
        "    \n",
        "\n",
        "\n",
        "    ##making masks\n",
        "    s_mask = self.make_attention_mask(this_line_ids,self.s_tok)\n",
        "    k_mask = self.make_attention_mask(keyword_ids,self.k_tok)\n",
        "    d_mask = self.make_attention_mask(input_ids,self.d_tok)\n",
        "    \n",
        "    ##getting hidden states\n",
        "    k_hidden_cls = self.k_encoder(keyword_ids,attention_mask=k_mask).last_hidden_state[:,0,:][:,None,]\n",
        "    hidden_state= self.bert2gpt2(input_ids=this_line_ids,attention_mask=s_mask, \n",
        "                        decoder_input_ids = input_ids, decoder_attention_mask = d_mask).encoder_last_hidden_state\n",
        "    # phoneme_embs = self.p_emb(phoneme_ids)\n",
        "    # p_hidden = self.p_layer(phoneme_embs)\n",
        "\n",
        "    #repeating cls hidden states for M tokens\n",
        "    k_hidden_cls = k_hidden_cls.repeat(1,M,1)\n",
        "    k_hidden_cls = self.w_k(k_hidden_cls)\n",
        "\n",
        "    #Create phoneme feature tensor to the last 2 words of the this_line\n",
        "    # temp_p = torch.zeros_like(hidden_state)\n",
        "    # for n in range(B):\n",
        "    #   seq = this_line_ids[n]\n",
        "    #   for i in range(M):\n",
        "    #     if M>2:\n",
        "    #       if seq[i] == self.s_tok.pad_token_id:\n",
        "    #         temp_p[n,i-2] = p_hidden[n,0]\n",
        "    #         temp_p[n,i-1] = p_hidden[n,1]\n",
        "    \n",
        "\n",
        "    ##Adding the features together\n",
        "    # hidden_state = torch.add(hidden_state,temp_p)\n",
        "    hidden_state = torch.add(hidden_state,k_hidden_cls)\n",
        "    # hidden_state = hidden_state[None,:,:,:]\n",
        "    print('hidden' , hidden_state)\n",
        "    ##Feeding the hidden states into the decoder\n",
        "    outputs = self.bert2gpt2(encoder_outputs = (hidden_state,), attention_mask = s_mask,\n",
        "                        decoder_input_ids = input_ids, decoder_attention_mask = d_mask,return_dict = True)\n",
        "    # outputs = self.bert2gpt2(input_ids=this_line_ids,attention_mask=s_mask, \n",
        "    #                     decoder_input_ids = input_ids, decoder_attention_mask = d_mask)\n",
        "\n",
        "    return outputs\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D458zn0xYzXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initilize model"
      ],
      "metadata": {
        "id": "Z3i7E22tw50K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Encoder_Decoder_LM(k_encoder,bert2gpt2,sentence_encoder_tokenizer,keyword_encoder_tokenizer,decoder_tokenizer)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "4ttwj5ZWjqCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec84ad2-d444-41c4-8f72-95c810510252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder_Decoder_LM(\n",
              "  (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (k_encoder): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bert2gpt2): EncoderDecoderModel(\n",
              "    (encoder): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (6): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (7): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (8): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (9): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (10): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (11): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (decoder): GPT2LMHeadModel(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(50257, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0): GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (crossattention): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (q_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (crossattention): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (q_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (crossattention): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (q_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (crossattention): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (q_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (crossattention): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (q_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (crossattention): GPT2Attention(\n",
              "              (c_attn): Conv1D()\n",
              "              (q_attn): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D()\n",
              "              (c_proj): Conv1D()\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz4qQ0Zp7HHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd68f62-2ee1-4137-a791-520ae6a22e50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "# # # load model from epoch 19 from your Google drive\n",
        "model_path = \"./no_phoneme_Bert2GPT/16 ppl 41.8/pytorch_model.bin\"\n",
        "\n",
        "# # # read state dict (each parameter has a name that maps to a tensor)\n",
        "state_dict = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "# # # load weights into model\n",
        "model.load_state_dict(state_dict,strict=False)\n",
        "# # #print(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training\n",
        "\n"
      ],
      "metadata": {
        "id": "LIOEGCEGwuKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train set up"
      ],
      "metadata": {
        "id": "8Kev8zoewsyK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9ZWvxxxfhbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23eec05-a56c-4d99-9f1a-954446c623e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "gradient_accumulation_steps = 1\n",
        "learning_rate = 1e-5\n",
        "adam_epsilon = 1e-8\n",
        "max_grad_norm = 1.0\n",
        "num_train_epochs = 10\n",
        "warmup_steps = 0\n",
        "\n",
        "seed = 42\n",
        "batch_size = 8\n",
        "max_seq_length = 139\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Loss and Optimizer\n",
        "t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index = decoder_tokenizer.pad_token_id)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
        ")\n",
        "print(learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-MgHbNoVfyU"
      },
      "source": [
        "##Model on training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hgXQnT2uaMI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "output_root = \"./no_phoneme_Bert2GPT\"\n",
        "num_train_epochs = 4\n",
        "starting_epoch = 19\n",
        "# Start training\n",
        "model.train()\n",
        "global_step = 0\n",
        "for epoch in tqdm(range(num_train_epochs)):\n",
        "  tr_loss = 0.0\n",
        "  last_tr_loss = 0.0\n",
        "  print(\"epoch:\", epoch+starting_epoch)\n",
        "  local_step = 0\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    global_step += 1\n",
        "    local_step += 1\n",
        "\n",
        "    # get context and target label\n",
        "    input_ids, lm_labels, phoneme_ids, keyword_ids, this_line_ids = batch\n",
        "\n",
        "    # zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # print(encoder_attention_mask)\n",
        "    # print(decoder_attention_mask)\n",
        "    # logits\n",
        "    outputs = model(this_line_ids,keyword_ids,input_ids,phoneme_ids)\n",
        "\n",
        "    # get shapes\n",
        "    logits = outputs[0]\n",
        "    B, N, V = logits.size()\n",
        "    # print('ll' ,logits.size() ,  lm_labels.size())\n",
        "    # TODO: calculate sum loss.\n",
        "    # Hint: flatten logits and lm_labels into 2-D tensor and 1-D tensor respectively.\n",
        "    # i.e. (B*N, V) for the predicts, labels (B*N)\n",
        "    # 1 line of code\n",
        "    loss = criterion(logits.view(B*N, V), lm_labels.view(B*N))\n",
        "\n",
        "    # accumulate total lossa\n",
        "    tr_loss += loss.item()\n",
        "\n",
        "    # backprop\n",
        "    loss.backward()\n",
        "\n",
        "    # apply gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(bert2gpt2.parameters(), max_grad_norm)\n",
        "\n",
        "    # model update\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # report performance\n",
        "    # if (step+1) % (batch_size*4) == 0:\n",
        "    #   # write avg train loss of current batch to tensorboard\n",
        "    #   writer.add_scalar(\"Loss/train\", loss.item(), global_step)\n",
        "\n",
        "    \n",
        "    if local_step % 800 == 0 and local_step != 0:\n",
        "      added_loss = tr_loss - last_tr_loss\n",
        "      last_tr_loss = tr_loss\n",
        "      print(\"Average loss over Step\", local_step-800, \"~ Step\",local_step,\":\", added_loss/800)\n",
        "\n",
        "  print('loss for epoch {}'.format(epoch+starting_epoch),tr_loss)\n",
        "  # save model\n",
        "  state_dict = model.state_dict()\n",
        "  output_dir = output_root + \"/\" + str(epoch+starting_epoch)\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  torch.save(state_dict, os.path.join(output_dir, \"pytorch_model.bin\"))\n",
        "  print(\"----------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "upWwjryHrOmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMnGJ-8WYXyC"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toARAiWnYnfB"
      },
      "source": [
        "## Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLbYi2RjYqMU"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def perplexity(iter):\n",
        "  total_loss = 0.0\n",
        "  total_tokens = 0\n",
        "  bert2gpt2.eval()\n",
        "  criterion1 = torch.nn.CrossEntropyLoss(ignore_index = decoder_tokenizer.pad_token_id, reduction='sum')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in iter:\n",
        "      # get context and target label\n",
        "      input_ids, lm_labels, phoneme_ids, keyword_ids, this_line_ids = batch\n",
        "\n",
        "      # TODO: obtain the valid positions in minibatch to calculate valid number \n",
        "      # of tokens\n",
        "      # 1-2 lines of code\n",
        "      #print(lm_lables)\n",
        "      num_tokens = torch.sum(lm_labels != decoder_tokenizer.pad_token_id)\n",
        "\n",
        "      outputs = model(this_line_ids,keyword_ids,input_ids,phoneme_ids)\n",
        "\n",
        "      # get shapes\n",
        "      logits = outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "      # get shapes\n",
        "      B, N, V = logits.size()\n",
        "\n",
        "      # TODO: calculate sum loss. Notice criterion1 is using 'sum' instead of 'mean'\n",
        "      # Hint: flatten logits and lm_labels into 2-D tensor and 1-D tensor respectively.\n",
        "      # i.e. (B*N, V) for the predicts, labels (B*N)\n",
        "      # 1 line of code\n",
        "      loss = criterion1(logits.view(B*N, V), lm_labels.view(B*N))\n",
        "\n",
        "      # accumulate total loss\n",
        "      total_loss += loss.sum().item()\n",
        "\n",
        "      # accumulate total number of predicted positions\n",
        "      total_tokens += num_tokens\n",
        "  print(total_tokens , total_loss)\n",
        "  return math.exp(total_loss / total_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pRAmbtuvYsU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfd1288b-3905-4c4c-d475-4909626c109a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden tensor([[[ 0.8959,  0.2142, -0.6306,  ...,  1.0976, -1.0475,  0.9246],\n",
            "         [ 0.8967,  0.2143, -0.6308,  ...,  1.0968, -1.0477,  0.9250],\n",
            "         [ 0.8956,  0.2145, -0.6309,  ...,  1.0972, -1.0478,  0.9245],\n",
            "         ...,\n",
            "         [ 0.8957,  0.2146, -0.6309,  ...,  1.0973, -1.0478,  0.9245],\n",
            "         [ 0.8957,  0.2145, -0.6309,  ...,  1.0972, -1.0478,  0.9245],\n",
            "         [ 0.8956,  0.2146, -0.6309,  ...,  1.0973, -1.0478,  0.9245]],\n",
            "\n",
            "        [[ 0.1614,  0.4972, -0.3261,  ...,  1.0073, -0.5150,  0.8894],\n",
            "         [ 0.1594,  0.4971, -0.3257,  ...,  1.0084, -0.5147,  0.8887],\n",
            "         [ 0.1624,  0.4979, -0.3274,  ...,  1.0065, -0.5143,  0.8903],\n",
            "         ...,\n",
            "         [ 0.1601,  0.4969, -0.3264,  ...,  1.0076, -0.5143,  0.8893],\n",
            "         [ 0.1531,  0.5038, -0.3342,  ...,  1.0090, -0.5024,  0.8948],\n",
            "         [ 0.1608,  0.4977, -0.3275,  ...,  1.0077, -0.5140,  0.8898]],\n",
            "\n",
            "        [[ 0.4728,  0.8108,  0.0750,  ...,  0.5671, -0.8998,  1.2157],\n",
            "         [ 0.4735,  0.8110,  0.0741,  ...,  0.5664, -0.8994,  1.2166],\n",
            "         [ 0.4740,  0.8110,  0.0736,  ...,  0.5657, -0.8994,  1.2172],\n",
            "         ...,\n",
            "         [ 0.4733,  0.8111,  0.0744,  ...,  0.5665, -0.9000,  1.2159],\n",
            "         [ 0.4734,  0.8111,  0.0742,  ...,  0.5664, -0.9000,  1.2161],\n",
            "         [ 0.4728,  0.8110,  0.0747,  ...,  0.5670, -0.9001,  1.2152]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.8005,  0.4433, -0.8162,  ...,  0.4915, -0.6701,  0.9181],\n",
            "         [ 0.8028,  0.4419, -0.8155,  ...,  0.4901, -0.6703,  0.9187],\n",
            "         [ 0.8019,  0.4431, -0.8169,  ...,  0.4902, -0.6695,  0.9189],\n",
            "         ...,\n",
            "         [ 0.0947, -0.1952, -0.0845,  ..., -0.3176,  0.0715,  0.0719],\n",
            "         [ 0.8027,  0.4421, -0.8154,  ...,  0.4909, -0.6706,  0.9183],\n",
            "         [ 0.7982,  0.4461, -0.8217,  ...,  0.4912, -0.6665,  0.9206]],\n",
            "\n",
            "        [[ 0.7119,  0.4896, -0.3913,  ...,  1.0321, -0.9941,  0.5859],\n",
            "         [ 0.7114,  0.4899, -0.3921,  ...,  1.0322, -0.9937,  0.5860],\n",
            "         [ 0.7125,  0.4901, -0.3926,  ...,  1.0318, -0.9932,  0.5868],\n",
            "         ...,\n",
            "         [ 0.7114,  0.4903, -0.3928,  ...,  1.0325, -0.9932,  0.5856],\n",
            "         [ 0.7115,  0.4902, -0.3928,  ...,  1.0324, -0.9933,  0.5856],\n",
            "         [ 0.7117,  0.4902, -0.3924,  ...,  1.0321, -0.9936,  0.5856]],\n",
            "\n",
            "        [[ 0.5989,  1.2175, -1.2784,  ...,  0.5959, -0.8509,  1.0989],\n",
            "         [ 0.5994,  1.2179, -1.2790,  ...,  0.5957, -0.8505,  1.0995],\n",
            "         [ 0.5978,  1.2177, -1.2782,  ...,  0.5976, -0.8503,  1.0981],\n",
            "         ...,\n",
            "         [ 0.5967,  1.2186, -1.2782,  ...,  0.5989, -0.8490,  1.0981],\n",
            "         [ 0.5976,  1.2182, -1.2784,  ...,  0.5977, -0.8497,  1.0984],\n",
            "         [ 0.5976,  1.2183, -1.2783,  ...,  0.5977, -0.8496,  1.0985]]],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-ba333cba2274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# valid and test PPL, expected range: 50-60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#valid_ppl = perplexity(valid_dataloader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvalid_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(valid_ppl)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-eec4bdf349d6>\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(iter)\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mnum_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_labels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdecoder_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_line_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeyword_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphoneme_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;31m# get shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-b948332a76aa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, this_line_ids, keyword_ids, input_ids, phoneme_ids)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m##Feeding the hidden states into the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     outputs = self.bert2gpt2(encoder_outputs = (hidden_state,), attention_mask = s_mask,\n\u001b[0;32m---> 63\u001b[0;31m                         decoder_input_ids = input_ids, decoder_attention_mask = d_mask,return_dict = False)\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;31m# outputs = self.bert2gpt2(input_ids=this_line_ids,attention_mask=s_mask,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m#                     decoder_input_ids = input_ids, decoder_attention_mask = d_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         return Seq2SeqLMOutput(\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"BaseModelOutput\") to tuple"
          ]
        }
      ],
      "source": [
        "# valid and test PPL, expected range: 50-60\n",
        "#valid_ppl = perplexity(valid_dataloader)\n",
        "valid_ppl = perplexity(valid_dataloader)\n",
        "test_ppl = perplexity(test_dataloader)\n",
        "#print(valid_ppl)\n",
        "print(valid_ppl)\n",
        "print(test_ppl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5RhtLCSH2gt"
      },
      "outputs": [],
      "source": [
        "print(decoder_tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a63PvAxpnUOA"
      },
      "source": [
        "#Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0XulcVThw_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2677e46c-6095-492a-f64f-e71768ba24a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package cmudict to /content/drive/My\n",
            "[nltk_data]     Drive/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "tensor([[3, 2]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('cmudict')\n",
        "\n",
        "\n",
        "def sentence_vowel(line,limit=2):\n",
        "    line = decoder_tokenizer.decode(line)\n",
        "    l = []\n",
        "    all_vowels = {\"AA\":0,\"AE\":1,\"AH\":2,\"AO\":3,\"AW\":4,\"AY\":5,\"EH\":6,\"ER\":7,\"EY\":8,\"IH\":9,\"IY\":10,\"OW\":11,\"OY\":12,\"UH\":13,\"UW\":14,\"[Empty]\":15}\n",
        "    dic = nltk.corpus.cmudict.dict()\n",
        "    if line != '[Empty]':\n",
        "        vowels = []\n",
        "        words = line.split()[-5:]\n",
        "        for i in range(len(words)-1,-1,-1):\n",
        "            if len(vowels)>limit-1:\n",
        "                break\n",
        "            try:\n",
        "                word = words[i].lower()\n",
        "                count = 0\n",
        "                while word not in dic.keys() and count < 2:\n",
        "                    count += 1\n",
        "                    word = word[:-1]\n",
        "                sounds = dic[word]\n",
        "                sounds = sounds[0]\n",
        "                for j in range(len(sounds)-1,-1,-1):\n",
        "                    if len(vowels)>limit-1:\n",
        "                        break\n",
        "                    if sounds[j] in all_vowels.keys():\n",
        "                        vowels.append(sounds[j])\n",
        "                    elif sounds[j][:-1] in all_vowels.keys():\n",
        "                        vowels.append(sounds[j][:-1])\n",
        "            except:\n",
        "                continue\n",
        "        while len(vowels) < limit:\n",
        "            vowels.append(\"[Empty]\")\n",
        "\n",
        "\n",
        "\n",
        "    else:\n",
        "        vowels = [\"[Empty]\"]*limit\n",
        "\n",
        "\n",
        "    vowels.reverse()\n",
        "    vowel_ids = [all_vowels[vowels[i]] for i in range(len(vowels))]\n",
        "    vowel_ids = torch.LongTensor(vowel_ids)[None , :].to(device)\n",
        "\n",
        "\n",
        "    return vowel_ids\n",
        "\n",
        "print(sentence_vowel(decoder_tokenizer.encode(\"I fuck your mum\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hdh1lwNlWKo"
      },
      "outputs": [],
      "source": [
        "def top_filtering(logits, top_k=0, top_p=0.0, threshold=-float('Inf'), filter_value=-float('Inf')):    \n",
        "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (..., vocabulary size)\n",
        "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
        "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
        "                whose total probability mass is greater than or equal to the threshold top_p.\n",
        "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
        "                the threshold top_p.\n",
        "            threshold: a minimal threshold to keep logits\n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        # Compute cumulative probabilities of sorted tokens\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Back to unsorted indices and set them to -infinity\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    indices_to_remove = logits < threshold\n",
        "    logits[indices_to_remove] = filter_value\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tri(tri , sentence):\n",
        "  count = 0\n",
        "  l = len(tri)\n",
        "  for i in range(len(sentence) - (l-1)):\n",
        "    my_tri = sentence[i:i+l]\n",
        "    if my_tri == tri:\n",
        "      count += 1\n",
        "  return count"
      ],
      "metadata": {
        "id": "QTaZbE2zbJb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def phoneme_score(p1 , p2):\n",
        "  p1 = p1.float()\n",
        "  p2 = p2.float()\n",
        "  return math.log(torch.dot(p1.squeeze() , p2.squeeze()).item())"
      ],
      "metadata": {
        "id": "buyfxWzqBcou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pDLCTN15JA4"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import random as rd\n",
        "def beam_search(decoder , this_line_ids , keyword , phoneme, beam=50 , min_len=15 , max_len = 30  , last_tri = None):\n",
        "    k = beam\n",
        "    k_prev_words = torch.full((k, 1), decoder_tokenizer.bos_token_id , dtype=torch.long).to(device) # (k, 1)\n",
        "    seqs = k_prev_words #(k, 1)\n",
        "    # if second:\n",
        "    #   kk = torch.full((k, 1), f_word , dtype=torch.long).to(device)\n",
        "    #   seqs = torch.cat((k_prev_words,kk),1)\n",
        "    top_k_scores = torch.zeros(k, 1).to(device)\n",
        "    complete_seqs = []\n",
        "    complete_seqs_scores = []\n",
        "    incomplete_seqs = []\n",
        "    incomplete_seqs_scores = []\n",
        "    step = 1\n",
        "    seqs_scores = torch.zeros(beam).to(device)\n",
        "    vocab_size = decoder_tokenizer.vocab_size\n",
        "    the = 5e-4\n",
        "    while True:\n",
        "        print(step , seqs.size())\n",
        "        print('depth {}'.format(step),seqs)\n",
        "        outputs = model(this_line_ids,keyword,seqs,phoneme)[0]  # outputs: (k, seq_len, vocab_size)\n",
        "        outputs = top_filtering(outputs / 0.8)\n",
        "        next_token_logits = nn.Softmax(dim=-1)(outputs)[:,-1,:] # (k, vocab_size)\n",
        "        print('next',next_token_logits.size())\n",
        "        for i in range(k):\n",
        "          ls = seqs[i][1:]\n",
        "          for ele in ls:\n",
        "            next_token_logits[i][ele] = 0\n",
        "        if step == 1:\n",
        "        \n",
        "          top_k_scores, top_k_words = next_token_logits[0].topk(k, dim=0, largest=True, sorted=True)\n",
        "        else:\n",
        "        \n",
        "          top_k_scores, top_k_words = next_token_logits.contiguous().view(-1).topk(k, 0, True, True)\n",
        "        if step >= min_len:\n",
        "          f_logits =  next_token_logits.contiguous().view(-1)\n",
        "          for i in range(k):\n",
        "            if f_logits[i * decoder_tokenizer.eos_token_id] > the:\n",
        "              top_k_words[i] = decoder_tokenizer.eos_token_id\n",
        "\n",
        "        # seqs_scores += top_k_scores\n",
        "        prev_word_inds = top_k_words // vocab_size  # (k)  beam_id\n",
        "        next_word_inds = top_k_words % vocab_size  # (k)  token_id\n",
        "        print('prev word' , prev_word_inds)\n",
        "        print('topk {}'.format(step) , next_word_inds)\n",
        "        print('topk scores{}'.format(step) , top_k_scores)\n",
        "        # seqs: (k, step) ==> (k, step+1)\n",
        "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n",
        "        # print('sss',seqs)\n",
        "        # unfinished sentence\n",
        "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                                next_word != decoder_tokenizer.eos_token_id]\n",
        "        # finished sentence\n",
        "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "        if len(complete_inds) > 0:\n",
        "            complete_seqs.extend(seqs[complete_inds].tolist()) \n",
        "            complete_seqs_scores.extend(top_k_scores[complete_inds]) \n",
        "        # if len(incomplete_inds) > 0):\n",
        "        #     incomplete_seqs.extend(seqs[incomplete_inds].tolist()) \n",
        "        #     incomplete_seqs_scores.extend(top_k_scores[incomplete_inds] \n",
        "        k -= len(complete_inds) \n",
        "  \n",
        "        if k == 0: # finished\n",
        "           break\n",
        "  \n",
        "        # updating \n",
        "        seqs = seqs[incomplete_inds]\n",
        "        # hidden = hidden[prev_word_inds[incomplete_inds]]\n",
        "        top_k_scores = top_k_scores[incomplete_inds]   #(s, 1) s < k\n",
        "        top_k_scores = top_k_scores.unsqueeze(1)\n",
        "        k_prev_words = next_word_inds[incomplete_inds] #(s, 1) s < k\n",
        "        k_prev_words = k_prev_words.unsqueeze(1)\n",
        "        if step > max_len: #too long\n",
        "            break\n",
        "        step += 1\n",
        "    \n",
        "    ## Prune Trigram path\n",
        "    for i in range(len(complete_seqs_scores)):\n",
        "      if last_tri is not None and count_tri([46, 1860, 10898, 318] , complete_seqs[i]) >= 1:\n",
        "        complete_seqs_scores[i] = 0\n",
        "\n",
        "    ## Integrate beam_search_scores with phoneme_score\n",
        "    final_score = []\n",
        "    for i in range(len(complete_seqs_scores)):\n",
        "      list_sen = complete_seqs[i]\n",
        "      p1 = sentence_vowel(list_sen)\n",
        "      p2 = phoneme\n",
        "      print('ppppp' ,p1)\n",
        "      print('ppppp' ,p2)\n",
        "      p_score = phoneme_score(p1 ,p2)\n",
        "      print('ppppp' ,p_score)\n",
        "      beam_score = complete_seqs_scores[i]\n",
        "      final_score.append(beam_score + 0.08*p_score)\n",
        "    try:\n",
        "      i = final_score.index(max(final_score)) # max score sequence\n",
        "      seq = torch.LongTensor(complete_seqs[i])\n",
        "    except BaseException:\n",
        "      # value, i = seqs_scores.topk(1, 0, True, True)\n",
        "      i = rd.randint(0,9)\n",
        "      seq = torch.LongTensor(seqs[i].to('cpu'))\n",
        "    return seq[None,:].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E5kpoedcOBh"
      },
      "outputs": [],
      "source": [
        "import random as rd\n",
        "def FS():\n",
        "  target = rd.randint(1,100)\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    if step == target:\n",
        "      input_ids, lm_labels, phoneme_ids, keyword_ids, this_line_ids = batch\n",
        "      sentence = this_line_ids[1]\n",
        "      keyword = keyword_ids[1]\n",
        "      target = input_ids[1]\n",
        "      sentence = sentence[sentence.nonzero()].squeeze()\n",
        "      keyword = keyword[keyword.nonzero()].squeeze()\n",
        "      return sentence[None,:].to(device) , keyword.to(device) , target.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tri_1 = \"I'm sorry\"\n",
        "tri_2 = 'You know what'\n",
        "tri_3 = decoder_tokenizer.encode(\"I've been\")\n",
        "tri_1 = decoder_tokenizer.encode(tri_1)\n",
        "tri_2 = decoder_tokenizer.encode(tri_2)\n",
        "print(tri_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3WbJMIMbFog",
        "outputId": "3fc520a6-62c4-4473-e17b-9929e36553c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40, 1101, 7926]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzUej3kqwfr5"
      },
      "outputs": [],
      "source": [
        "def generator(model , keyword):\n",
        "  lyrics = []\n",
        "  first_line ,keyword1 , target= FS()\n",
        "  lyrics.append(first_line)\n",
        "  last_tri = None\n",
        "  phoneme = None\n",
        "  keyword = keyword_encoder_tokenizer.encode(keyword)\n",
        "  keyword = torch.LongTensor(keyword)[None,:].to(device)\n",
        "\n",
        "  print('sentence 0',first_line)\n",
        "  print('target' , decoder_tokenizer.decode(target))\n",
        "  print('target_id',target)\n",
        "  for i in range(1,4):\n",
        "    print('iiii',i)\n",
        "    last_line = lyrics[i-1]\n",
        "    if i > 1:\n",
        "      last_line = last_line[:,1:].squeeze()\n",
        "\n",
        "      if count_tri(tri_1 , last_line) >= 1:\n",
        "        last_tri = tri_1\n",
        "      # elif count_tri(tri_2 , last_line) >= 1:\n",
        "      #   last_tri = tri_2\n",
        "      \n",
        "      string = decoder_tokenizer.decode(last_line)\n",
        "      last_line = sentence_encoder_tokenizer.encode(string)\n",
        "      phoneme = sentence_vowel(last_line)\n",
        "      last_line = torch.LongTensor(last_line)[None,:].to(device)\n",
        "    else:\n",
        "      s_l_line = last_line.squeeze()\n",
        "      phoneme = sentence_vowel(s_l_line)\n",
        "    next_line = beam_search(model, last_line , keyword , phoneme , last_tri=last_tri )\n",
        "    lyrics.append(next_line)\n",
        "  for i in range(len(lyrics)):\n",
        "    sentence = lyrics[i]\n",
        "    if i == 0:\n",
        "      print(sentence_encoder_tokenizer.decode(sentence.squeeze()))\n",
        "    else:\n",
        "      print(decoder_tokenizer.decode(sentence.squeeze()))\n",
        "  return lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnps90rkcWxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510f3350-d64b-4a8e-d688-feb888697c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence 0 tensor([[ 101, 1045, 2001, 2196, 6069, 2994, 1996, 2168,  102]],\n",
            "       device='cuda:0')\n",
            "target <|endoftext|>All I ever really wanted was to change the game<|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "target_id tensor([50256,  3237,   314,  1683,  1107,  2227,   373,   284,  1487,   262,\n",
            "          983, 50256, 50256, 50256, 50256], device='cuda:0')\n",
            "iiii 1\n",
            "1 torch.Size([50, 1])\n",
            "depth 1 tensor([[50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256]], device='cuda:0')\n",
            "hidden tensor([[[ 0.1718,  0.4105, -0.4181,  ...,  0.8287, -0.5513,  0.4993],\n",
            "         [-0.6281, -0.2360,  0.2015,  ..., -0.1387,  0.5130, -0.2779],\n",
            "         [ 0.1738,  0.4111, -0.4201,  ...,  0.8270, -0.5506,  0.5015],\n",
            "         ...,\n",
            "         [ 0.1738,  0.4113, -0.4198,  ...,  0.8267, -0.5505,  0.5015],\n",
            "         [ 0.1710,  0.4110, -0.4174,  ...,  0.8294, -0.5513,  0.4983],\n",
            "         [ 0.1724,  0.4113, -0.4197,  ...,  0.8285, -0.5505,  0.5002]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0], device='cuda:0')\n",
            "topk 1 tensor([31, 30, 28, 29, 25, 24, 26, 27, 19, 18, 16, 17, 21, 20, 22, 23,  7,  6,\n",
            "         4,  5,  1,  0,  2,  3, 11, 10,  8,  9, 13, 12, 14, 15, 47, 46, 44, 45,\n",
            "        41, 40, 42, 43, 35, 34, 32, 33, 37, 36, 38, 39, 49, 48],\n",
            "       device='cuda:0')\n",
            "topk scores1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "2 torch.Size([50, 2])\n",
            "depth 2 tensor([[50256,    31],\n",
            "        [50256,    30],\n",
            "        [50256,    28],\n",
            "        [50256,    29],\n",
            "        [50256,    25],\n",
            "        [50256,    24],\n",
            "        [50256,    26],\n",
            "        [50256,    27],\n",
            "        [50256,    19],\n",
            "        [50256,    18],\n",
            "        [50256,    16],\n",
            "        [50256,    17],\n",
            "        [50256,    21],\n",
            "        [50256,    20],\n",
            "        [50256,    22],\n",
            "        [50256,    23],\n",
            "        [50256,     7],\n",
            "        [50256,     6],\n",
            "        [50256,     4],\n",
            "        [50256,     5],\n",
            "        [50256,     1],\n",
            "        [50256,     0],\n",
            "        [50256,     2],\n",
            "        [50256,     3],\n",
            "        [50256,    11],\n",
            "        [50256,    10],\n",
            "        [50256,     8],\n",
            "        [50256,     9],\n",
            "        [50256,    13],\n",
            "        [50256,    12],\n",
            "        [50256,    14],\n",
            "        [50256,    15],\n",
            "        [50256,    47],\n",
            "        [50256,    46],\n",
            "        [50256,    44],\n",
            "        [50256,    45],\n",
            "        [50256,    41],\n",
            "        [50256,    40],\n",
            "        [50256,    42],\n",
            "        [50256,    43],\n",
            "        [50256,    35],\n",
            "        [50256,    34],\n",
            "        [50256,    32],\n",
            "        [50256,    33],\n",
            "        [50256,    37],\n",
            "        [50256,    36],\n",
            "        [50256,    38],\n",
            "        [50256,    39],\n",
            "        [50256,    49],\n",
            "        [50256,    48]], device='cuda:0')\n",
            "hidden tensor([[[ 0.1718,  0.4105, -0.4181,  ...,  0.8287, -0.5513,  0.4993],\n",
            "         [-0.6281, -0.2360,  0.2015,  ..., -0.1387,  0.5130, -0.2779],\n",
            "         [ 0.1738,  0.4111, -0.4201,  ...,  0.8270, -0.5506,  0.5015],\n",
            "         ...,\n",
            "         [ 0.1738,  0.4113, -0.4198,  ...,  0.8267, -0.5505,  0.5015],\n",
            "         [ 0.1710,  0.4110, -0.4174,  ...,  0.8294, -0.5513,  0.4983],\n",
            "         [ 0.1724,  0.4113, -0.4197,  ...,  0.8285, -0.5505,  0.5002]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([17, 35, 46, 32, 43, 45, 42,  6, 13, 34, 48, 11, 36, 47, 41, 33, 12, 31,\n",
            "        31, 22, 14, 10,  5,  8,  9, 12, 38, 44, 28,  5, 15, 10, 14, 22, 26, 24,\n",
            "        15,  8, 20, 26, 34, 38,  8, 39, 39, 13,  9,  4, 23, 49],\n",
            "       device='cuda:0')\n",
            "topk 2 tensor([42323,  6950, 12375,  2433,  2007,  8505,   259,   314,   812,  2002,\n",
            "         2530,   812, 15746,   436, 10277,  1860,    13,   657,    13,  1303,\n",
            "          812,  1510,    13,   400,   812,   812,  4509, 19296,  2231,   812,\n",
            "          812,   301,    13,    16,    64,   314,    13,   812,    40,   284,\n",
            "         1689,  3605,    13,   452,  1286,    13,    13,   314,    24,  1195],\n",
            "       device='cuda:0')\n",
            "topk scores2 tensor([0.9895, 0.8580, 0.8466, 0.8445, 0.6686, 0.6363, 0.6206, 0.5659, 0.5198,\n",
            "        0.4998, 0.4657, 0.4470, 0.4322, 0.4297, 0.4272, 0.4249, 0.4058, 0.3880,\n",
            "        0.3801, 0.3793, 0.3716, 0.3504, 0.3374, 0.3186, 0.3178, 0.3163, 0.2877,\n",
            "        0.2857, 0.2708, 0.2636, 0.2627, 0.2604, 0.2474, 0.2410, 0.2366, 0.2352,\n",
            "        0.2336, 0.2333, 0.2203, 0.2183, 0.2178, 0.2154, 0.2093, 0.1951, 0.1807,\n",
            "        0.1752, 0.1731, 0.1684, 0.1608, 0.1568], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "3 torch.Size([50, 3])\n",
            "depth 3 tensor([[50256,     6, 42323],\n",
            "        [50256,    45,  6950],\n",
            "        [50256,    38, 12375],\n",
            "        [50256,    47,  2433],\n",
            "        [50256,    33,  2007],\n",
            "        [50256,    36,  8505],\n",
            "        [50256,    32,   259],\n",
            "        [50256,    26,   314],\n",
            "        [50256,    20,   812],\n",
            "        [50256,    44,  2002],\n",
            "        [50256,    49,  2530],\n",
            "        [50256,    17,   812],\n",
            "        [50256,    41, 15746],\n",
            "        [50256,    39,   436],\n",
            "        [50256,    34, 10277],\n",
            "        [50256,    46,  1860],\n",
            "        [50256,    21,    13],\n",
            "        [50256,    15,   657],\n",
            "        [50256,    15,    13],\n",
            "        [50256,     2,  1303],\n",
            "        [50256,    22,   812],\n",
            "        [50256,    16,  1510],\n",
            "        [50256,    24,    13],\n",
            "        [50256,    19,   400],\n",
            "        [50256,    18,   812],\n",
            "        [50256,    21,   812],\n",
            "        [50256,    42,  4509],\n",
            "        [50256,    37, 19296],\n",
            "        [50256,    13,  2231],\n",
            "        [50256,    24,   812],\n",
            "        [50256,    23,   812],\n",
            "        [50256,    16,   301],\n",
            "        [50256,    22,    13],\n",
            "        [50256,     2,    16],\n",
            "        [50256,     8,    64],\n",
            "        [50256,    11,   314],\n",
            "        [50256,    23,    13],\n",
            "        [50256,    19,   812],\n",
            "        [50256,     1,    40],\n",
            "        [50256,     8,   284],\n",
            "        [50256,    44,  1689],\n",
            "        [50256,    42,  3605],\n",
            "        [50256,    19,    13],\n",
            "        [50256,    43,   452],\n",
            "        [50256,    43,  1286],\n",
            "        [50256,    20,    13],\n",
            "        [50256,    18,    13],\n",
            "        [50256,    25,   314],\n",
            "        [50256,     3,    24],\n",
            "        [50256,    48,  1195]], device='cuda:0')\n",
            "hidden tensor([[[ 0.1718,  0.4105, -0.4181,  ...,  0.8287, -0.5513,  0.4993],\n",
            "         [-0.6281, -0.2360,  0.2015,  ..., -0.1387,  0.5130, -0.2779],\n",
            "         [ 0.1738,  0.4111, -0.4201,  ...,  0.8270, -0.5506,  0.5015],\n",
            "         ...,\n",
            "         [ 0.1738,  0.4113, -0.4198,  ...,  0.8267, -0.5505,  0.5015],\n",
            "         [ 0.1710,  0.4110, -0.4174,  ...,  0.8294, -0.5513,  0.4983],\n",
            "         [ 0.1724,  0.4113, -0.4197,  ...,  0.8285, -0.5505,  0.5002]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "9 torch.Size([50, 9])\n",
            "depth 9 tensor([[50256,    45,   328,  4908,    11,   345, 18959,   447,   247],\n",
            "        [50256,    44,  1436,   286,  1109,    11,   314,   447,   247],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247],\n",
            "        [50256,    19,   400,  9559,    11,   314,   447,   247,    76],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,    11],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486],\n",
            "        [50256,    44,  2002,    64,    11,   314,  1101,   257,  2802],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,  1521,   314],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   257],\n",
            "        [50256,    45,   328,  4908,    11,   345, 18959,   470, 17753],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    47,  2433,   278,   329,  1793,    11,   314,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356],\n",
            "        [50256,    44,  1436,   286,  1109,    11,   314,  1101,   407],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,   257],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717],\n",
            "        [50256,    44,  1436,   286,  1109,    11,   314,  1101,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   284],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,  1521,   345],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   284,   651],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307,   257],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314, 12472,   329],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   257,   582],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   407],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   257,  4302],\n",
            "        [50256,    47,  2433,   278,   329,  1793,    11,   314, 12472],\n",
            "        [50256,    39,  3229,  7319,  8872,  2855,    11,   314,  1101],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356],\n",
            "        [50256,    39,  3229,  7319,  8872,  2855,    11,   314,   447],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,  1949]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([18, 48,  0, 49,  1,  2, 12, 11, 40, 42, 37, 23, 19,  5, 21, 43, 36,  8,\n",
            "        20, 27, 41, 15,  9, 16, 25, 41, 39, 36, 35,  3,  4, 22,  9, 35, 24,  8,\n",
            "        45,  6, 39, 46, 27, 26, 26, 27,  7, 31, 10,  4, 47, 38],\n",
            "       device='cuda:0')\n",
            "topk 9 tensor([ 247,  247,   83, 2616,   76,   76,   69,   11,  286,   11, 2099,  286,\n",
            "         460,  257,  257,  257,  356,  616,  460, 1285, 1487,  307,  616,  460,\n",
            "         460, 2245,  262,  314,  345,  257,  257,  582,  262,  262, 1101,  262,\n",
            "         284,  257,  257,  257, 1711,  460,  765,  614, 1842, 2107,  314,  655,\n",
            "         447,  460], device='cuda:0')\n",
            "topk scores9 tensor([1.0000, 1.0000, 1.0000, 0.9945, 0.9945, 0.9758, 0.8344, 0.7617, 0.7500,\n",
            "        0.7465, 0.6851, 0.5993, 0.5574, 0.4728, 0.4210, 0.3950, 0.3808, 0.3678,\n",
            "        0.3333, 0.3181, 0.3075, 0.3073, 0.3024, 0.3011, 0.2716, 0.2715, 0.2651,\n",
            "        0.2626, 0.2554, 0.2538, 0.2440, 0.2379, 0.2224, 0.2223, 0.2199, 0.2145,\n",
            "        0.2104, 0.2089, 0.2048, 0.2041, 0.1941, 0.1853, 0.1847, 0.1794, 0.1777,\n",
            "        0.1675, 0.1670, 0.1587, 0.1584, 0.1581], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "10 torch.Size([50, 10])\n",
            "depth 10 tensor([[50256,    47,  2433,   278,   329,  1793,    11,   314,   447,   247],\n",
            "        [50256,    39,  3229,  7319,  8872,  2855,    11,   314,   447,   247],\n",
            "        [50256,    45,   328,  4908,    11,   345, 18959,   447,   247,    83],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,  1949,  2616],\n",
            "        [50256,    44,  1436,   286,  1109,    11,   314,   447,   247,    76],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247,    76],\n",
            "        [50256,    44,  2002,    64,    11,   314,  1101,   257,  2802,    69],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   257,   582,   286],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    44,  1436,   286,  1109,    11,   314,  1101,   407,   257],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   407,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   356],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   616],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487],\n",
            "        [50256,    45,   328,  4908,    11,   345, 18959,   470, 17753,   307],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   616],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  2245],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   314],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314, 12472,   329,   345],\n",
            "        [50256,    19,   400,  9559,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,   257,   582],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   262],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314, 12472,   329,   262],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   262],\n",
            "        [50256,    47,  2433,   278,   329,  1793,    11,   314, 12472,   284],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257],\n",
            "        [50256,    39,  3229,  7319,  8872,  2855,    11,   314,  1101,   257],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1711],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   765],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,   614],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   284,  2107],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,    11,   314],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([48,  0,  6,  1, 22, 17, 38, 10, 44, 43, 25, 46,  8, 32, 19, 31, 16, 20,\n",
            "        21,  2, 37, 40, 40, 19, 27, 42, 25, 35,  3, 34, 41, 35,  5,  4, 47, 30,\n",
            "        33, 43,  4, 12, 23, 18,  9, 12,  7, 26, 44, 47, 49, 10],\n",
            "       device='cuda:0')\n",
            "topk 10 tensor([  247,    76, 12603,    76,  1573,  1573,  2292,   284,   502,   286,\n",
            "          502,   447,   262,  1479,   286,   286,   460,   502,   257, 17753,\n",
            "         2802,   286,    11,    11,   460,   284,   514,  1479,   307,   257,\n",
            "          466,   661,   257,   257,   257,   582,  4453,    11,   407,   651,\n",
            "         2107,  2107,   314,  2107,   314,  3430,   514,  1949,  2107,   286],\n",
            "       device='cuda:0')\n",
            "topk scores10 tensor([1.0000, 0.9711, 0.9668, 0.9643, 0.9242, 0.9230, 0.8036, 0.7658, 0.6466,\n",
            "        0.5958, 0.5466, 0.4948, 0.4829, 0.4821, 0.4469, 0.4436, 0.4367, 0.4317,\n",
            "        0.4306, 0.4275, 0.4140, 0.4123, 0.4007, 0.3717, 0.3675, 0.3093, 0.3016,\n",
            "        0.2874, 0.2651, 0.2564, 0.2474, 0.2407, 0.2335, 0.2322, 0.2304, 0.2131,\n",
            "        0.2096, 0.2074, 0.2073, 0.1862, 0.1815, 0.1712, 0.1707, 0.1689, 0.1673,\n",
            "        0.1641, 0.1608, 0.1604, 0.1579, 0.1548], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "11 torch.Size([50, 11])\n",
            "depth 11 tensor([[50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247],\n",
            "        [50256,    47,  2433,   278,   329,  1793,    11,   314,   447,   247,\n",
            "            76],\n",
            "        [50256,    44,  2002,    64,    11,   314,  1101,   257,  2802,    69,\n",
            "         12603],\n",
            "        [50256,    39,  3229,  7319,  8872,  2855,    11,   314,   447,   247,\n",
            "            76],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   616,\n",
            "          1573],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   616,\n",
            "          1573],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           284],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,   614,\n",
            "           286],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  2245,\n",
            "           502],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,    11,   314,\n",
            "           447],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   257,   582,   286,\n",
            "           262],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "          1479],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285,\n",
            "           286],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,   257,   582,\n",
            "           286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   356,\n",
            "           460],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502],\n",
            "        [50256,    45,   328,  4908,    11,   345, 18959,   470, 17753,   307,\n",
            "           257],\n",
            "        [50256,    45,   328,  4908,    11,   345, 18959,   447,   247,    83,\n",
            "         17753],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1711,\n",
            "           286],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1711,\n",
            "            11],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285,\n",
            "            11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   314,\n",
            "           460],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   765,\n",
            "           284],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  2245,\n",
            "           514],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "          1479],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,  1949,  2616,\n",
            "           307],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "           661],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247,    76,\n",
            "           257],\n",
            "        [50256,    44,  1436,   286,  1109,    11,   314,   447,   247,    76,\n",
            "           257],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "           257],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   257,\n",
            "           582],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314, 12472,   329,   262,\n",
            "          4453],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,   614,\n",
            "            11],\n",
            "        [50256,    44,  1436,   286,  1109,    11,   314,   447,   247,    76,\n",
            "           407],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "           651],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11,\n",
            "           314],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           514],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([11, 47,  0,  6, 26, 20, 10, 35, 31, 40, 17,  5, 27, 45, 48, 15, 38, 49,\n",
            "        12,  8, 46, 25, 46,  8,  7, 41, 13, 19, 30, 34,  9, 27, 37, 49, 43, 28,\n",
            "        30,  5, 23, 42, 39,  3, 22, 42, 25, 44, 49, 44,  4, 20],\n",
            "       device='cuda:0')\n",
            "topk 11 tensor([  247,  2616,   260,   810,   422,    69,   422,   286,    11,   616,\n",
            "           11,    11,    11,    11,   616,   262,   257, 44873,   661,   290,\n",
            "           11,   466,   290,    11,   307,   674,   290,   307,   284,   582,\n",
            "          428,   290,   362,   582,   534,   257,    11,   290,   362,   447,\n",
            "          340,   257,   362,  1101,   766,  1101,  1048,   447,   290, 31699],\n",
            "       device='cuda:0')\n",
            "topk scores11 tensor([1.0000, 0.9972, 0.9595, 0.8363, 0.8346, 0.8020, 0.7556, 0.7396, 0.7250,\n",
            "        0.7046, 0.7000, 0.6237, 0.5537, 0.5111, 0.5004, 0.4848, 0.4369, 0.4367,\n",
            "        0.4243, 0.3914, 0.3874, 0.3806, 0.3352, 0.3310, 0.3224, 0.3122, 0.3021,\n",
            "        0.2929, 0.2892, 0.2708, 0.2706, 0.2700, 0.2417, 0.2328, 0.2313, 0.2295,\n",
            "        0.2259, 0.2226, 0.2175, 0.2157, 0.2129, 0.2094, 0.2022, 0.1983, 0.1944,\n",
            "        0.1924, 0.1899, 0.1888, 0.1851, 0.1790], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "12 torch.Size([50, 12])\n",
            "depth 12 tensor([[50256,    16,   301,   290,  1315,   400, 10675,  2084,    11,   314,\n",
            "           447,   247],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  2245,\n",
            "           514,   422],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802,    69],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  2245,\n",
            "           502,   422],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   257,\n",
            "           582,   286],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "           661,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   616,\n",
            "          1573,    11],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "          1479,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,   257,   582,\n",
            "           286,   262],\n",
            "        [50256,    44,  1436,   286,  1109,    11,   314,   447,   247,    76,\n",
            "           407,   257],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286, 44873],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   257,   582,   286,\n",
            "           262,   661],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,   290],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           514,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   765,\n",
            "           284,   466],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           514,   290],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           284,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "          1479,   290],\n",
            "        [50256,    45,   328,  4908,    11,   345, 18959,   447,   247,    83,\n",
            "         17753,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "           257,   582],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,   614,\n",
            "           286,   428],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "          1479,   290],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,   614,\n",
            "            11,   362],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,   582],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,  1949,  2616,\n",
            "           307,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,    11],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   616,\n",
            "          1573,   290],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285,\n",
            "            11,   362],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "           651,   340],\n",
            "        [50256,    39,  3229,  7319,  8872,  2855,    11,   314,   447,   247,\n",
            "            76,   257],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1711,\n",
            "            11,   362],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314,  1101],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   765,\n",
            "           284,   766],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11,\n",
            "           314,  1101],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11,\n",
            "           314,   447],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   616,\n",
            "          1573,   290],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([39, 47, 49,  9, 34,  5,  0, 14, 13, 15, 46, 33, 40, 25, 25, 21, 35,  7,\n",
            "        27,  3, 33, 46, 29, 10,  1, 44, 28,  8,  3,  1,  7, 44, 23, 24, 28, 12,\n",
            "        10,  2,  2,  2, 28, 19,  1,  7, 18, 20, 16, 16, 28, 36],\n",
            "       device='cuda:0')\n",
            "topk 12 tensor([  247,   247,   259,  1204,  1204, 12603,    76,  1204,   314,   661,\n",
            "          326,   326,    11,  3160,  1204,    11,   582,   262,   257,   345,\n",
            "          284,   284,   286,   314,   307,  1645,  1487,   407,   314,   651,\n",
            "          616,   345,   314,   257,   787,   407, 44873,   287,   994,  2877,\n",
            "          651,   616,  2652,  1793,   290,   290, 14971,  4302,  2245, 44873],\n",
            "       device='cuda:0')\n",
            "topk scores12 tensor([1.0000, 1.0000, 0.9963, 0.9942, 0.9908, 0.9742, 0.9089, 0.8977, 0.6936,\n",
            "        0.6214, 0.5351, 0.5213, 0.4906, 0.4699, 0.4585, 0.4582, 0.4526, 0.4373,\n",
            "        0.4320, 0.4235, 0.3776, 0.3521, 0.3337, 0.2259, 0.2255, 0.2148, 0.1981,\n",
            "        0.1912, 0.1910, 0.1789, 0.1778, 0.1756, 0.1631, 0.1481, 0.1463, 0.1412,\n",
            "        0.1289, 0.1289, 0.1274, 0.1220, 0.1213, 0.1194, 0.1174, 0.1103, 0.1058,\n",
            "        0.1040, 0.1028, 0.1026, 0.1017, 0.0997], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "13 torch.Size([50, 13])\n",
            "depth 13 tensor([[50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314,   447,   247],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11,\n",
            "           314,   447,   247],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699,   259],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802,    69, 12603],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,    11,   314,\n",
            "           447,   247,    76],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616,  1204],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   314],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,   257,   582,\n",
            "           286,   262,   661],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   326],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,   582,   326],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "           651,   340,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   765,\n",
            "           284,   466,    11],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,  1949,  2616,\n",
            "           307,   257,   582],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   257,\n",
            "           582,   286,   262],\n",
            "        [50256,    45,   328,  4908,    11,   345, 18959,   447,   247,    83,\n",
            "         17753,   307,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,   582,   284],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   284],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "           257,   582,   286],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   765,\n",
            "           284,   766,  1645],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  1487],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "           661,    11,   407],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616,   651],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   257,\n",
            "           582,   286,   616],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   765,\n",
            "           284,   766,   345],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           284,   307,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787],\n",
            "        [50256,    26,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "          1479,    11,   407],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11, 44873],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   651],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,   290,   616],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616,  2652],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   257,\n",
            "           582,   286,  1793],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   257,   582,   286,\n",
            "           262,   661,   290],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           514,    11,   290],\n",
            "        [50256,    44,  1436,   286,  1109,    11,   314,   447,   247,    76,\n",
            "           407,   257, 14971],\n",
            "        [50256,    44,  1436,   286,  1109,    11,   314,   447,   247,    76,\n",
            "           407,   257,  4302],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,    11, 44873]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([36, 49,  0,  1, 30, 39,  2, 37, 17, 19, 22, 10, 48, 37, 28, 20,  3, 32,\n",
            "        11, 41, 21, 48, 40, 16, 23, 19, 38, 26, 24,  4, 14, 28, 34, 23, 38, 22,\n",
            "        17,  7, 13, 14, 12, 42, 13,  4, 13,  6, 29, 34,  4,  3],\n",
            "       device='cuda:0')\n",
            "topk 13 tensor([ 4908,  4908,    76,    76,  1573,   287,     6,   262,   661,   821,\n",
            "          262,   345,   340,   257,  1101,   307,    11,   447,   345,  1641,\n",
            "          307,   428,   616,   290,   447,   460,   757,   616,   257,    11,\n",
            "           11,   460,   257,  1101,   284,   616,  1479,  1231,   287,   287,\n",
            "        44873,  6776,  1978,  1231,    11,   257,   616,   340,   287,   287],\n",
            "       device='cuda:0')\n",
            "topk scores13 tensor([1.0000, 1.0000, 0.9651, 0.9631, 0.9177, 0.7231, 0.6986, 0.4608, 0.4507,\n",
            "        0.4036, 0.4031, 0.3959, 0.3930, 0.3837, 0.3711, 0.3322, 0.3308, 0.3199,\n",
            "        0.3184, 0.2905, 0.2871, 0.2722, 0.2613, 0.2570, 0.2532, 0.2468, 0.2455,\n",
            "        0.2441, 0.2381, 0.2366, 0.2270, 0.2144, 0.2111, 0.2089, 0.2086, 0.2043,\n",
            "        0.2002, 0.1887, 0.1848, 0.1780, 0.1762, 0.1761, 0.1756, 0.1697, 0.1647,\n",
            "        0.1646, 0.1611, 0.1607, 0.1532, 0.1412], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "14 torch.Size([50, 14])\n",
            "depth 14 tensor([[50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,    11, 44873,  4908],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314,   447,   247,    76],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11,\n",
            "           314,   447,   247,    76],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   257,\n",
            "           582,   286,   616,  1573],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877,   287],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699,   259,     6],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   262],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   257,\n",
            "           582,   286,   262,   661],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "           257,   582,   286,   262],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   326,   345],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,  1101],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,   582,   284,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,    11],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,   447],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,   582,   326,   345],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,   290,   616,  1641],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   284,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   651,   616],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,  1949,  2616,\n",
            "           307,   257,   582,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   460],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994,   757],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  1487,   616],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616,   307,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,   460],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,  1101],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994,   284],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "           257,   582,   286,   616],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   257,\n",
            "           582,   286,   262,  1479],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616,  1204,  1231],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "           651,   340,    11, 44873],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616,  2652,  6776],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,  1231],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,    11],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,    11,   314,\n",
            "           447,   247,    76,   257],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616,   651,   616],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([40, 17, 24, 35, 34, 27,  5, 26, 10, 32, 14,  7, 21,  9, 42, 23, 19, 38,\n",
            "        28, 12, 39, 49, 48, 47, 13, 39, 13, 47, 38, 49, 29, 36, 43, 48, 33, 42,\n",
            "        30, 16, 15, 22, 41, 33, 43, 20, 48, 12, 37, 44, 11, 32],\n",
            "       device='cuda:0')\n",
            "topk 14 tensor([ 4908,   247,   247,  1573,  3613,  1204,   257,    11,   661,  3580,\n",
            "          407,   976, 23208,   407,    11,   407,    11,  4167,   582,    11,\n",
            "          257,   257,   257,  1645,  1181,  4167,  1175,   670,   257,  4167,\n",
            "        44873,   290,   257,  4167,   257,   290, 44873, 44873,   257,   661,\n",
            "          290,   407,  3252,   257,   428,   422,   340, 44873,   460,  1487],\n",
            "       device='cuda:0')\n",
            "topk scores14 tensor([1.0000, 1.0000, 1.0000, 0.9084, 0.7863, 0.7328, 0.7215, 0.6377, 0.6284,\n",
            "        0.6160, 0.6096, 0.5626, 0.5519, 0.5277, 0.4760, 0.4734, 0.4295, 0.4268,\n",
            "        0.4223, 0.4022, 0.3985, 0.3963, 0.3894, 0.3734, 0.3610, 0.3480, 0.3471,\n",
            "        0.3436, 0.3413, 0.3408, 0.3045, 0.2408, 0.2299, 0.2275, 0.2272, 0.2167,\n",
            "        0.2097, 0.2083, 0.2060, 0.1918, 0.1893, 0.1786, 0.1745, 0.1617, 0.1594,\n",
            "        0.1546, 0.1537, 0.1529, 0.1478, 0.1462], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "15 torch.Size([50, 15])\n",
            "depth 15 tensor([[50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "           651,   340,    11, 44873,  4908],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,   447,   247],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "           257,   582,   286,   616,  1573],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994,   284,  3613],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  1487,   616,  1204],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877,   287,   257],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994,   757,    11],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "           257,   582,   286,   262,   661],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,  1101,   407],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   262,   976],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821,   407],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,    11],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   655,  1949,  2616,\n",
            "           307,   257,   582,   290,   407],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,   290,   616,  1641,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,  4167],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616,   307,   257,   582],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,  1645],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257,  1181],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257,  1175],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,  4167],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,    11, 44873],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   257,\n",
            "           582,   286,   262,  1479,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,  1231,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,  1101,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,    11, 44873],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,   582,   284,   307,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   651,   616,   661],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616,  2652,  6776,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,  1101,   407],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,  1231,  3252],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   284,   307,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,   428],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616,  1204,  1231,   340],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,    11, 44873],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   326,   345,   460],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([ 0, 30, 36, 47, 24,  1,  2, 45, 42, 11, 49, 23, 13,  6,  4,  5, 41,  9,\n",
            "        29, 27, 25, 17, 33, 10, 46,  9, 27, 46, 17, 19, 25, 18, 12, 48, 32, 14,\n",
            "        21, 12, 29, 28, 26,  7, 28, 33, 20, 35, 12, 35, 26, 34],\n",
            "       device='cuda:0')\n",
            "topk 15 tensor([50256,  4908,  4908,  4908,   286,    76,    76,  5836,   286,  8848,\n",
            "          287,    11,  3142,   995,   262,    11,   257,   287,    11,    11,\n",
            "           11,    11,    11,  3142,    11,    11,   329,    13,   290, 44873,\n",
            "          290,   290,   422,  3774,  2485, 44873, 33241,    11,   290, 33241,\n",
            "         1028, 44873,  1365,   290, 33241,   307,   287,   407,    11, 14034],\n",
            "       device='cuda:0')\n",
            "topk scores15 tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.9633, 0.9576, 0.8244, 0.8225, 0.8159,\n",
            "        0.6975, 0.6965, 0.6928, 0.6115, 0.5763, 0.5531, 0.5364, 0.4757, 0.4743,\n",
            "        0.4575, 0.4163, 0.4113, 0.4047, 0.3758, 0.3512, 0.3449, 0.3415, 0.3354,\n",
            "        0.3017, 0.2886, 0.2783, 0.2713, 0.2626, 0.2234, 0.2223, 0.2156, 0.2037,\n",
            "        0.2017, 0.1975, 0.1946, 0.1894, 0.1819, 0.1717, 0.1678, 0.1673, 0.1667,\n",
            "        0.1630, 0.1605, 0.1483, 0.1295, 0.1282], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "16 torch.Size([49, 16])\n",
            "depth 16 tensor([[50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,    11, 44873,  4908],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257,  1181,   286],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,   447,   247,    76],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247,    76],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,  1231,  3252,   286],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   262,   976,  8848],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,  1645,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821,   407,  3142],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877,   287,   257,   995],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994,   284,  3613,   262],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  1487,   616,  1204,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,  1101,   407,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,  4167,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,  4167,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,  1101,   407,  3142],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616,  1204,  1231,   340,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,   329],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616,  1204,  1231,   340,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,  4167,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616,   307,   257,   582,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   326,   345,   460,  3774],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,  1231,   257,  2485],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,   257, 33241],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,  4167,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257,  1175,  1028],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994,   757,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,   257, 33241],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257,  1175,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,  1101,   257, 14034]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([49, 50257])\n",
            "prev word tensor([ 0, 34, 40, 25, 31,  3, 12, 27, 45,  9, 13, 39, 30, 29, 16, 21, 14, 44,\n",
            "        38, 36, 24, 41, 13, 17, 33, 12, 23,  6, 18,  5,  4, 43, 48, 33, 10, 19,\n",
            "        16, 42, 38, 31,  5, 32, 42, 46,  8, 46, 37,  9, 41], device='cuda:0')\n",
            "topk 16 tensor([50256,  4908,  4908,   502,  5836,  1175,   810, 22471,   616,   616,\n",
            "          995,   262,   407, 22471,   616, 44873, 44873,  1479,  1204, 44873,\n",
            "        44873,  1204,  5440, 44873,   287,   286, 44873,   757, 44873,   257,\n",
            "          257,  3592,   290,   290, 44873, 44873,  3592, 22471,  3592,  1645,\n",
            "          407,   284,  2461,   307,   326,  1309,  2461,  3592,  1295],\n",
            "       device='cuda:0')\n",
            "topk scores16 tensor([1.0000, 1.0000, 0.9996, 0.6832, 0.6102, 0.5684, 0.5353, 0.5192, 0.5182,\n",
            "        0.5112, 0.4951, 0.4862, 0.4798, 0.4547, 0.4281, 0.4225, 0.3913, 0.3878,\n",
            "        0.3863, 0.3717, 0.3584, 0.3091, 0.2985, 0.2979, 0.2872, 0.2825, 0.2806,\n",
            "        0.2748, 0.2681, 0.2567, 0.2532, 0.2490, 0.2313, 0.2298, 0.2295, 0.2258,\n",
            "        0.2105, 0.2032, 0.1973, 0.1968, 0.1953, 0.1938, 0.1893, 0.1859, 0.1822,\n",
            "        0.1820, 0.1784, 0.1679, 0.1676], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "17 torch.Size([48, 17])\n",
            "depth 17 tensor([[50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,    11, 44873,  4908],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994,   757,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,   329,   502],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257,  1181,   286,  1175],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877,   287,   257,   995,   810],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,  4167,   290, 22471],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994,   284,  3613,   262,   995],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257,  1175,  1028,   262],\n",
            "        [50256,    45,   328,  4908,    11,   314,   447,   247,    76,   655,\n",
            "          1949,  2616,   307,   257,   582,   290,   407],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290, 22471],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  1487,   616,  1204,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1204],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994,   284,  3613,   262,  5440],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,  4167,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,  1231,   257,  2485,   287],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877,   287,   257,   995,   286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616,  1204,  1231,   340,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,   257, 33241,  3592],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,  1101,   257, 14034,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,  1231,   257,  2485,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,  1645,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,  3592],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  3592],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  1645],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247,    76,   407],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   326,   345,   460,  3774,   284],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   262,   976,  8848,   326],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,  4167,   290,  2461],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,  3592],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1295]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([48, 50257])\n",
            "prev word tensor([ 0, 18, 19, 27, 33, 14, 22, 34, 25, 38,  8, 13,  7,  2,  3, 35, 17, 39,\n",
            "        12,  6, 40, 46, 20, 47, 36, 36, 47, 26,  4, 30, 20, 37, 17, 16, 30, 37,\n",
            "        41, 16,  5,  9, 42, 26, 16, 44, 29, 10, 28, 43], device='cuda:0')\n",
            "topk 17 tensor([50256,  4908,  4908,  4908,  4908,  4908,  4908,  4908,  4908,   259,\n",
            "         1204,  1204,  1204,    11,   757,    11,    11,   257,    11,    11,\n",
            "          307,    11,    11,    11,    11,   351,   621,    11,    11,    11,\n",
            "          621,    11,    13,    11,   810,   810,  3511,   757,   262,    11,\n",
            "         7787,   290,   422,   340,  4302,   995,   582,   345],\n",
            "       device='cuda:0')\n",
            "topk scores17 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        0.9999, 0.9614, 0.9397, 0.8027, 0.6125, 0.5381, 0.5163, 0.5067, 0.4824,\n",
            "        0.4777, 0.4745, 0.4431, 0.4221, 0.3857, 0.3383, 0.3197, 0.3100, 0.2883,\n",
            "        0.2882, 0.2868, 0.2800, 0.2613, 0.2523, 0.2520, 0.2445, 0.2432, 0.2401,\n",
            "        0.2394, 0.2372, 0.2134, 0.1998, 0.1718, 0.1673, 0.1611, 0.1515, 0.1486,\n",
            "        0.1439, 0.1423, 0.1401], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "18 torch.Size([47, 18])\n",
            "depth 18 tensor([[50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,  1645,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,  4167,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616,  1204,  1231,   340,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  1645,   259],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,   329,   502,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,  3592,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247,    76,   407,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290, 22471,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,  4167,   290, 22471,    11],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   326,   345,   460,  3774,   284,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,  3592,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1204,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1295,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1295,   621],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257,  1181,   286,  1175,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,   257, 33241,  3592,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1204,   621],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  3592,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,   257, 33241,  3592,   810],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  3592,   810],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877,   287,   257,   995,   810,   262],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   994,   284,  3613,   262,   995,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   422],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,   447,   247,    76,   257,  4302],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257,  1175,  1028,   262,   995],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247,    76,   257,   582],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   262,   976,  8848,   326,   345]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([47, 50257])\n",
            "prev word tensor([ 0, 39, 31, 10,  9, 11, 45, 32, 25, 36, 42, 19, 29, 26, 23, 35, 24, 13,\n",
            "        20, 14, 44,  7,  4,  1, 37,  7,  6,  7,  5,  0, 36, 13, 17, 39, 12, 15,\n",
            "        13, 42, 41,  6, 10,  6,  4, 18, 33, 16,  3], device='cuda:0')\n",
            "topk 18 tensor([50256,   284,   447,    11,    11,    11,   286, 44873,   428,    11,\n",
            "          467,   257,   428, 44873, 44873,    11,  1793,    11, 44873, 44873,\n",
            "           11,    13,    13,    13,  1230,     0,    13,    30,    13,    13,\n",
            "           13,   290, 44873,   286, 44873, 44873,    13,   307,   428,    30,\n",
            "          290,     0,     0, 44873,   645,  4302,    13], device='cuda:0')\n",
            "topk scores18 tensor([0.9535, 0.8283, 0.5206, 0.4962, 0.4959, 0.4720, 0.4716, 0.4628, 0.4542,\n",
            "        0.4502, 0.3643, 0.3252, 0.3096, 0.3041, 0.2949, 0.2887, 0.2692, 0.2659,\n",
            "        0.2631, 0.2599, 0.2299, 0.2152, 0.2075, 0.2025, 0.2023, 0.1979, 0.1966,\n",
            "        0.1950, 0.1855, 0.1847, 0.1817, 0.1811, 0.1730, 0.1700, 0.1549, 0.1480,\n",
            "        0.1465, 0.1462, 0.1452, 0.1435, 0.1392, 0.1380, 0.1285, 0.1238, 0.1225,\n",
            "        0.1224, 0.1220], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "19 torch.Size([46, 19])\n",
            "depth 19 tensor([[50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   284],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247,    76,   257,   582,   286],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1295,   621,   428],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   326,   345,   460,  3774,   284,   307,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1204,   621,   428],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,  3592,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,  3592,    11, 44873],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,   287,   257,  1175,  1028,   262,   995,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616,  1204,  1231,   340,    11, 44873,  4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,    11, 44873,  4908,    13],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877,   287,   257,   995,   810,   262,  1230],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616,  1204,  1231,   340,    11, 44873,  4908,     0],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "          2107,   616,  1204,  1231,   340,    11, 44873,  4908,    30],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,  4167,    11, 44873,  4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,    11, 44873,  4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290, 22471,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,   329,   502,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   422,   428],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873,  4908,    30],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873,  4908,     0],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,     0],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,  4167,   290, 22471,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,   257, 33241,  3592,   810,   645],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247,    76,   407,   257,  4302],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,  1645,    11, 44873,  4908,    13]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([46, 50257])\n",
            "prev word tensor([12, 17, 18, 33, 13, 31,  6, 34, 42,  1, 43, 28, 41,  8, 35, 23, 40, 38,\n",
            "        22,  2, 29, 32,  3,  4, 27, 21, 45, 14,  7, 25, 16,  9,  5, 15,  9,  4,\n",
            "         2, 37, 21, 11,  0, 11, 10,  3, 15, 37], device='cuda:0')\n",
            "topk 19 tensor([ 4908,  4908,  4908,  4908,  4908,  4908,  4908,  4908,  4908,   251,\n",
            "          530,   447,   447, 44873,   447,   318,   447,   447,   447, 44873,\n",
            "          447,   340, 44873, 44873,   447,   447,   447, 44873,    13,   447,\n",
            "        44873,   284,   616,    11,  1497,   582,   582,  3685,   921,    13,\n",
            "          910,   530,   582,   582,   447,  6181], device='cuda:0')\n",
            "topk scores19 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        0.9999, 0.7213, 0.6411, 0.6166, 0.6080, 0.5897, 0.5116, 0.4900, 0.4880,\n",
            "        0.4868, 0.4760, 0.4471, 0.4347, 0.4218, 0.4178, 0.3829, 0.3753, 0.3460,\n",
            "        0.3162, 0.3020, 0.3003, 0.2906, 0.2619, 0.2425, 0.2167, 0.2042, 0.1939,\n",
            "        0.1700, 0.1649, 0.1560, 0.1505, 0.1496, 0.1386, 0.1325, 0.1297, 0.1296,\n",
            "        0.1215], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "20 torch.Size([46, 20])\n",
            "depth 20 tensor([[50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,  3592,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,  3592,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,   329,   502,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290, 22471,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,  4167,   290, 22471,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,   257, 33241,  3592,   810,   645,   530],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,    11, 44873,  4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,     0,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    13,   447],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877,   287,   257,   995,   810,   262,  1230,   318],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873,  4908,     0,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873,  4908,    30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,    11, 44873,  4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,  4167,    11, 44873,  4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,  1645,    11, 44873,  4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1295,   621,   428,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873,  4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,   284],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247,    76,   257,   582,   286,   616],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,  1497],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11,   582],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11,   582],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   422,   428,  3685],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1204,   621,   428,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   284,   910],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1204,   621,   428,   530],\n",
            "        [50256,    45,   328,  4908,    11,   314,  1101,   407,   262,  2099,\n",
            "           286,  1048,   326,   345,   460,  3774,   284,   307,   257,   582],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11,   582],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   422,   428,  6181]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([46, 50257])\n",
            "prev word tensor([ 0, 22, 23, 30, 13, 27, 14, 11, 26, 18, 16, 12, 29, 24, 20, 17, 25, 44,\n",
            "        32, 39, 28, 31, 21,  8, 36,  7,  5,  2, 43, 34,  1, 35,  4,  7,  6,  3,\n",
            "        35,  0, 35,  4, 36,  7, 40, 15, 38,  5], device='cuda:0')\n",
            "topk 20 tensor([50256,  4908,  4908,  4908,  4908,  4908,   251,   251,   251,   251,\n",
            "          251,   251,   251,   251,   251,   251,   251,   247,  1573,   447,\n",
            "          447,  7030,    11,    30,    13,    30,    30,    13,    13,    11,\n",
            "           13,    13,    30,    13,    30,    13,     0,    13,    30,    13,\n",
            "           30,     0,  7510, 10622,   836,    13], device='cuda:0')\n",
            "topk scores20 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9999,\n",
            "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9909,\n",
            "        0.8449, 0.6863, 0.6627, 0.6175, 0.4997, 0.3874, 0.3435, 0.3410, 0.3357,\n",
            "        0.3210, 0.3203, 0.2870, 0.2722, 0.2625, 0.2285, 0.2211, 0.2206, 0.2000,\n",
            "        0.1953, 0.1922, 0.1819, 0.1728, 0.1703, 0.1702, 0.1524, 0.1470, 0.1452,\n",
            "        0.1426], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "21 torch.Size([45, 21])\n",
            "depth 21 tensor([[50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11, 44873,\n",
            "          4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    11, 44873,\n",
            "          4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873,\n",
            "          4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11, 44873,\n",
            "          4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    13,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,    11, 44873,  4908,    13,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,  1645,    11, 44873,  4908,    13,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,    11, 44873,  4908,    13,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873,  4908,     0,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,     0,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873,  4908,    13,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   314,   460,\n",
            "          2107,   616,  1204,   287,  4167,    11, 44873,  4908,    13,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    13,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,    11, 44873,  4908,    30,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   447,\n",
            "           251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247,    76,   257,   582,   286,   616,\n",
            "          1573],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1204,   621,   428,    13,\n",
            "           447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1295,   621,   428,    13,\n",
            "           447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,   284,\n",
            "          7030],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            30],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11,   582,\n",
            "            13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908,\n",
            "            30],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            30],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,  3592,    11, 44873,  4908,\n",
            "            13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11,   582,\n",
            "            13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,  1497,\n",
            "            11],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,  3592,    11, 44873,  4908,\n",
            "            13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11,   582,\n",
            "            13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            30],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908,\n",
            "            13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,    11, 44873,  4908,\n",
            "            30],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,   329,   502,    11, 44873,  4908,\n",
            "            13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11,   582,\n",
            "             0],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908,\n",
            "            13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11,   582,\n",
            "            30],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11,   582,\n",
            "            30],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908,\n",
            "             0],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   284,   910,\n",
            "          7510],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877,   287,   257,   995,   810,   262,  1230,   318,\n",
            "         10622],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([ 0, 19, 16, 35, 40, 37, 23, 26, 39, 24, 32, 30, 31, 22, 43, 33, 25, 38,\n",
            "        44, 27, 29, 34, 43, 20,  3, 28,  4,  0,  1, 21,  2,  1, 17,  2, 36, 42,\n",
            "         3,  0, 21,  4,  1,  3, 38, 15, 36], device='cuda:0')\n",
            "topk 21 tensor([50256,   251,    82,   447,   447,   447,   447,   447,   447,   447,\n",
            "          447,   447,   447,   447,   447,   447,   447,   447,   447,   447,\n",
            "          447,   447,   470,    11,    30, 44873,    13,    13,    30, 44873,\n",
            "            0,    13,   290,    13,   447,    11,     0,    30,   645,     0,\n",
            "            0,    13,   921,   921,  2094], device='cuda:0')\n",
            "topk scores21 tensor([0.9999, 0.9999, 0.9997, 0.7620, 0.7045, 0.6656, 0.6564, 0.6527, 0.6463,\n",
            "        0.6153, 0.6042, 0.5948, 0.5865, 0.5675, 0.5664, 0.5506, 0.5405, 0.5154,\n",
            "        0.5032, 0.4738, 0.4643, 0.4368, 0.4336, 0.4023, 0.3740, 0.3723, 0.3586,\n",
            "        0.3239, 0.3088, 0.2860, 0.2692, 0.2605, 0.2532, 0.2474, 0.2445, 0.2227,\n",
            "        0.1920, 0.1685, 0.1490, 0.1296, 0.1276, 0.1164, 0.1158, 0.1063, 0.0998],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "22 torch.Size([44, 22])\n",
            "depth 22 tensor([[50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257,  1365,  1295,   621,   428,    13,\n",
            "           447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247,    82],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11,   582,\n",
            "             0,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908,\n",
            "             0,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11,   582,\n",
            "            30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11,   582,\n",
            "            13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,  3592,    11, 44873,  4908,\n",
            "            13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11,   582,\n",
            "            30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908,\n",
            "            30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908,\n",
            "            13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11,   582,\n",
            "            13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,    11, 44873,  4908,\n",
            "            30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11,   582,\n",
            "            13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,  3592,    11, 44873,  4908,\n",
            "            13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,   329,   502,    11, 44873,  4908,\n",
            "            13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,   284,\n",
            "          7030,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873,\n",
            "          4908,    30],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,  1497,\n",
            "            11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11, 44873,\n",
            "          4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11, 44873,\n",
            "          4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,    30],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    11, 44873,\n",
            "          4908,     0],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   460,  1487,\n",
            "           502,    11,   314,   447,   247,    76,   257,   582,   286,   616,\n",
            "          1573,   290],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    11, 44873,\n",
            "          4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908,\n",
            "            13,   447],\n",
            "        [50256,    16,   301,   290,  1315,   400, 10675,  2084,   356,   447,\n",
            "           247,   260,  2877,   287,   257,   995,   810,   262,  1230,   318,\n",
            "         10622,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873,\n",
            "          4908,     0],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11, 44873,\n",
            "          4908,    30],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11, 44873,\n",
            "          4908,     0],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,     0],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873,\n",
            "          4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   921],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   447,\n",
            "           251,   921],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908,\n",
            "            13,  2094]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([44, 50257])\n",
            "prev word tensor([24,  0, 13, 20,  6, 19,  5, 33, 18,  7, 10,  3,  4,  2, 12, 14, 17,  8,\n",
            "        15,  9, 16, 11, 29, 39, 27, 38, 30, 35, 43, 36, 26, 32, 23, 40, 25, 21,\n",
            "        43, 22, 37, 37, 21, 41, 21,  1], device='cuda:0')\n",
            "topk 22 tensor([ 4908, 50256,   247,   251,   251,   251,   251,   251,   251,   251,\n",
            "          251,   251,   251,   251,   251,   251,   251,   251,   251,   251,\n",
            "          251,   251,   447,   447,   447,   447,   447,   447,   470,   447,\n",
            "          447,   447,   447,   447,   447,   761,   447, 44873,  2300,   517,\n",
            "          765,   836,   423,  1410], device='cuda:0')\n",
            "topk scores22 tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
            "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
            "        0.9999, 0.9999, 0.9998, 0.9998, 0.8776, 0.8670, 0.7886, 0.7377, 0.7368,\n",
            "        0.7346, 0.7113, 0.5744, 0.5681, 0.5591, 0.5509, 0.5312, 0.4879, 0.4726,\n",
            "        0.2887, 0.2802, 0.2307, 0.2274, 0.2112, 0.1455, 0.1450, 0.1353],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "23 torch.Size([43, 23])\n",
            "depth 23 tensor([[50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,  1497,\n",
            "            11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   447,   247],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,   329,   502,    11, 44873,  4908,\n",
            "            13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,  3592,    11, 44873,  4908,\n",
            "            13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,  3592,    11, 44873,  4908,\n",
            "            13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11,   582,\n",
            "            13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908,\n",
            "            13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11,   582,\n",
            "            13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11,   582,\n",
            "            30,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11,   582,\n",
            "            13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908,\n",
            "             0,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11,   582,\n",
            "            30,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11,   582,\n",
            "             0,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            30,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,    11, 44873,  4908,\n",
            "            30,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908,\n",
            "            30,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            30,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908,\n",
            "            13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            30,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    11, 44873,\n",
            "          4908,     0,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,     0,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,    30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11, 44873,\n",
            "          4908,     0,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873,\n",
            "          4908,     0,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908,\n",
            "            13,  2094,   470],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11, 44873,\n",
            "          4908,    30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11, 44873,\n",
            "          4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    11, 44873,\n",
            "          4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873,\n",
            "          4908,    30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873,\n",
            "          4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11, 44873,\n",
            "          4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   761],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908,\n",
            "            13,  2094,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,   284,\n",
            "          7030,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,  2300],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,   517],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   765],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   921,   836],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   423],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247,    82,  1410]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([43, 50257])\n",
            "prev word tensor([ 0, 36,  1, 21, 30, 29, 28, 22, 25, 23, 26, 32, 31, 24, 33, 41, 39, 40,\n",
            "        42, 34, 37, 40, 37, 27, 38, 34,  0,  0,  0, 37, 38, 42, 19, 27, 34, 39,\n",
            "        42, 10,  2,  7,  5,  3, 18], device='cuda:0')\n",
            "topk 23 tensor([50256,  4908,    83,   251,   251,   251,   251,   251,   251,   251,\n",
            "          251,   251,   251,   251,   251,   284,   284,   447,   284,   284,\n",
            "          644,   470,   703,  1309,  3252,   257,    13,     0,    30,   508,\n",
            "         7363,    11,   921,   307,   645,   257,   329,   784,   960,  1320,\n",
            "         1320,   960, 50256], device='cuda:0')\n",
            "topk scores23 tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
            "        0.9999, 0.9999, 0.9999, 0.9998, 0.9998, 0.9998, 0.9383, 0.6151, 0.5784,\n",
            "        0.5713, 0.5274, 0.4792, 0.4216, 0.3068, 0.2912, 0.2901, 0.2388, 0.2190,\n",
            "        0.2005, 0.1882, 0.1430, 0.1354, 0.1124, 0.1123, 0.1075, 0.0993, 0.0911,\n",
            "        0.0905, 0.0863, 0.0795, 0.0765, 0.0739, 0.0723, 0.0718],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "24 torch.Size([41, 24])\n",
            "depth 24 tensor([[50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,   284,\n",
            "          7030,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   447,   247,    83],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    11, 44873,\n",
            "          4908,     0,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   422,  5836,   757,    11, 44873,\n",
            "          4908,    13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11, 44873,\n",
            "          4908,    13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11, 44873,\n",
            "          4908,    30,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,     0,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,    13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,    30,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873,\n",
            "          4908,     0,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873,\n",
            "          4908,    13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   307,  1479,   757,    11, 44873,\n",
            "          4908,    30,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11, 44873,\n",
            "          4908,     0,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11, 44873,\n",
            "          4908,    13,   447,   251],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   423,   284],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   765,   284],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   921,   836,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247,    82,  1410,   284],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   761,   284],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,  2300,   644],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   921,   836,   470],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,  2300,   703],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908,\n",
            "            13,  2094,   470,  1309],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,   517,  3252],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   761,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,  1497,\n",
            "            11, 44873,  4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,  1497,\n",
            "            11, 44873,  4908,     0],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,  1497,\n",
            "            11, 44873,  4908,    30],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,  2300,   508],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,   517,  7363],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247,    82,  1410,    11],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   447,   251,   921],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908,\n",
            "            13,  2094,   470,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   761,   645],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   765,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247,    82,  1410,   329],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,   287,   257, 33241,  1204,    11, 44873,  4908,\n",
            "             0,   447,   251,   784],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   340,   670,   329,   502,    11, 44873,  4908,\n",
            "            13,   447,   251,   960],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11,   582,\n",
            "            13,   447,   251,  1320],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11,   582,\n",
            "            13,   447,   251,  1320],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,  3592,    11, 44873,  4908,\n",
            "            13,   447,   251,   960]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([41, 50257])\n",
            "prev word tensor([ 0, 26, 20, 27, 25,  1, 32, 28, 18,  1, 24, 15,  0, 30,  0, 32, 28,  0,\n",
            "        14, 20, 34, 19, 38, 31, 13, 20, 17, 39, 34, 35,  1, 35, 17, 21, 19, 31,\n",
            "        21,  6,  6,  3, 12], device='cuda:0')\n",
            "topk 24 tensor([50256,   447,   761,   447,   447,   761,  7787,   345,   307,   765,\n",
            "         2485,   307,    30, 44873,     0, 12008,   338,    13,   307,   765,\n",
            "         1175,   345,   338,   836,   921,   423,   787,   338,   582,   257,\n",
            "          423, 12157,  1494,   881,  4325,  1365,   867,   784,   960, 50256,\n",
            "          921], device='cuda:0')\n",
            "topk scores24 tensor([1.0000, 0.7046, 0.5419, 0.5087, 0.4811, 0.4734, 0.3940, 0.2931, 0.2516,\n",
            "        0.2504, 0.2466, 0.2428, 0.2352, 0.2303, 0.2287, 0.2272, 0.2222, 0.2056,\n",
            "        0.1880, 0.1860, 0.1654, 0.1585, 0.1419, 0.1365, 0.1330, 0.1261, 0.1239,\n",
            "        0.1221, 0.1134, 0.1080, 0.1074, 0.1061, 0.0972, 0.0959, 0.0947, 0.0942,\n",
            "        0.0914, 0.0907, 0.0884, 0.0865, 0.0863], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "25 torch.Size([39, 25])\n",
            "depth 25 tensor([[50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,  1497,\n",
            "            11, 44873,  4908,     0,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   921,   836,   470,   761],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,  1497,\n",
            "            11, 44873,  4908,    30,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,  1497,\n",
            "            11, 44873,  4908,    13,   447],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   447,   247,    83,   761],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908,\n",
            "            13,  2094,   470,   307,  7787],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,  2300,   508,   345],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   761,   284,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   447,   247,    83,   765],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   761,   257,  2485],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   765,   284,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,   284,\n",
            "          7030,    11, 44873,  4908,    30],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247,    82,  1410,    11, 44873],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,   284,\n",
            "          7030,    11, 44873,  4908,     0],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   340,   422,  5836,   757,    11, 44873,  4908,\n",
            "            13,  2094,   470,   307, 12008],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,  2300,   508,   338],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,  1309,   340,   467,   284,\n",
            "          7030,    11, 44873,  4908,    13],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   423,   284,   307],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   921,   836,   470,   765],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   765,   257,  1175],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,  2300,   644,   345],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  1487,   287,   616,  1204,    11,   582,\n",
            "            13,   447,   251,  1320,   338],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   447,   251,   921,   836],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11, 44873,\n",
            "          4908,    13,   447,   251,   921],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   921,   836,   470,   423],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247,    82,  1410,   284,   787],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,   787,   257,  3580,   287,   616,  1204,    11,   582,\n",
            "            13,   447,   251,  1320,   338],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   470,   765,   257,   582],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247,    82,  1410,   329,   257],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,    11, 44873,  4908,    13,   921,\n",
            "           836,   447,   247,    83,   423],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247,    82,  1410,   329, 12157],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,   351,  1793,   447,\n",
            "           247,    82,  1410,   284,  1494],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,  2300,   703,   881],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,  2300,   644,  4325],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290, 22471,    11, 44873,  4908,\n",
            "            13,   447,   251,   921,  1365],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   356,   460,\n",
            "          2107,   674,  3160,  1978,   290,   407,   307,  7787,   286,   340,\n",
            "            11,   645,  2300,   703,   867],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,     0,   447,   251,   784],\n",
            "        [50256,    35,  3008,   318,   262,   691,  1517,   326,   314,   460,\n",
            "           466,   284,  2245,   428, 23208,   287,   616,  1204,    11, 44873,\n",
            "          4908,     0,   447,   251,   960],\n",
            "        [50256,    35,  3008,   318,   262,   691,   835,   326,   345,   460,\n",
            "          2107,   534,  1204,   287,  4167,   290,  2461,  3511,    11, 44873,\n",
            "          4908,     0,   447,   251,   921]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([39, 50257])\n",
            "prev word tensor([12,  0,  3,  2, 29, 24, 14, 13, 27, 16,  8, 18, 11, 20, 17, 33,  4,  9,\n",
            "         6,  1, 26, 21, 15,  7, 28, 34,  5, 10, 19,  1, 35,  4, 15, 28, 32, 35,\n",
            "        20, 23, 38], device='cuda:0')\n",
            "topk 25 tensor([ 4908,   251,   251,   251,   284,   284,   286,   447,   284,   447,\n",
            "          284,   284,   447,   910,   257,   284,   284,   284,   389,   284,\n",
            "         1521,  1521,   287,   257,  1365,   407,   286,   257,   351,   257,\n",
            "         1661,   257,  2157, 17717,   345,  7363,   466,   836,  1365],\n",
            "       device='cuda:0')\n",
            "topk scores25 tensor([1.0000, 0.9999, 0.9999, 0.9999, 0.9561, 0.9492, 0.8400, 0.7834, 0.6634,\n",
            "        0.6264, 0.6171, 0.6056, 0.6006, 0.5789, 0.5690, 0.5664, 0.5457, 0.5451,\n",
            "        0.5284, 0.4873, 0.4846, 0.4679, 0.4482, 0.4393, 0.3933, 0.3617, 0.3516,\n",
            "        0.2947, 0.2755, 0.2401, 0.2385, 0.2369, 0.2349, 0.2268, 0.2050, 0.1730,\n",
            "        0.1602, 0.1577, 0.1551], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "26 torch.Size([39, 26])\n",
            "depth 26 tensor([[50256,    35,  3008,  ...,    11, 44873,  4908],\n",
            "        [50256,    35,  3008,  ...,     0,   447,   251],\n",
            "        [50256,    35,  3008,  ...,    13,   447,   251],\n",
            "        ...,\n",
            "        [50256,    35,  3008,  ...,   644,   345,   466],\n",
            "        [50256,    35,  3008,  ...,   251,   921,   836],\n",
            "        [50256,    35,  3008,  ...,   251,   921,  1365]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([39, 50257])\n",
            "prev word tensor([ 0,  9, 12, 33, 15, 36, 38, 19, 11, 16, 22, 27, 31, 22,  0, 10,  5, 35,\n",
            "        15,  0,  4, 35, 29, 25, 25, 23, 13, 17,  0, 14, 34, 36,  8, 27, 27, 14,\n",
            "        22, 17, 23], device='cuda:0')\n",
            "topk 26 tensor([50256,   251,   251,   259,   514,   284,   407,   307,   307,   307,\n",
            "         2607, 11778,  2485,  1176,    13,   307,   307,   345,   345,    30,\n",
            "          307,   484,  2485,   307,  1309, 11778,   284,  1494,     0,  4302,\n",
            "         5465,   393,   307,   636, 47641, 11778,  1630,   307,   636],\n",
            "       device='cuda:0')\n",
            "topk scores26 tensor([0.9999, 0.9999, 0.9999, 0.9995, 0.6293, 0.4289, 0.3346, 0.3007, 0.2990,\n",
            "        0.2555, 0.2518, 0.2485, 0.2451, 0.2399, 0.2338, 0.2320, 0.2301, 0.2219,\n",
            "        0.2174, 0.2062, 0.1871, 0.1675, 0.1656, 0.1630, 0.1548, 0.1474, 0.1456,\n",
            "        0.1441, 0.1431, 0.1287, 0.1264, 0.1262, 0.1165, 0.1143, 0.1108, 0.1078,\n",
            "        0.1046, 0.1023, 0.1007], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "27 torch.Size([38, 27])\n",
            "depth 27 tensor([[50256,    35,  3008,  ...,    13,   447,   251],\n",
            "        [50256,    35,  3008,  ...,    30,   447,   251],\n",
            "        [50256,    35,  3008,  ...,   257, 17717,   259],\n",
            "        ...,\n",
            "        [50256,    35,  3008,  ...,   338,   287,  1630],\n",
            "        [50256,    35,  3008,  ...,  2485,   284,   307],\n",
            "        [50256,    35,  3008,  ...,   307,   257,   636]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([38, 50257])\n",
            "prev word tensor([32,  0,  2, 25, 15, 19, 11, 21,  6, 23,  8, 26, 20, 30,  7, 20, 14, 26,\n",
            "        22, 16, 16,  5, 16,  4, 16,  5,  4, 12,  9, 20, 30, 13, 20, 18, 25, 22,\n",
            "         0,  0], device='cuda:0')\n",
            "topk 27 tensor([  286, 50256,     6,   502,   257,   257,   284,   284,   257,   340,\n",
            "          257,   502,  6486,   910,   257,   910,   257,  3511,   257,   910,\n",
            "         6486,  1309,  1560,   502,   787,   307,   514,   393,   393,   787,\n",
            "          508,   921,  1560,   921,   514,  7787, 50256,  2094],\n",
            "       device='cuda:0')\n",
            "topk scores27 tensor([0.9999, 0.9999, 0.9030, 0.7992, 0.6181, 0.5620, 0.5328, 0.5309, 0.4793,\n",
            "        0.4432, 0.4317, 0.3858, 0.3658, 0.3314, 0.3172, 0.3132, 0.3079, 0.3035,\n",
            "        0.2997, 0.2847, 0.2529, 0.1930, 0.1613, 0.1499, 0.1454, 0.1396, 0.1194,\n",
            "        0.1154, 0.1143, 0.1080, 0.1050, 0.1049, 0.1039, 0.0842, 0.0764, 0.0742,\n",
            "        0.0738, 0.0717], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "28 torch.Size([36, 28])\n",
            "depth 28 tensor([[50256,    35,  3008,  ...,   257,   636,   286],\n",
            "        [50256,    35,  3008,  ..., 17717,   259,     6],\n",
            "        [50256,    35,  3008,  ...,   910,   284,   502],\n",
            "        ...,\n",
            "        [50256,    35,  3008,  ...,   910,   284,   514],\n",
            "        [50256,    35,  3008,  ...,   407,   307,  7787],\n",
            "        [50256,    35,  3008,  ...,   447,   251,  2094]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([36, 50257])\n",
            "prev word tensor([31, 19, 34, 11, 21,  8, 28, 20, 24, 34, 11, 13, 15, 28,  0, 21,  8, 17,\n",
            "        30,  7,  5,  6,  6,  9,  0, 32,  3,  3, 15,  4, 21, 32, 18, 19,  5, 15],\n",
            "       device='cuda:0')\n",
            "topk 28 tensor([  514,   284,   284,   284,   514,   467,   345,   340,   257,   286,\n",
            "          546, 11778, 11778,   514,   340,   502,   307,  9192,   836, 11778,\n",
            "         1494,  1494,   307, 11778,   428,  1365, 11778,  4302, 47641,  4302,\n",
            "          705,   836,   484,   546,   307,   636], device='cuda:0')\n",
            "topk scores28 tensor([0.8992, 0.8510, 0.6997, 0.6565, 0.5842, 0.4900, 0.3625, 0.3555, 0.2865,\n",
            "        0.2861, 0.2715, 0.2663, 0.2430, 0.2232, 0.2197, 0.2009, 0.1888, 0.1675,\n",
            "        0.1625, 0.1624, 0.1523, 0.1479, 0.1382, 0.1378, 0.1331, 0.1282, 0.1264,\n",
            "        0.1258, 0.1212, 0.1194, 0.1159, 0.1122, 0.1114, 0.1099, 0.1088, 0.1056],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "29 torch.Size([36, 29])\n",
            "depth 29 tensor([[50256,    35,  3008,  ...,   484,  1560,   514],\n",
            "        [50256,    35,  3008,  ...,   345,  6486,   284],\n",
            "        [50256,    35,  3008,  ...,   307,  7787,   284],\n",
            "        ...,\n",
            "        [50256,    35,  3008,  ...,   345,  6486,   546],\n",
            "        [50256,    35,  3008,  ...,  2485,   284,   307],\n",
            "        [50256,    35,  3008,  ...,   307,   257,   636]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([36, 50257])\n",
            "prev word tensor([ 0, 30,  3, 15,  7,  0, 21,  4, 20,  1, 24,  1, 25,  5, 13, 20,  6, 10,\n",
            "        21,  9, 13,  3, 32,  8, 16,  4,  7,  0,  6,  9,  6,  8,  2, 13,  8,  6],\n",
            "       device='cuda:0')\n",
            "topk 29 tensor([50256,   368,   514,   284,   467,   284,   502,   284,   502,   514,\n",
            "         7510,   502,   407,   284,  1975,  3511,  6486,   514,  3511,   340,\n",
            "          466,   502,  6486,  9192,   257,   546,   307,   546,   910,   644,\n",
            "        26633, 26769,   467, 26633, 47641,  1975], device='cuda:0')\n",
            "topk scores29 tensor([0.9999, 0.9997, 0.7814, 0.5348, 0.5342, 0.5263, 0.4516, 0.4168, 0.4167,\n",
            "        0.3999, 0.3790, 0.3699, 0.3392, 0.3136, 0.3019, 0.2925, 0.2394, 0.2266,\n",
            "        0.2238, 0.2164, 0.2113, 0.1793, 0.1786, 0.1707, 0.1673, 0.1569, 0.1491,\n",
            "        0.1399, 0.1321, 0.1315, 0.1257, 0.1243, 0.1120, 0.1114, 0.1081, 0.0914],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "30 torch.Size([35, 30])\n",
            "depth 30 tensor([[50256,    35,  3008,  ...,  1560,   705,   368],\n",
            "        [50256,    35,  3008,  ...,  6486,   284,   514],\n",
            "        [50256,    35,  3008,  ...,  1560,   502,   284],\n",
            "        ...,\n",
            "        [50256,    35,  3008,  ...,   787,   514, 26633],\n",
            "        [50256,    35,  3008,  ...,   307,   257, 47641],\n",
            "        [50256,    35,  3008,  ...,   787,   345,  1975]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([35, 50257])\n",
            "prev word tensor([ 0,  6,  2, 13, 15, 34, 21, 12,  0,  3, 23, 21, 11, 25, 31, 15, 23, 19,\n",
            "        31, 11, 27, 20, 11, 33, 33, 16,  9,  3, 23, 30, 11, 22, 19, 23,  0],\n",
            "       device='cuda:0')\n",
            "topk 30 tensor([50256,  6486,  6486,   287,   284,   287,   284,  7030,   284,   284,\n",
            "          636,   546,  1309,   257,   503,   546,  6486,   284,  1363,   307,\n",
            "          284,    13,   467,   393,   618,    13,  7471,  1231,  7813,   618,\n",
            "         2666,   618,   393,  4065,   546], device='cuda:0')\n",
            "topk scores30 tensor([0.9169, 0.9080, 0.8321, 0.8318, 0.7651, 0.7597, 0.7490, 0.6403, 0.4331,\n",
            "        0.3479, 0.3421, 0.2029, 0.1976, 0.1798, 0.1676, 0.1648, 0.1605, 0.1522,\n",
            "        0.1364, 0.1117, 0.0819, 0.0780, 0.0774, 0.0755, 0.0738, 0.0594, 0.0578,\n",
            "        0.0557, 0.0556, 0.0547, 0.0529, 0.0520, 0.0482, 0.0478, 0.0476],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "31 torch.Size([34, 31])\n",
            "depth 31 tensor([[50256,    35,  3008,  ...,   514,   284,  6486],\n",
            "        [50256,    35,  3008,  ...,   502,   284,  6486],\n",
            "        [50256,    35,  3008,  ...,   514,  1975,   287],\n",
            "        ...,\n",
            "        [50256,    35,  3008,  ...,   514,   466,   393],\n",
            "        [50256,    35,  3008,  ...,   307,   257,  4065],\n",
            "        [50256,    35,  3008,  ...,   705,   368,   546]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2301,  0.3664, -0.4415,  ...,  0.7985, -0.5204,  0.5531],\n",
            "         [ 0.2285,  0.3656, -0.4398,  ...,  0.8009, -0.5208,  0.5512],\n",
            "         [ 0.2316,  0.3663, -0.4415,  ...,  0.7965, -0.5210,  0.5541],\n",
            "         ...,\n",
            "         [ 0.2285,  0.3660, -0.4407,  ...,  0.8031, -0.5196,  0.5511],\n",
            "         [ 0.2277,  0.3658, -0.4403,  ...,  0.8045, -0.5195,  0.5503],\n",
            "         [ 0.2293,  0.3667, -0.4425,  ...,  0.8007, -0.5195,  0.5528]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([34, 50257])\n",
            "prev word tensor([ 0,  7, 19,  5,  8, 21, 12, 11, 24,  3, 29, 13, 10, 18,  3, 20,  5, 26,\n",
            "        31, 19, 15,  0,  1, 12, 14, 11, 20,  8, 27, 18, 17, 31, 29, 14],\n",
            "       device='cuda:0')\n",
            "topk 31 tensor([50256,  6486,   502,   514,  7030,   284,   636,   340,   447,   514,\n",
            "          340,   286,   514,   257,   502,   447,   502,   257,   910,   514,\n",
            "          284,   546,   546,  6486,    30,   683,   314,  1175,   284,   523,\n",
            "         1231,  4656,   428,   514], device='cuda:0')\n",
            "topk scores31 tensor([0.9999, 0.9102, 0.6959, 0.6333, 0.4911, 0.4701, 0.4158, 0.4078, 0.4075,\n",
            "        0.3778, 0.3351, 0.3321, 0.3120, 0.3097, 0.3055, 0.3001, 0.2505, 0.2457,\n",
            "        0.2383, 0.1920, 0.1832, 0.1607, 0.1359, 0.1286, 0.1119, 0.0864, 0.0822,\n",
            "        0.0802, 0.0771, 0.0742, 0.0714, 0.0714, 0.0689, 0.0685],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "ppppp tensor([[6, 9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.189654742026425\n",
            "ppppp tensor([[3, 5]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.58351893845611\n",
            "ppppp tensor([[6, 7]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.9889840465642745\n",
            "ppppp tensor([[1, 2]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 2.6390573296152584\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.6888794541139363\n",
            "ppppp tensor([[5, 5]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.6888794541139363\n",
            "ppppp tensor([[8, 1]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.091042453358316\n",
            "ppppp tensor([[2, 8]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.9512437185814275\n",
            "ppppp tensor([[8, 5]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.828641396489095\n",
            "ppppp tensor([[14,  8]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.330733340286331\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.6888794541139363\n",
            "ppppp tensor([[0, 1]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 1.791759469228055\n",
            "ppppp tensor([[14,  8]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.330733340286331\n",
            "ppppp tensor([[14,  8]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.330733340286331\n",
            "ppppp tensor([[8, 6]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.9512437185814275\n",
            "ppppp tensor([[14,  6]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.1588830833596715\n",
            "ppppp tensor([[ 2, 14]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.477336814478207\n",
            "iiii 3\n",
            "1 torch.Size([50, 1])\n",
            "depth 1 tensor([[50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0], device='cuda:0')\n",
            "topk 1 tensor([31, 30, 28, 29, 25, 24, 26, 27, 19, 18, 16, 17, 21, 20, 22, 23,  7,  6,\n",
            "         4,  5,  1,  0,  2,  3, 11, 10,  8,  9, 13, 12, 14, 15, 47, 46, 44, 45,\n",
            "        41, 40, 42, 43, 35, 34, 32, 33, 37, 36, 38, 39, 49, 48],\n",
            "       device='cuda:0')\n",
            "topk scores1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "2 torch.Size([50, 2])\n",
            "depth 2 tensor([[50256,    31],\n",
            "        [50256,    30],\n",
            "        [50256,    28],\n",
            "        [50256,    29],\n",
            "        [50256,    25],\n",
            "        [50256,    24],\n",
            "        [50256,    26],\n",
            "        [50256,    27],\n",
            "        [50256,    19],\n",
            "        [50256,    18],\n",
            "        [50256,    16],\n",
            "        [50256,    17],\n",
            "        [50256,    21],\n",
            "        [50256,    20],\n",
            "        [50256,    22],\n",
            "        [50256,    23],\n",
            "        [50256,     7],\n",
            "        [50256,     6],\n",
            "        [50256,     4],\n",
            "        [50256,     5],\n",
            "        [50256,     1],\n",
            "        [50256,     0],\n",
            "        [50256,     2],\n",
            "        [50256,     3],\n",
            "        [50256,    11],\n",
            "        [50256,    10],\n",
            "        [50256,     8],\n",
            "        [50256,     9],\n",
            "        [50256,    13],\n",
            "        [50256,    12],\n",
            "        [50256,    14],\n",
            "        [50256,    15],\n",
            "        [50256,    47],\n",
            "        [50256,    46],\n",
            "        [50256,    44],\n",
            "        [50256,    45],\n",
            "        [50256,    41],\n",
            "        [50256,    40],\n",
            "        [50256,    42],\n",
            "        [50256,    43],\n",
            "        [50256,    35],\n",
            "        [50256,    34],\n",
            "        [50256,    32],\n",
            "        [50256,    33],\n",
            "        [50256,    37],\n",
            "        [50256,    36],\n",
            "        [50256,    38],\n",
            "        [50256,    39],\n",
            "        [50256,    49],\n",
            "        [50256,    48]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([17, 35, 45, 32, 43, 46, 12,  5, 36, 14, 33, 42, 31, 47, 15, 13,  8, 38,\n",
            "        34, 48, 22, 41, 31, 10,  6,  9, 26, 39, 10,  8, 34, 44, 11, 22, 28, 34,\n",
            "        40, 49, 11, 13, 46, 10,  5, 39,  9, 33, 39,  9, 36, 20],\n",
            "       device='cuda:0')\n",
            "topk 2 tensor([42323,  6950,  8505,  2433,  2007, 12375,    13,    13, 15746,    13,\n",
            "         1860,   259,    13,   436,    13,    13,    13,  4509,  2002,  2530,\n",
            "         1303, 10277,   657,  1510,   314,    13,   284,   452,    13,   400,\n",
            "          432, 19296,    13,    16,  2231,  1689,  3008,    25,   812,   812,\n",
            "         3087,   301,    14, 50192,    12,  1219,  2135,   812,   603,    40],\n",
            "       device='cuda:0')\n",
            "topk scores2 tensor([0.9896, 0.8485, 0.7143, 0.6660, 0.6456, 0.6429, 0.6272, 0.5049, 0.5008,\n",
            "        0.4895, 0.4688, 0.4672, 0.4614, 0.4071, 0.3680, 0.3629, 0.3551, 0.3486,\n",
            "        0.3236, 0.3201, 0.3120, 0.2895, 0.2797, 0.2765, 0.2613, 0.2529, 0.2509,\n",
            "        0.2323, 0.2301, 0.2297, 0.2285, 0.2255, 0.2089, 0.2082, 0.2069, 0.1918,\n",
            "        0.1848, 0.1769, 0.1590, 0.1572, 0.1564, 0.1547, 0.1528, 0.1466, 0.1459,\n",
            "        0.1348, 0.1326, 0.1326, 0.1304, 0.1290], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "3 torch.Size([50, 3])\n",
            "depth 3 tensor([[50256,     6, 42323],\n",
            "        [50256,    45,  6950],\n",
            "        [50256,    36,  8505],\n",
            "        [50256,    47,  2433],\n",
            "        [50256,    33,  2007],\n",
            "        [50256,    38, 12375],\n",
            "        [50256,    21,    13],\n",
            "        [50256,    24,    13],\n",
            "        [50256,    41, 15746],\n",
            "        [50256,    22,    13],\n",
            "        [50256,    46,  1860],\n",
            "        [50256,    32,   259],\n",
            "        [50256,    15,    13],\n",
            "        [50256,    39,   436],\n",
            "        [50256,    23,    13],\n",
            "        [50256,    20,    13],\n",
            "        [50256,    19,    13],\n",
            "        [50256,    42,  4509],\n",
            "        [50256,    44,  2002],\n",
            "        [50256,    49,  2530],\n",
            "        [50256,     2,  1303],\n",
            "        [50256,    34, 10277],\n",
            "        [50256,    15,   657],\n",
            "        [50256,    16,  1510],\n",
            "        [50256,    26,   314],\n",
            "        [50256,    18,    13],\n",
            "        [50256,     8,   284],\n",
            "        [50256,    43,   452],\n",
            "        [50256,    16,    13],\n",
            "        [50256,    19,   400],\n",
            "        [50256,    44,   432],\n",
            "        [50256,    37, 19296],\n",
            "        [50256,    17,    13],\n",
            "        [50256,     2,    16],\n",
            "        [50256,    13,  2231],\n",
            "        [50256,    44,  1689],\n",
            "        [50256,    35,  3008],\n",
            "        [50256,    48,    25],\n",
            "        [50256,    17,   812],\n",
            "        [50256,    20,   812],\n",
            "        [50256,    38,  3087],\n",
            "        [50256,    16,   301],\n",
            "        [50256,    24,    14],\n",
            "        [50256,    43, 50192],\n",
            "        [50256,    18,    12],\n",
            "        [50256,    46,  1219],\n",
            "        [50256,    43,  2135],\n",
            "        [50256,    18,   812],\n",
            "        [50256,    41,   603],\n",
            "        [50256,     1,    40]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([ 1, 40, 18, 10, 27,  8, 11, 45,  4, 38, 41, 44, 39, 13, 47, 42,  3, 28,\n",
            "        28, 33, 33, 33, 43,  3, 14, 23, 35, 46, 39, 25, 49, 23, 19, 47,  3, 19,\n",
            "        26, 46, 13,  2, 38,  0, 19, 21, 46,  5, 37, 24, 26, 43],\n",
            "       device='cuda:0')\n",
            "topk 3 tensor([  292,   259,    64, 10898,   259,    88,   470,    11,    11,  1468,\n",
            "          290,    19,  1468,  8116,  1468,  1157,   278,    17,   362,    25,\n",
            "           13,    11,   290,   284,    24,   661,  1297,  3511,  2084,    19,\n",
            "          836,    11,   832,  2084,   329,   351,   307,   510,   293,   287,\n",
            "         2084,   356,   287,   484,   534,  1394,  1867,  1101,   651,    11],\n",
            "       device='cuda:0')\n",
            "topk scores3 tensor([1.0000, 0.9999, 0.9998, 0.9996, 0.9926, 0.9456, 0.9347, 0.7303, 0.6707,\n",
            "        0.6644, 0.5585, 0.5342, 0.5133, 0.4744, 0.4498, 0.4450, 0.4124, 0.3961,\n",
            "        0.3603, 0.3170, 0.2951, 0.2924, 0.2832, 0.2657, 0.2625, 0.2602, 0.2515,\n",
            "        0.2323, 0.2314, 0.2259, 0.2252, 0.2238, 0.2152, 0.2126, 0.2098, 0.2091,\n",
            "        0.2047, 0.1999, 0.1995, 0.1993, 0.1957, 0.1903, 0.1902, 0.1886, 0.1868,\n",
            "        0.1835, 0.1761, 0.1709, 0.1697, 0.1685], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "4 torch.Size([50, 4])\n",
            "depth 4 tensor([[50256,    45,  6950,   292],\n",
            "        [50256,    38,  3087,   259],\n",
            "        [50256,    44,  2002,    64],\n",
            "        [50256,    46,  1860, 10898],\n",
            "        [50256,    43,   452,   259],\n",
            "        [50256,    41, 15746,    88],\n",
            "        [50256,    32,   259,   470],\n",
            "        [50256,    46,  1219,    11],\n",
            "        [50256,    33,  2007,    11],\n",
            "        [50256,    17,   812,  1468],\n",
            "        [50256,    16,   301,   290],\n",
            "        [50256,    18,    12,    19],\n",
            "        [50256,    20,   812,  1468],\n",
            "        [50256,    39,   436,  8116],\n",
            "        [50256,    18,   812,  1468],\n",
            "        [50256,    24,    14,  1157],\n",
            "        [50256,    47,  2433,   278],\n",
            "        [50256,    16,    13,    17],\n",
            "        [50256,    16,    13,   362],\n",
            "        [50256,     2,    16,    25],\n",
            "        [50256,     2,    16,    13],\n",
            "        [50256,     2,    16,    11],\n",
            "        [50256,    43, 50192,   290],\n",
            "        [50256,    47,  2433,   284],\n",
            "        [50256,    23,    13,    24],\n",
            "        [50256,    16,  1510,   661],\n",
            "        [50256,    44,  1689,  1297],\n",
            "        [50256,    43,  2135,  3511],\n",
            "        [50256,    20,   812,  2084],\n",
            "        [50256,    18,    13,    19],\n",
            "        [50256,     1,    40,   836],\n",
            "        [50256,    16,  1510,    11],\n",
            "        [50256,    49,  2530,   832],\n",
            "        [50256,    18,   812,  2084],\n",
            "        [50256,    47,  2433,   329],\n",
            "        [50256,    49,  2530,   351],\n",
            "        [50256,     8,   284,   307],\n",
            "        [50256,    43,  2135,   510],\n",
            "        [50256,    39,   436,   293],\n",
            "        [50256,    36,  8505,   287],\n",
            "        [50256,    17,   812,  2084],\n",
            "        [50256,     6, 42323,   356],\n",
            "        [50256,    49,  2530,   287],\n",
            "        [50256,    34, 10277,   484],\n",
            "        [50256,    43,  2135,   534],\n",
            "        [50256,    38, 12375,  1394],\n",
            "        [50256,    48,    25,  1867],\n",
            "        [50256,    26,   314,  1101],\n",
            "        [50256,     8,   284,   651],\n",
            "        [50256,    43, 50192,    11]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([30,  4, 22,  1, 26, 27, 23, 14, 12,  9, 39, 32, 35, 34, 16, 31, 42,  6,\n",
            "        33, 17, 28, 10, 45, 15, 37,  3, 25, 40,  6, 38,  5, 37, 46, 29, 40, 10,\n",
            "        11,  8, 21,  8, 24, 31, 28, 15, 33, 36,  2, 47, 38, 40],\n",
            "       device='cuda:0')\n",
            "topk 4 tensor([  470,     6, 28527,     6,   502,   510,  1793,    11,    11,    11,\n",
            "          262,   262,   262,   262,   329,   830,   262,   645,    11,  1510,\n",
            "           11,  1315,   340,    11,   262,   318,   287,    11,  8168,   287,\n",
            "          318,   534,   338,  1510,   356,   362,    11,   345,   362,   314,\n",
            "         1510,   362,   356,   860,   356,   257,  1297,   257,   351,   345],\n",
            "       device='cuda:0')\n",
            "topk scores4 tensor([0.9843, 0.9642, 0.9560, 0.9323, 0.9306, 0.9006, 0.8180, 0.8016, 0.7679,\n",
            "        0.7569, 0.7227, 0.7118, 0.6347, 0.6303, 0.5536, 0.5098, 0.5040, 0.4898,\n",
            "        0.4764, 0.4762, 0.4728, 0.4473, 0.4209, 0.3832, 0.3341, 0.3340, 0.3309,\n",
            "        0.3224, 0.2818, 0.2811, 0.2809, 0.2764, 0.2763, 0.2679, 0.2600, 0.2585,\n",
            "        0.2464, 0.2383, 0.2306, 0.2216, 0.2208, 0.2202, 0.2167, 0.2092, 0.2088,\n",
            "        0.2057, 0.1966, 0.1962, 0.1846, 0.1830], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "5 torch.Size([50, 5])\n",
            "depth 5 tensor([[50256,     1,    40,   836,   470],\n",
            "        [50256,    43,   452,   259,     6],\n",
            "        [50256,    43, 50192,   290, 28527],\n",
            "        [50256,    38,  3087,   259,     6],\n",
            "        [50256,    44,  1689,  1297,   502],\n",
            "        [50256,    43,  2135,  3511,   510],\n",
            "        [50256,    47,  2433,   284,  1793],\n",
            "        [50256,    18,   812,  1468,    11],\n",
            "        [50256,    20,   812,  1468,    11],\n",
            "        [50256,    17,   812,  1468,    11],\n",
            "        [50256,    36,  8505,   287,   262],\n",
            "        [50256,    49,  2530,   832,   262],\n",
            "        [50256,    49,  2530,   351,   262],\n",
            "        [50256,    47,  2433,   329,   262],\n",
            "        [50256,    47,  2433,   278,   329],\n",
            "        [50256,    16,  1510,    11,   830],\n",
            "        [50256,    49,  2530,   287,   262],\n",
            "        [50256,    32,   259,   470,   645],\n",
            "        [50256,    18,   812,  2084,    11],\n",
            "        [50256,    16,    13,    17,  1510],\n",
            "        [50256,    20,   812,  2084,    11],\n",
            "        [50256,    16,   301,   290,  1315],\n",
            "        [50256,    38, 12375,  1394,   340],\n",
            "        [50256,    24,    14,  1157,    11],\n",
            "        [50256,    43,  2135,   510,   262],\n",
            "        [50256,    46,  1860, 10898,   318],\n",
            "        [50256,    16,  1510,   661,   287],\n",
            "        [50256,    17,   812,  2084,    11],\n",
            "        [50256,    32,   259,   470,  8168],\n",
            "        [50256,    39,   436,   293,   287],\n",
            "        [50256,    41, 15746,    88,   318],\n",
            "        [50256,    43,  2135,   510,   534],\n",
            "        [50256,    48,    25,  1867,   338],\n",
            "        [50256,    18,    13,    19,  1510],\n",
            "        [50256,    17,   812,  2084,   356],\n",
            "        [50256,    16,   301,   290,   362],\n",
            "        [50256,    18,    12,    19,    11],\n",
            "        [50256,    33,  2007,    11,   345],\n",
            "        [50256,     2,    16,    11,   362],\n",
            "        [50256,    33,  2007,    11,   314],\n",
            "        [50256,    23,    13,    24,  1510],\n",
            "        [50256,    16,  1510,    11,   362],\n",
            "        [50256,    20,   812,  2084,   356],\n",
            "        [50256,    24,    14,  1157,   860],\n",
            "        [50256,    18,   812,  2084,   356],\n",
            "        [50256,     8,   284,   307,   257],\n",
            "        [50256,    44,  2002,    64,  1297],\n",
            "        [50256,    26,   314,  1101,   257],\n",
            "        [50256,    39,   436,   293,   351],\n",
            "        [50256,    17,   812,  2084,   345]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([21, 35, 46,  2, 10, 29, 26, 48, 22, 14, 23, 32, 11, 19,  5,  6, 40, 30,\n",
            "        11, 33, 30, 33, 15, 17, 18, 20,  0,  1, 40, 27, 16, 39, 19,  5, 27, 18,\n",
            "         6, 47,  4,  4, 25, 37, 25, 20, 39,  1,  0, 37, 14,  0],\n",
            "       device='cuda:0')\n",
            "topk 5 tensor([  400,   358,   502,    11,  1633,   262,   262,   262,  1103,   262,\n",
            "          860,   262,  1748,    11,    11,    11,    11,   644,  6483,    11,\n",
            "          262,   287,   661,   761,   356,   356,   765,   262,   287,   356,\n",
            "         6483,   447,   661,   290,   345,   345,   329,  2802,   326,    11,\n",
            "          262,   760,   644,   314,  1101,  1842,   760,  1365,  1793, 18869],\n",
            "       device='cuda:0')\n",
            "topk scores5 tensor([0.9999, 0.9990, 0.9590, 0.7732, 0.7571, 0.7239, 0.6493, 0.6425, 0.5231,\n",
            "        0.4772, 0.4277, 0.4103, 0.4024, 0.3992, 0.3932, 0.3875, 0.3842, 0.3371,\n",
            "        0.3337, 0.3273, 0.2784, 0.2705, 0.2653, 0.2635, 0.2520, 0.2511, 0.2459,\n",
            "        0.2426, 0.2386, 0.2368, 0.2279, 0.2258, 0.2202, 0.2122, 0.2070, 0.1980,\n",
            "        0.1946, 0.1911, 0.1805, 0.1757, 0.1704, 0.1683, 0.1674, 0.1617, 0.1557,\n",
            "        0.1552, 0.1495, 0.1477, 0.1435, 0.1344], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "6 torch.Size([50, 6])\n",
            "depth 6 tensor([[50256,    16,   301,   290,  1315,   400],\n",
            "        [50256,    16,   301,   290,   362,   358],\n",
            "        [50256,    44,  2002,    64,  1297,   502],\n",
            "        [50256,    43, 50192,   290, 28527,    11],\n",
            "        [50256,    36,  8505,   287,   262,  1633],\n",
            "        [50256,    39,   436,   293,   287,   262],\n",
            "        [50256,    16,  1510,   661,   287,   262],\n",
            "        [50256,    39,   436,   293,   351,   262],\n",
            "        [50256,    38, 12375,  1394,   340,  1103],\n",
            "        [50256,    47,  2433,   278,   329,   262],\n",
            "        [50256,    24,    14,  1157,    11,   860],\n",
            "        [50256,    48,    25,  1867,   338,   262],\n",
            "        [50256,    49,  2530,   832,   262,  1748],\n",
            "        [50256,    16,    13,    17,  1510,    11],\n",
            "        [50256,    43,  2135,  3511,   510,    11],\n",
            "        [50256,    47,  2433,   284,  1793,    11],\n",
            "        [50256,    23,    13,    24,  1510,    11],\n",
            "        [50256,    41, 15746,    88,   318,   644],\n",
            "        [50256,    49,  2530,   832,   262,  6483],\n",
            "        [50256,    18,    13,    19,  1510,    11],\n",
            "        [50256,    41, 15746,    88,   318,   262],\n",
            "        [50256,    18,    13,    19,  1510,   287],\n",
            "        [50256,    16,  1510,    11,   830,   661],\n",
            "        [50256,    32,   259,   470,   645,   761],\n",
            "        [50256,    18,   812,  2084,    11,   356],\n",
            "        [50256,    20,   812,  2084,    11,   356],\n",
            "        [50256,     1,    40,   836,   470,   765],\n",
            "        [50256,    43,   452,   259,     6,   262],\n",
            "        [50256,    23,    13,    24,  1510,   287],\n",
            "        [50256,    17,   812,  2084,    11,   356],\n",
            "        [50256,    49,  2530,   287,   262,  6483],\n",
            "        [50256,    33,  2007,    11,   314,   447],\n",
            "        [50256,    16,    13,    17,  1510,   661],\n",
            "        [50256,    43,  2135,  3511,   510,   290],\n",
            "        [50256,    17,   812,  2084,    11,   345],\n",
            "        [50256,    18,   812,  2084,    11,   345],\n",
            "        [50256,    47,  2433,   284,  1793,   329],\n",
            "        [50256,    26,   314,  1101,   257,  2802],\n",
            "        [50256,    44,  1689,  1297,   502,   326],\n",
            "        [50256,    44,  1689,  1297,   502,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262],\n",
            "        [50256,    33,  2007,    11,   345,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   644],\n",
            "        [50256,    20,   812,  2084,    11,   314],\n",
            "        [50256,    33,  2007,    11,   314,  1101],\n",
            "        [50256,    43,   452,   259,     6,  1842],\n",
            "        [50256,     1,    40,   836,   470,   760],\n",
            "        [50256,    33,  2007,    11,   345,  1365],\n",
            "        [50256,    47,  2433,   278,   329,  1793],\n",
            "        [50256,     1,    40,   836,   470, 18869]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([31, 21, 28, 23, 45, 39,  8, 37,  4, 20, 11, 13, 49, 41, 48, 36, 42, 26,\n",
            "        46, 40,  3, 32, 37, 18, 12, 30, 30, 38, 17,  2, 18, 22, 17, 46, 44, 15,\n",
            "         6, 27, 22, 23, 12, 35, 44, 43,  5,  9,  0, 42, 16, 33],\n",
            "       device='cuda:0')\n",
            "topk 6 tensor([  247,   262,   262,   284,   318,   366,    11,    69,    11,  1738,\n",
            "          966,   362,   307,   644,    11,   262,   356,   284,   644,   691,\n",
            "          356,   287, 31699,   351,   351,    11,   351,   673,   314,   326,\n",
            "           11,   287,   356,  1521,   257, 12472,  6483,   995,   257,   329,\n",
            "           11,   760,   407,   373,  6483,   661,    11,   314,   807,   651],\n",
            "       device='cuda:0')\n",
            "topk scores6 tensor([1.0000, 0.8083, 0.7998, 0.7947, 0.7326, 0.7099, 0.6693, 0.5879, 0.5708,\n",
            "        0.5433, 0.5086, 0.4991, 0.4450, 0.4118, 0.4108, 0.4007, 0.3999, 0.3741,\n",
            "        0.3579, 0.3565, 0.3393, 0.3367, 0.3334, 0.3193, 0.3138, 0.2947, 0.2873,\n",
            "        0.2867, 0.2628, 0.2281, 0.2233, 0.2220, 0.2106, 0.2092, 0.2081, 0.2060,\n",
            "        0.2030, 0.2024, 0.1992, 0.1977, 0.1936, 0.1932, 0.1927, 0.1785, 0.1775,\n",
            "        0.1762, 0.1714, 0.1675, 0.1625, 0.1603], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "7 torch.Size([50, 7])\n",
            "depth 7 tensor([[50256,    33,  2007,    11,   314,   447,   247],\n",
            "        [50256,    18,    13,    19,  1510,   287,   262],\n",
            "        [50256,    23,    13,    24,  1510,   287,   262],\n",
            "        [50256,    32,   259,   470,   645,   761,   284],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366],\n",
            "        [50256,    38, 12375,  1394,   340,  1103,    11],\n",
            "        [50256,    26,   314,  1101,   257,  2802,    69],\n",
            "        [50256,    36,  8505,   287,   262,  1633,    11],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738],\n",
            "        [50256,    48,    25,  1867,   338,   262,   966],\n",
            "        [50256,    16,    13,    17,  1510,    11,   362],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644],\n",
            "        [50256,    47,  2433,   278,   329,  1793,    11],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356],\n",
            "        [50256,     1,    40,   836,   470,   765,   284],\n",
            "        [50256,     1,    40,   836,   470,   760,   644],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287],\n",
            "        [50256,    26,   314,  1101,   257,  2802, 31699],\n",
            "        [50256,    49,  2530,   832,   262,  6483,   351],\n",
            "        [50256,    49,  2530,   832,   262,  1748,   351],\n",
            "        [50256,    49,  2530,   287,   262,  6483,    11],\n",
            "        [50256,    49,  2530,   287,   262,  6483,   351],\n",
            "        [50256,    44,  1689,  1297,   502,   326,   673],\n",
            "        [50256,    41, 15746,    88,   318,   644,   314],\n",
            "        [50256,    44,  2002,    64,  1297,   502,   326],\n",
            "        [50256,    49,  2530,   832,   262,  6483,    11],\n",
            "        [50256,    16,  1510,    11,   830,   661,   287],\n",
            "        [50256,    41, 15746,    88,   318,   644,   356],\n",
            "        [50256,     1,    40,   836,   470,   760,  1521],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257],\n",
            "        [50256,    47,  2433,   284,  1793,    11, 12472],\n",
            "        [50256,    16,  1510,   661,   287,   262,  6483],\n",
            "        [50256,    43,   452,   259,     6,   262,   995],\n",
            "        [50256,    16,  1510,    11,   830,   661,   257],\n",
            "        [50256,    32,   259,   470,   645,   761,   329],\n",
            "        [50256,    49,  2530,   832,   262,  1748,    11],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407],\n",
            "        [50256,    20,   812,  2084,    11,   314,   373],\n",
            "        [50256,    39,   436,   293,   287,   262,  6483],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661],\n",
            "        [50256,    16,   301,   290,  1315,   400,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   314],\n",
            "        [50256,    23,    13,    24,  1510,    11,   807],\n",
            "        [50256,    43,  2135,  3511,   510,   290,   651]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([22,  0, 10,  7, 13, 31, 21, 33, 38, 45, 44, 18, 20, 32, 38, 41, 49, 29,\n",
            "         9, 17, 42, 19, 17, 36, 19, 39,  4, 37, 44, 39, 34,  4, 24, 12, 45, 12,\n",
            "         9,  9,  3,  5, 42,  1, 41, 16, 16, 15, 16, 43,  2,  6],\n",
            "       device='cuda:0')\n",
            "topk 7 tensor([  259,    76,   286, 12603,   314,   262,   262,   345,  1110,   326,\n",
            "           11,   345,   761,   761,   614,   644,   534,   673,  1521,   307,\n",
            "          257,   835,   766,    11,  1517,   257,   262,    11,   290,   262,\n",
            "         2802,   257,   257,   257,    11,   994,   314,   326,   307,    40,\n",
            "          262,   717,   356,   761,   466,   661,   869,   257,   717,   356],\n",
            "       device='cuda:0')\n",
            "topk scores7 tensor([0.9643, 0.9591, 0.9386, 0.8752, 0.7030, 0.6559, 0.6260, 0.5939, 0.5539,\n",
            "        0.4873, 0.4391, 0.4199, 0.4132, 0.3842, 0.3812, 0.3467, 0.3283, 0.3040,\n",
            "        0.3022, 0.3018, 0.2929, 0.2726, 0.2617, 0.2588, 0.2524, 0.2520, 0.2441,\n",
            "        0.2388, 0.2303, 0.2255, 0.2182, 0.2031, 0.1957, 0.1937, 0.1861, 0.1804,\n",
            "        0.1803, 0.1723, 0.1687, 0.1661, 0.1513, 0.1504, 0.1425, 0.1406, 0.1400,\n",
            "        0.1368, 0.1355, 0.1298, 0.1263, 0.1259], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "8 torch.Size([50, 8])\n",
            "depth 8 tensor([[50256,    26,   314,  1101,   257,  2802, 31699,   259],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76],\n",
            "        [50256,    48,    25,  1867,   338,   262,   966,   286],\n",
            "        [50256,    26,   314,  1101,   257,  2802,    69, 12603],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314],\n",
            "        [50256,    16,  1510,    11,   830,   661,   287,   262],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262],\n",
            "        [50256,     1,    40,   836,   470,   760,  1521,   345],\n",
            "        [50256,    16,  1510,    11,   830,   661,   257,  1110],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326],\n",
            "        [50256,    39,   436,   293,   287,   262,  6483,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761],\n",
            "        [50256,    41, 15746,    88,   318,   644,   356,   761],\n",
            "        [50256,    16,  1510,    11,   830,   661,   257,   614],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644],\n",
            "        [50256,    43,  2135,  3511,   510,   290,   651,   534],\n",
            "        [50256,    44,  2002,    64,  1297,   502,   326,   673],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,  1521],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766],\n",
            "        [50256,    16,  1510,   661,   287,   262,  6483,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517],\n",
            "        [50256,    32,   259,   470,   645,   761,   329,   257],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11],\n",
            "        [50256,    39,   436,   293,   287,   262,  6483,   290],\n",
            "        [50256,    32,   259,   470,   645,   761,   329,   262],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257,  2802],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   257],\n",
            "        [50256,    49,  2530,   832,   262,  1748,   351,   257],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   257],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,    11],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   994],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,   314],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,   326],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262],\n",
            "        [50256,    18,    13,    19,  1510,   287,   262,   717],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   356],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   761],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   466],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   869],\n",
            "        [50256,    20,   812,  2084,    11,   314,   373,   257],\n",
            "        [50256,    23,    13,    24,  1510,   287,   262,   717],\n",
            "        [50256,    38, 12375,  1394,   340,  1103,    11,   356]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([ 0, 30, 24, 40, 12,  3, 45,  4, 43, 13, 15, 44, 11, 41, 35, 37, 38, 21,\n",
            "        13, 44, 15,  9, 21, 37, 48, 18, 37, 18,  4, 26, 18, 22,  6, 48,  1, 49,\n",
            "        41, 43,  5, 39,  1, 46, 48, 41, 19, 27, 21, 42, 12, 29],\n",
            "       device='cuda:0')\n",
            "topk 8 tensor([    6,    69,   326,  2099,   284,    11,   326,  1612,   284,   284,\n",
            "          314,   618,   821,  1285,    11,   356, 22461,   356,    11,    11,\n",
            "          356,  1842,   284,   345,  1711,   345,   314,   314,  1101,   691,\n",
            "          356,   345,   717,  1295,   257, 17753,  1295,    11,  6483,   836,\n",
            "          407,   340,  1285,  1711,   287,   356,   326,   447,   257,  1230],\n",
            "       device='cuda:0')\n",
            "topk scores8 tensor([0.9471, 0.8399, 0.6984, 0.6940, 0.6676, 0.6627, 0.6541, 0.6182, 0.5833,\n",
            "        0.5623, 0.4618, 0.4471, 0.4331, 0.3762, 0.3331, 0.3181, 0.3092, 0.3069,\n",
            "        0.2944, 0.2742, 0.2702, 0.2605, 0.2575, 0.2499, 0.2469, 0.2465, 0.2407,\n",
            "        0.2394, 0.2338, 0.2230, 0.2220, 0.2207, 0.2190, 0.2156, 0.2143, 0.2089,\n",
            "        0.2075, 0.1971, 0.1926, 0.1781, 0.1754, 0.1749, 0.1744, 0.1665, 0.1620,\n",
            "        0.1568, 0.1544, 0.1485, 0.1471, 0.1470], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "9 torch.Size([50, 9])\n",
            "depth 9 tensor([[50256,    26,   314,  1101,   257,  2802, 31699,   259,     6],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257,  2802,    69],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   284],\n",
            "        [50256,    26,   314,  1101,   257,  2802,    69, 12603,    11],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1612],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   761,   284],\n",
            "        [50256,    41, 15746,    88,   318,   644,   356,   761,   284],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   466,   618],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821],\n",
            "        [50256,    18,    13,    19,  1510,   287,   262,   717,  1285],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   994,    11],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,   326,   356],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356],\n",
            "        [50256,    41, 15746,    88,   318,   644,   356,   761,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   466,    11],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   284],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,   326,   345],\n",
            "        [50256,    23,    13,    24,  1510,   287,   262,   717,  1711],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,  1521,   345],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,   326,   314],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,  1521,   314],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691],\n",
            "        [50256,    41, 15746,    88,   318,   262,  1738,  1521,   356],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717],\n",
            "        [50256,    23,    13,    24,  1510,   287,   262,   717,  1295],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    38, 12375,  1394,   340,  1103,    11,   356, 17753],\n",
            "        [50256,    18,    13,    19,  1510,   287,   262,   717,  1295],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   761,    11],\n",
            "        [50256,    16,  1510,    11,   830,   661,   287,   262,  6483],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   407],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   869,   340],\n",
            "        [50256,    23,    13,    24,  1510,   287,   262,   717,  1285],\n",
            "        [50256,    18,    13,    19,  1510,   287,   262,   717,  1711],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   356],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   356,   447],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   257],\n",
            "        [50256,    32,   259,   470,   645,   761,   329,   262,  1230]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([47, 39, 49,  1,  8, 41,  9, 33,  7, 36, 29, 16, 28,  3, 46, 14, 43, 24,\n",
            "        13, 42, 10, 42, 21, 48, 24, 21, 40,  3, 32, 32, 44, 43, 13, 17, 12, 10,\n",
            "        20,  4,  6, 34, 44, 22, 21, 28, 12,  2, 31, 44, 46, 22],\n",
            "       device='cuda:0')\n",
            "topk 9 tensor([  247,   470,   284, 12603,   466,    11,   466,    11,    30,    11,\n",
            "         1517,   286,  2282,   284,   356,   314,    11,    11,    11,    11,\n",
            "         1612,   286,   514,  1487,   286,   345,   257,   286,   614,  1285,\n",
            "          262,   286,   286,   460,  2282,  1101,   466,   307,  1842,  2802,\n",
            "          257,  2107,   502,   910,  3375,   356,   287,  7356,   345,   651],\n",
            "       device='cuda:0')\n",
            "topk scores9 tensor([1.0000, 0.9918, 0.9380, 0.9162, 0.9151, 0.8628, 0.8490, 0.8151, 0.7801,\n",
            "        0.7613, 0.7287, 0.6650, 0.6480, 0.6059, 0.5897, 0.5856, 0.5614, 0.5475,\n",
            "        0.5329, 0.4743, 0.4666, 0.3919, 0.3918, 0.3387, 0.3244, 0.3133, 0.2786,\n",
            "        0.2779, 0.2722, 0.2695, 0.2658, 0.2590, 0.2583, 0.2518, 0.2499, 0.2249,\n",
            "        0.2239, 0.2138, 0.2071, 0.1977, 0.1929, 0.1721, 0.1710, 0.1677, 0.1614,\n",
            "        0.1590, 0.1472, 0.1429, 0.1398, 0.1392], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "10 torch.Size([50, 10])\n",
            "depth 10 tensor([[50256,    18,   812,  2084,    11,   345,   760,   356,   447,   247],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470],\n",
            "        [50256,    32,   259,   470,   645,   761,   329,   262,  1230,   284],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257,  2802,    69, 12603],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   761,   284,   466],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   869,   340,    11],\n",
            "        [50256,    41, 15746,    88,   318,   644,   356,   761,   284,   466],\n",
            "        [50256,    23,    13,    24,  1510,   287,   262,   717,  1295,    11],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1612,    30],\n",
            "        [50256,    18,    13,    19,  1510,   287,   262,   717,  1295,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,  2282],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   356],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   994,    11,   314],\n",
            "        [50256,    18,    13,    19,  1510,   287,   262,   717,  1711,    11],\n",
            "        [50256,    23,    13,    24,  1510,   287,   262,   717,  1711,    11],\n",
            "        [50256,    18,    13,    19,  1510,   287,   262,   717,  1285,    11],\n",
            "        [50256,    23,    13,    24,  1510,   287,   262,   717,  1285,    11],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1612],\n",
            "        [50256,    23,    13,    24,  1510,   287,   262,   717,  1285,   286],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   514],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   257,  1487],\n",
            "        [50256,    23,    13,    24,  1510,   287,   262,   717,  1711,   286],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   345],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   407,   257],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,   614],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262],\n",
            "        [50256,    18,    13,    19,  1510,   287,   262,   717,  1711,   286],\n",
            "        [50256,    18,    13,    19,  1510,   287,   262,   717,  1285,   286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   284,   307],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   284,  2107],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   502],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,  7356],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   284,   651]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([43,  0, 44, 20, 40, 39, 12, 48, 10, 28, 34, 47, 22, 42, 29, 25, 38, 36,\n",
            "        14, 46, 36, 35, 37, 27, 45, 13, 23, 35,  1,  8, 42, 29,  4,  1, 22, 38,\n",
            "        27, 25,  6, 11, 23, 15, 38, 27, 39, 46, 30, 28, 15, 46],\n",
            "       device='cuda:0')\n",
            "topk 10 tensor([  259,   260,   546,    30,  2292,    69,    30,   460,   326,   286,\n",
            "           11,    11,    11,    11,   286,    11,   514,    30,   460,  7356,\n",
            "          259,   910,  1913,  1048,   460,   307,   287,  2282,   765,   921,\n",
            "          290,    11,    11,   761,   290,   502,  2802,   290,    11,   340,\n",
            "          286,   765,   345, 44873, 31699,  3770,  3430,    11,   655,   262],\n",
            "       device='cuda:0')\n",
            "topk scores10 tensor([1.0000, 0.9887, 0.9876, 0.9384, 0.8227, 0.8068, 0.6691, 0.6375, 0.6288,\n",
            "        0.6188, 0.6041, 0.5883, 0.5531, 0.5041, 0.4834, 0.4823, 0.4363, 0.4171,\n",
            "        0.4169, 0.3889, 0.3862, 0.3747, 0.3698, 0.3650, 0.3557, 0.3389, 0.3336,\n",
            "        0.3254, 0.3197, 0.3025, 0.3011, 0.2988, 0.2834, 0.2574, 0.2450, 0.2437,\n",
            "        0.2381, 0.2347, 0.2127, 0.1956, 0.1881, 0.1867, 0.1864, 0.1791, 0.1732,\n",
            "        0.1652, 0.1599, 0.1550, 0.1525, 0.1495], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "11 torch.Size([50, 11])\n",
            "depth 11 tensor([[50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910,\n",
            "           259],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   356,   447,   247,\n",
            "           260],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1612,\n",
            "            30],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802,\n",
            "            69],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,  2282,\n",
            "            30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,   614,\n",
            "           286],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,  7356,\n",
            "            11],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   514,\n",
            "            11],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   502,\n",
            "            11],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285,\n",
            "           286],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   345,\n",
            "            11],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           514],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "            30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   356,\n",
            "           460],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          7356],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "           910],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   284,   307,\n",
            "          1913],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   284,\n",
            "           307],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   257,  1487,\n",
            "           287],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "          2282],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1612,    30,\n",
            "           921],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   502,\n",
            "           290],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285,\n",
            "            11],\n",
            "        [50256,    46,  1860, 10898,   318,   644,   356,   761,   284,   466,\n",
            "            11],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   514,\n",
            "           290],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   345,\n",
            "           290],\n",
            "        [50256,    41, 15746,    88,   318,   644,   356,   761,   284,   466,\n",
            "            11],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   257,  1487,\n",
            "           286],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   994,    11,   314,\n",
            "           765],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802,\n",
            "         31699],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          3770],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,   614,\n",
            "            11],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   994,    11,   314,\n",
            "           655],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "           262]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([21, 43, 44,  5,  0, 27,  4, 39, 36, 23, 45, 24, 28,  2, 19, 40, 11, 46,\n",
            "        41, 35, 20, 16, 49, 48, 16, 40, 26, 36, 35, 42,  3, 20, 42, 17,  6, 10,\n",
            "        20, 33,  9,  8, 33, 23, 33, 47, 22,  8, 31, 37, 24,  7],\n",
            "       device='cuda:0')\n",
            "topk 11 tensor([  259,  4908,   259, 12603, 30960,    30,   810,    11,    69,   326,\n",
            "           11,   466,   284,    11,    11,  8761,   314,    11,   284,   290,\n",
            "            6,    11,  6483,   765,   290,  2612,   262, 31699,    11,    11,\n",
            "          921, 30960,   290,   775,   921,   314,   447,   284,   428,   460,\n",
            "          257,   284,   645,   362,  1576,   356,   362,   534,  1487,  2107],\n",
            "       device='cuda:0')\n",
            "topk scores11 tensor([1.0000, 1.0000, 0.9961, 0.9398, 0.8689, 0.8491, 0.8463, 0.7702, 0.6617,\n",
            "        0.6422, 0.5867, 0.5568, 0.5566, 0.5398, 0.5329, 0.5228, 0.5138, 0.5028,\n",
            "        0.4713, 0.4275, 0.4214, 0.3838, 0.3734, 0.3583, 0.3478, 0.3394, 0.3244,\n",
            "        0.3240, 0.3166, 0.3157, 0.3050, 0.2970, 0.2929, 0.2879, 0.2851, 0.2746,\n",
            "        0.2661, 0.2397, 0.2392, 0.2306, 0.2132, 0.2099, 0.2050, 0.2002, 0.1984,\n",
            "        0.1905, 0.1884, 0.1718, 0.1593, 0.1576], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "12 torch.Size([50, 12])\n",
            "depth 12 tensor([[50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802,\n",
            "         31699,   259],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802,\n",
            "            69, 12603],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910,\n",
            "           259, 30960],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "          2282,    30],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          3770,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765,   284],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          7356,    11],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   257,  1487,\n",
            "           286,  8761],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,  7356,\n",
            "            11,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   994,    11,   314,\n",
            "           765,   284],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,   290],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259,     6],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           514,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "           262,  6483],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   994,    11,   314,\n",
            "           655,   765],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           514,   290],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   257,  1487,\n",
            "           286,  2612],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   257,  1487,\n",
            "           287,   262],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,    11],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1612,\n",
            "            30,   921],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259, 30960],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,   290],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "            30,   775],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,  2282,\n",
            "            30,   921],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259,   447],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   284],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,   614,\n",
            "           286,   428],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,   614,\n",
            "            11,   362],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   284,   307,\n",
            "          1913,  1576],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285,\n",
            "            11,   362],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   345,\n",
            "           290,   534],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([36,  8,  2, 27,  0,  1, 26, 47, 23, 17,  9, 20,  6, 22, 10,  5,  4, 14,\n",
            "        12, 48, 48, 41, 13, 31, 37, 11, 45, 31, 39, 49, 45, 39, 40, 27,  0, 45,\n",
            "        11, 13, 32, 11,  1, 35,  7,  6, 33, 17, 35, 49, 42, 29],\n",
            "       device='cuda:0')\n",
            "topk 12 tensor([  247, 12603,     6,   259, 30960,   326,   835,  1641,   284,   314,\n",
            "          345,   318,   345,    11,   314,   921,   921,   314,   307,   546,\n",
            "           11,   307,   314,   775,   307,   284,   423,   921,  1487,   340,\n",
            "          460,   787,  2802,   364,     6,   761,   329,   475,   534,   546,\n",
            "          284,   655,   314,   484,   836,   345,  1101,   534,  1037,   290],\n",
            "       device='cuda:0')\n",
            "topk scores12 tensor([1.0000, 0.9977, 0.8317, 0.7884, 0.7836, 0.7423, 0.7147, 0.6204, 0.6037,\n",
            "        0.5520, 0.5036, 0.4930, 0.4688, 0.4320, 0.3468, 0.3255, 0.3232, 0.3197,\n",
            "        0.3066, 0.2904, 0.2863, 0.2861, 0.2751, 0.2659, 0.2525, 0.2507, 0.2477,\n",
            "        0.2430, 0.2413, 0.2256, 0.2217, 0.2136, 0.2075, 0.1934, 0.1916, 0.1875,\n",
            "        0.1839, 0.1608, 0.1546, 0.1445, 0.1411, 0.1348, 0.1345, 0.1244, 0.1220,\n",
            "        0.1183, 0.1153, 0.1078, 0.1051, 0.0959], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "13 torch.Size([50, 13])\n",
            "depth 13 tensor([[50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259,   447,   247],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802,\n",
            "         31699,   259,     6],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   259],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259, 30960],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   257,  1487,\n",
            "           287,   262,   835],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   345,\n",
            "           290,   534,  1641],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   994,    11,   314,\n",
            "           655,   765,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   314],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259,     6,   318],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "           262,  6483,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          3770,    11,   314],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "          2282,    30,   921],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910,\n",
            "           259, 30960,   921],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          7356,    11,   314],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765,   284,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,    11],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259, 30960,   775],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   284,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   423],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259, 30960,   921],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,  1487],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   340],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   364],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259,     6],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   761],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   329],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   475],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,   290,   534],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   284],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   484],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "            30,   775,   836],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,  1101],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,    11,   290]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([26, 32, 39,  3, 47, 35, 30, 33, 38,  7,  1, 44, 34,  0, 37, 44, 46,  4,\n",
            "         5, 12,  6, 13, 25, 31, 12, 43, 42, 41, 28, 19, 41, 29, 31, 31, 25, 18,\n",
            "        28, 36, 22, 40,  1, 48, 41, 10, 45, 43, 18, 34, 42, 46],\n",
            "       device='cuda:0')\n",
            "topk 13 tensor([  284,    69,   340,     6,  1204,   284,   466,   326,  1641,    11,\n",
            "          326,   447,   318,    30,   314,   470,   655,   921,   345,   821,\n",
            "          326,   314,  1487,   514,   460,   460,   655,   765,   514,   340,\n",
            "        18869,    11,   345,   257,   787,   994,   502,   922,   655,   307,\n",
            "          284,   422,  2911,   815,   760,   821,   257,    30,  1101,   407],\n",
            "       device='cuda:0')\n",
            "topk scores13 tensor([0.9504, 0.9477, 0.9453, 0.9442, 0.8503, 0.8501, 0.7628, 0.7012, 0.6952,\n",
            "        0.6205, 0.6093, 0.5929, 0.5600, 0.5304, 0.4243, 0.4071, 0.3957, 0.3679,\n",
            "        0.3494, 0.3439, 0.3418, 0.3326, 0.3026, 0.2900, 0.2836, 0.2667, 0.2612,\n",
            "        0.2527, 0.2481, 0.2324, 0.2303, 0.2161, 0.2122, 0.2117, 0.2042, 0.1929,\n",
            "        0.1915, 0.1832, 0.1829, 0.1787, 0.1776, 0.1719, 0.1406, 0.1384, 0.1382,\n",
            "        0.1381, 0.1362, 0.1339, 0.1317, 0.1304], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "14 torch.Size([50, 14])\n",
            "depth 14 tensor([[50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   423,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   259,     6],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   761,   284],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   364,   326],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,   290,   534,  1641],\n",
            "        [50256,    47,  2433,   278,   329,   262,   661,   326,  1842,   345,\n",
            "           290,   534,  1641,    11],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "            30,   775,   836,   447],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259,     6,   318],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259,   447,   247,    30],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   475,   314],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "            30,   775,   836,   470],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,  1101,   655],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259, 30960,   921],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821],\n",
            "        [50256,    43, 50192,   290, 28527,    11,   356,   761,   257,  1487,\n",
            "           287,   262,   835,   326],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "           262,  6483,    11,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,  1487],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   460],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   484,   460],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,   655],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655,   765],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,  1487,   514],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   340,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   345],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765,   284,   307,   994],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,  1487,   502],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   329,   922],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   284,   307],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655,  2911],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   484,   821],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765,   284,   307,   257],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259,     6,    30],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,  1101],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,  1101,   407]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([11,  1,  5,  0, 23, 41,  2, 19, 36, 29, 28, 27,  8,  6, 37, 44, 33, 45,\n",
            "        34, 43, 10, 26, 38,  3, 48, 47, 33, 34, 38, 46, 26, 13, 30,  7,  4, 13,\n",
            "        18, 49,  6,  3, 39, 31, 29, 48, 40,  4, 22, 30, 48, 32],\n",
            "       device='cuda:0')\n",
            "topk 14 tensor([  247, 12603,   466,   466,  7387,   262,    11,   407,    11,    11,\n",
            "           11,   284,    11,   284,    11,   644,  3580,   407,  1654,   307,\n",
            "          345,   765,   765,   326,   655,   921,   582,   340, 18869, 11778,\n",
            "        18869,   775,   307,   345,  1231,   921,   815,   257,   618,   345,\n",
            "          257, 44873,    13,  6613,   307,    11,   340,   910,   407,  7387],\n",
            "       device='cuda:0')\n",
            "topk scores14 tensor([1.0000, 0.8960, 0.7008, 0.6020, 0.5774, 0.5726, 0.5315, 0.5255, 0.5021,\n",
            "        0.4879, 0.4628, 0.4584, 0.4570, 0.4519, 0.4375, 0.4312, 0.4047, 0.4013,\n",
            "        0.3684, 0.3537, 0.3494, 0.3378, 0.3307, 0.2954, 0.2885, 0.2823, 0.2772,\n",
            "        0.2638, 0.2604, 0.2434, 0.2257, 0.2143, 0.2127, 0.2117, 0.1956, 0.1708,\n",
            "        0.1582, 0.1534, 0.1534, 0.1533, 0.1517, 0.1505, 0.1482, 0.1459, 0.1441,\n",
            "        0.1434, 0.1409, 0.1407, 0.1397, 0.1327], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "15 torch.Size([50, 15])\n",
            "depth 15 tensor([[50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "            30,   775,   836,   447,   247],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   761,   284,   466],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   423,   284,   466],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514,  7387],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821,   407],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,  1487,   502,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,  1487,   514,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655,   765,   284],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,   290,   534,  1641,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   329,   922,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   484,   821,   407],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,  1654],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,   655,   765],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   259,     6,   326],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,  1101,   655],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259,     6,    30,   921],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765,   284,   307,   257, 11778],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,   655, 18869],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259,   447,   247,    30,   775],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   307],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   364,   326,   345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259,   447,   247,    30,   921],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,  1101,   407,   257],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   618],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   259,     6,   345],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   284,   307,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   340,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,  1101,  6613],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   284,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,  1487,   340],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,  1101,   407],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   345,  7387]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([41,  0,  5,  7, 17, 27,  1, 15, 22, 49, 23, 46, 16, 34, 46, 36,  8,  4,\n",
            "        49, 26, 24, 47, 28, 42, 21, 15, 16, 11, 45, 44, 10, 19, 47, 40,  4, 20,\n",
            "        13, 26, 39, 27,  6, 13,  3, 28, 36, 16,  1, 19, 27, 31],\n",
            "       device='cuda:0')\n",
            "topk 15 tensor([ 4908,    83,  1230,  3142,  3142,  1365,   284,   314,   284,   621,\n",
            "          345,    11,   287,   340,    13,   307, 44873,    11,    11,  1254,\n",
            "          257,   326,   307,   775,   345,   326,    11,   307, 44873,   257,\n",
            "        44873, 22461,   340,   636,   621,   815,   787,  3772,   284,  1645,\n",
            "        44873,   651,   618,   910,  3774,   618,   588,  3142,   670,   836],\n",
            "       device='cuda:0')\n",
            "topk scores15 tensor([1.0000, 1.0000, 0.8173, 0.6478, 0.5532, 0.5021, 0.4845, 0.4796, 0.4622,\n",
            "        0.4489, 0.3857, 0.3519, 0.3410, 0.3344, 0.3001, 0.2843, 0.2706, 0.2691,\n",
            "        0.2691, 0.2642, 0.2610, 0.2423, 0.2326, 0.2266, 0.2231, 0.2221, 0.2184,\n",
            "        0.2082, 0.2041, 0.2029, 0.1940, 0.1865, 0.1843, 0.1780, 0.1704, 0.1574,\n",
            "        0.1501, 0.1482, 0.1482, 0.1430, 0.1401, 0.1384, 0.1371, 0.1311, 0.1266,\n",
            "        0.1256, 0.1242, 0.1205, 0.1195, 0.1180], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "16 torch.Size([50, 16])\n",
            "depth 16 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   340,    11, 44873,  4908],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "            30,   775,   836,   447,   247,    83],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821,   407,  3142],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   484,   821,   407,  3142],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   345,  7387,   621],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   259,     6,   326,   345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,  1487,   340,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,   287],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,  1487,   340,    13],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,  1487,   502,    11, 44873],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514,  7387,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   345,  7387,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,  1101,   655,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   326],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,   655,   765,   345],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655,   765,   284,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,    11, 44873],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   284,   307,   257],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,  1487,   514,    11, 44873],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307, 22461],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   340],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   284,   307,   257,   636],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514,  7387,   621],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  3772],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   259,     6,   345,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1645],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   651],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   423,   284,   466,   618],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,  3774],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,   618],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307,  3142],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670],\n",
            "        [50256,    18,   812,  2084,    11,   345,   760,   644,   356,   466,\n",
            "           259,   447,   247,    30,   775,   836]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([ 0, 30, 28, 40, 47,  5, 45,  7, 31, 36, 25, 48, 44, 39, 26, 25, 15, 12,\n",
            "        18,  9, 21, 17, 43, 13, 35, 13, 38, 46,  2, 14, 48,  2,  8, 39, 19, 17,\n",
            "        32,  7, 46, 43, 41, 48, 23, 41,  8, 36, 34,  2, 32, 19],\n",
            "       device='cuda:0')\n",
            "topk 16 tensor([50256,  4908,  4908,  4908,   284,   329,   345,  1612,   284,   340,\n",
            "         1724,   329,   287,    11, 44873,  1612, 22461,  1204, 44873,   257,\n",
            "          526, 44873,   326,    13,   307,    11,   307,   514,     1,   775,\n",
            "         1365,   526,   307,   329,   588,  1365,   526,  1101,   345,   340,\n",
            "          340,    11,   836,   832,   910,   257,  6731,   553,    25,   826],\n",
            "       device='cuda:0')\n",
            "topk scores16 tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.9934, 0.8335, 0.6855, 0.6577, 0.6417,\n",
            "        0.5722, 0.5203, 0.4481, 0.4369, 0.4170, 0.3916, 0.3885, 0.3424, 0.3420,\n",
            "        0.3283, 0.3217, 0.3116, 0.2967, 0.2962, 0.2942, 0.2881, 0.2785, 0.2707,\n",
            "        0.2681, 0.2494, 0.2308, 0.2119, 0.2023, 0.1998, 0.1953, 0.1864, 0.1795,\n",
            "        0.1711, 0.1591, 0.1570, 0.1524, 0.1436, 0.1368, 0.1337, 0.1233, 0.1233,\n",
            "        0.1221, 0.1200, 0.1187, 0.1175, 0.1162], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "17 torch.Size([49, 17])\n",
            "depth 17 tensor([[50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,  1487,   514,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307,  3142,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365,   329],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,   618,   345],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307, 22461,   284],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1724],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,   329],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,  3774,   287],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1645,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1612],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307, 22461],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,   287,  1204],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   345,  7387,    11, 44873],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   345,  7387,   621,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   326,   526],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514,  7387,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    13],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    11],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   259,     6,   345,   284,   307],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,     1],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,  1487,   340,    13,   775],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,  1365],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   526],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1645,   329],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   588],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514,  7387,    11,  1365],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   340,   526],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   340],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   651,   340],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   651,   832],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514,  7387,   621,  6731],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   553],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   340,    25],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   826]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([49, 50257])\n",
            "prev word tensor([ 0, 13, 20, 29, 41, 10, 36, 23, 15, 44,  6, 48,  9, 14, 16,  4,  7,  6,\n",
            "        14, 41,  8, 32, 35, 19, 43, 42,  9, 31, 26, 46, 21, 44, 36, 37, 37, 11,\n",
            "         1, 24, 37,  2,  8, 35, 39, 19, 33, 27,  1, 30, 35], device='cuda:0')\n",
            "topk 17 tensor([50256,  4908,  4908,   329,   470,   514,  2282, 22461,   284,  3580,\n",
            "         1701,   618,  1701,  1701,    11,   514,  9159,    30,    30,   447,\n",
            "         1645,   514,   784,   784,   326,   340,    30,  1598,     1,   475,\n",
            "          526, 17717,   910,   526,   466,   257,    13, 44873,     1,    13,\n",
            "          670,   366,   832,   851,  2241,   290,    30,   314,   851],\n",
            "       device='cuda:0')\n",
            "topk scores17 tensor([1.0000, 1.0000, 1.0000, 0.7764, 0.6258, 0.5994, 0.5943, 0.5461, 0.5450,\n",
            "        0.5202, 0.4950, 0.4580, 0.4565, 0.4410, 0.4342, 0.4091, 0.4088, 0.4070,\n",
            "        0.4029, 0.3742, 0.3731, 0.3621, 0.3605, 0.3576, 0.3416, 0.3342, 0.3295,\n",
            "        0.3006, 0.2844, 0.2771, 0.2631, 0.2290, 0.2219, 0.2205, 0.2190, 0.2160,\n",
            "        0.2120, 0.2026, 0.2004, 0.1961, 0.1943, 0.1929, 0.1918, 0.1912, 0.1911,\n",
            "        0.1883, 0.1883, 0.1863, 0.1863], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "18 torch.Size([48, 18])\n",
            "depth 18 tensor([[50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514,  7387,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,  1365,   329],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   470],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,   329,   514],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307, 22461,   284],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612,  1701],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   826,   618],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1724,  1701],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1612,  1701],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,   287,  1204,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365,   329,   514],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307, 22461,   284,  9159],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612,    30],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1612,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   447],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,  1645],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1645,   329,   514],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   340,   526,   784],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   326,   526,   784],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   651,   832,   340],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1724,    30],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   553,   475],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257, 17717],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,  3774,   287,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,    11, 44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    11, 44873],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,   670],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   340,   526,   366],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   651,   340,   832],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   326,   526,   851],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   588,  2241],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,     1,   290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,    11, 44873,  4908,    30],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   526,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   340,   526,   851]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([48, 50257])\n",
            "prev word tensor([ 0, 36, 31, 10, 11, 12, 19,  5,  6,  3,  9, 13, 33,  2,  7,  8, 29, 24,\n",
            "        23, 39, 25,  8,  4, 17, 16,  5, 39, 28, 38, 27, 37,  1, 43,  0, 32, 26,\n",
            "        17, 44, 27, 37, 29, 21, 25, 29, 14, 46, 40, 45], device='cuda:0')\n",
            "topk 18 tensor([50256,  4908,   259,   339,   366,   366,    11,    30,   284,   761,\n",
            "          366, 44873,   284,   514,  9159,   287,   784,    11,   526,   329,\n",
            "          921,    11,    11,   921,   921,  1701,    11,   314,   775,   284,\n",
            "          284,    30,    11,    13,   314,   526,   314,   314,   290,   290,\n",
            "          366,   366,   314,   851,   290,  1101,  1639,   921],\n",
            "       device='cuda:0')\n",
            "topk scores18 tensor([1.0000, 1.0000, 1.0000, 0.8735, 0.5608, 0.5531, 0.5523, 0.5403, 0.5022,\n",
            "        0.4989, 0.4902, 0.4857, 0.4808, 0.4688, 0.4490, 0.4090, 0.3742, 0.3643,\n",
            "        0.3490, 0.3478, 0.3450, 0.3254, 0.3237, 0.3154, 0.3147, 0.2965, 0.2904,\n",
            "        0.2846, 0.2646, 0.2614, 0.2400, 0.2369, 0.2302, 0.2272, 0.2225, 0.2131,\n",
            "        0.2122, 0.2026, 0.1944, 0.1884, 0.1879, 0.1839, 0.1795, 0.1779, 0.1746,\n",
            "        0.1617, 0.1602, 0.1446], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "19 torch.Size([47, 19])\n",
            "depth 19 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910,   259],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   826,   618,   339],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1724,  1701,   366],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1612,  1701,   366],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,  1645,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282,    30],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   470,   761],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612,  1701,   366],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,   287,  1204,    11, 44873],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,  1365,   329,   514],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307, 22461,   284,  9159],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580,   287],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   784],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   651,   832,   340,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,   670,   329],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1724,    30,   921],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,   329,   514,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1612,    30,   921],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612,    30,   921],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282,  1701],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,   670,    11],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   553,   475,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13,   775],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   284],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514,  7387,    11, 44873,  4908,    30],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   588,  2241,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908,    13],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1612,    30,   314],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,     1,   290,   314],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   290],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   366],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   340,   526,   784,   366],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1724,    30,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   851],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365,   329,   514,   290],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   526,   314,  1101],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314,   655, 18869,   910,   340,   526,   366,  1639],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,    11, 44873,  4908,    30,   921]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([47, 50257])\n",
            "prev word tensor([ 0,  1, 14, 18, 24, 20, 34,  7, 17, 29, 30, 44,  8,  6, 28, 25, 16,  4,\n",
            "         3,  9,  5, 43,  1, 36,  2, 12,  0,  0, 38, 17, 33,  2,  0, 37,  8, 46,\n",
            "        17, 15, 39, 26, 27, 27, 12, 32, 34,  7, 34], device='cuda:0')\n",
            "topk 19 tensor([50256, 30960,  1204,   514,   366, 44873,   784,  9159,   784,   307,\n",
            "          447,   407,   284,   921,   307, 44873, 44873,  1639,  1639,  1639,\n",
            "        44873,   674,     6,  1101,  5300,   290,    13,    30,   314,   851,\n",
            "         1101,   338,     0,   314,   257,  1365,   366,   366,  1639,  1101,\n",
            "          761,   836,    11,   921,   851,   910,   366], device='cuda:0')\n",
            "topk scores19 tensor([1.0000, 0.7194, 0.5594, 0.5068, 0.4962, 0.4759, 0.4476, 0.4228, 0.3907,\n",
            "        0.3888, 0.3682, 0.3653, 0.3586, 0.3531, 0.3227, 0.3126, 0.3099, 0.2851,\n",
            "        0.2800, 0.2723, 0.2653, 0.2626, 0.2520, 0.2268, 0.2140, 0.2115, 0.2020,\n",
            "        0.1929, 0.1891, 0.1865, 0.1859, 0.1771, 0.1681, 0.1661, 0.1632, 0.1605,\n",
            "        0.1603, 0.1585, 0.1584, 0.1566, 0.1553, 0.1529, 0.1503, 0.1493, 0.1481,\n",
            "        0.1420, 0.1359], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "20 torch.Size([46, 20])\n",
            "depth 20 tensor([[50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910,   259, 30960],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580,   287,  1204],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,   670,   329,   514],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282,  1701,   366],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   784],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,  9159],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   784],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   284,   307],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514,  7387,    11, 44873,  4908,    30,   447],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   526,   314,  1101,   407],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   470,   761,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282,    30,   921],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   284,   307],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,   670,    11, 44873],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   651,   832,   340,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1612,  1701,   366,  1639],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   326,  1724,  1701,   366,  1639],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612,  1701,   366,  1639],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,  1645,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365,   329,   514,   290,   674],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910,   259,     6],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,     1,   290,   314,  1101],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   826,   618,   339,  5300],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,  1365,   329,   514,   290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    11, 44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    11, 44873,  4908,    30],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   290,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   851],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314,  1101],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   826,   618,   339,   338],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    11, 44873,  4908,     0],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   470,   761,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,    11, 44873,  4908,    30,   921,  1365],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   366],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   784,   366],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   366,  1639],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   553,   475,   314,  1101],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13,   775,   761],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13,   775,   836],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,  1365,   329,   514,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908,    13,   921],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   851],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   910],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([46, 50257])\n",
            "prev word tensor([ 4, 14, 15, 19,  9, 23, 21, 39, 40, 20,  0,  1, 40, 30,  2, 38, 29, 10,\n",
            "         3, 34, 32, 27, 22, 24, 11, 44, 26, 25, 20,  1, 45, 29, 31, 33, 42, 45,\n",
            "        35, 42,  7, 44, 39, 35,  5, 36, 22, 42], device='cuda:0')\n",
            "topk 20 tensor([ 4908,  4908,  4908,  4908,   251,  2642,  1701,   284,   470,  3988,\n",
            "          921,    11,   447,   287,    11,   407,   407,   257,  1639,   407,\n",
            "         1101,  1101,   407,   674,   307,   340,   921,   921,  1751,    13,\n",
            "         1639,  2611,   921, 47921,   760,  2061,  1639, 17753,   366,   645,\n",
            "          257,  2061,   366,  2061,  2611,   836], device='cuda:0')\n",
            "topk scores20 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7001, 0.6282, 0.5864, 0.5765,\n",
            "        0.4598, 0.4540, 0.4467, 0.4235, 0.3523, 0.3492, 0.3191, 0.2996, 0.2865,\n",
            "        0.2729, 0.2600, 0.2567, 0.2451, 0.2013, 0.2000, 0.1884, 0.1826, 0.1750,\n",
            "        0.1681, 0.1659, 0.1631, 0.1613, 0.1612, 0.1577, 0.1559, 0.1528, 0.1504,\n",
            "        0.1464, 0.1393, 0.1373, 0.1346, 0.1278, 0.1239, 0.1180, 0.1156, 0.1082,\n",
            "        0.1047], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "21 torch.Size([46, 21])\n",
            "depth 21 tensor([[50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580,    11, 44873,\n",
            "          4908],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,   670,    11, 44873,\n",
            "          4908],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   651,   832,   340,    11, 44873,\n",
            "          4908],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,  1645,    11, 44873,\n",
            "          4908],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   514,  7387,    11, 44873,  4908,    30,   447,\n",
            "           251],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   826,   618,   339,  5300,\n",
            "          2642],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910,   259,     6,\n",
            "          1701],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13,   775,   761,\n",
            "           284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13,   775,   836,\n",
            "           470],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365,   329,   514,   290,   674,\n",
            "          3988],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910,   259, 30960,\n",
            "           921],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580,   287,  1204,\n",
            "            11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13,   775,   836,\n",
            "           447],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   826,   618,   339,   338,\n",
            "           287],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,   670,   329,   514,\n",
            "            11],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   553,   475,   314,  1101,\n",
            "           407],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314,  1101,\n",
            "           407],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   526,   314,  1101,   407,\n",
            "           257],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282,  1701,   366,\n",
            "          1639],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,    11, 44873,  4908,    30,   921,  1365,\n",
            "           407],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290,   314,\n",
            "          1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   290,   314,\n",
            "          1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,     1,   290,   314,  1101,\n",
            "           407],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,  1365,   329,   514,   290,\n",
            "           674],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   470,   761,   284,\n",
            "           307],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   910,\n",
            "           340],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    11, 44873,  4908,    30,\n",
            "           921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    11, 44873,  4908,    13,\n",
            "           921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365,   329,   514,   290,   674,\n",
            "          1751],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580,   287,  1204,\n",
            "            13],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366,\n",
            "          1639],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314,  1101,\n",
            "          2611],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    11, 44873,  4908,     0,\n",
            "           921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   470,   761,   257,\n",
            "         47921],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908,    13,   921,\n",
            "           760],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366,\n",
            "          2061],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   366,\n",
            "          1639],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908,    13,   921,\n",
            "         17753],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   784,\n",
            "           366],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   910,\n",
            "           645],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13,   775,   761,\n",
            "           257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   366,\n",
            "          2061],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   784,\n",
            "           366],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   784,   366,\n",
            "          2061],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,     1,   290,   314,  1101,\n",
            "          2611],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908,    13,   921,\n",
            "           836]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([46, 50257])\n",
            "prev word tensor([ 0, 45,  6, 23,  8, 11, 40, 21, 45, 34,  0, 20, 13, 33,  9, 41, 28,  3,\n",
            "        35,  2,  5, 22, 43, 14, 15,  9, 29,  1, 19, 17,  7, 37, 28, 24, 23, 33,\n",
            "        31, 37,  2, 17, 42, 31, 24,  5, 32, 19], device='cuda:0')\n",
            "topk 21 tensor([50256,   447,   366,  3988,   761, 44873,  1487,  2611,   470,   644,\n",
            "           13,  2611,  2356,    11,    11,   338,    11,    13,   338,    13,\n",
            "           11,   257,   338, 44873,   257,    13,   775,    13,  1309, 14971,\n",
            "          307,   307,    13,  7787,  1751,   284,  1560,  1833,    30,  6253,\n",
            "         2061,   307, 12008,    13, 17753,   307], device='cuda:0')\n",
            "topk scores21 tensor([1.0000, 0.6472, 0.5405, 0.5109, 0.4931, 0.4345, 0.3868, 0.3623, 0.3528,\n",
            "        0.3503, 0.3023, 0.3007, 0.2883, 0.2848, 0.2737, 0.2627, 0.2565, 0.2516,\n",
            "        0.2496, 0.2267, 0.2130, 0.2099, 0.2064, 0.2014, 0.1982, 0.1884, 0.1878,\n",
            "        0.1877, 0.1837, 0.1836, 0.1785, 0.1764, 0.1751, 0.1707, 0.1681, 0.1677,\n",
            "        0.1674, 0.1673, 0.1600, 0.1574, 0.1541, 0.1486, 0.1359, 0.1359, 0.1323,\n",
            "        0.1280], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "22 torch.Size([45, 22])\n",
            "depth 22 tensor([[50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908,    13,   921,\n",
            "           836,   447],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910,   259,     6,\n",
            "          1701,   366],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,  1365,   329,   514,   290,\n",
            "           674,  3988],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13,   775,   836,\n",
            "           470,   761],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580,   287,  1204,\n",
            "            11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13,   775,   761,\n",
            "           257,  1487],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   290,   314,\n",
            "          1101,  2611],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908,    13,   921,\n",
            "           836,   470],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908,    13,   921,\n",
            "           760,   644],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580,    11, 44873,\n",
            "          4908,    13],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290,   314,\n",
            "          1101,  2611],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   826,   618,   339,   338,\n",
            "           287,  2356],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   470,   761,   257,\n",
            "         47921,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365,   329,   514,   290,   674,\n",
            "          3988,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   366,\n",
            "          2061,   338],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365,   329,   514,   290,   674,\n",
            "          1751,    11],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,  1645,    11, 44873,\n",
            "          4908,    13],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366,\n",
            "          2061,   338],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   651,   832,   340,    11, 44873,\n",
            "          4908,    13],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   826,   618,   339,  5300,\n",
            "          2642,    11],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,     1,   290,   314,  1101,\n",
            "           407,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   784,   366,\n",
            "          2061,   338],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,   670,   329,   514,\n",
            "            11, 44873],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   553,   475,   314,  1101,\n",
            "           407,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365,   329,   514,   290,   674,\n",
            "          3988,    13],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   257,  3580,   287,  1204,\n",
            "            13,   775],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   787,   340,   670,    11, 44873,\n",
            "          4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,    11, 44873,  4908,    30,   921,  1365,\n",
            "           407,  1309],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   526,   314,  1101,   407,\n",
            "           257, 14971],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   546,   340,    11, 44873,  4908,    13,   775,   761,\n",
            "           284,   307],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908,    13,   921,\n",
            "         17753,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,  1365,   329,   514,   290,   674,\n",
            "          1751,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   470,   761,   284,\n",
            "           307,  7787],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,   466,   284,   787,   340,   670,  1365,   329,   514,   290,\n",
            "           674,  1751],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   470,   761,   257,\n",
            "         47921,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314,  1101,\n",
            "          2611,  1560],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,  3580,    11, 44873,  4908,    13,   921,\n",
            "         17753,  1833],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   356,   460,   466,   284,   651,   832,   340,    11, 44873,\n",
            "          4908,    30],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645,  1037,   422,   262,  1230,   526,   314,  1101,   407,\n",
            "           257,  6253],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   784,\n",
            "           366,  2061],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314,  1101,\n",
            "          2611,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   356,\n",
            "           460,  1487,   546,   340,    13,   775,   836,   470,   761,   284,\n",
            "           307, 12008],\n",
            "        [50256,    43,   452,   259,     6,  1842,   318,   262,   691,  1517,\n",
            "           326,   460,   787,   257,   582,  1254,   826,   618,   339,  5300,\n",
            "          2642,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,  1231,   340,    11, 44873,  4908,     0,\n",
            "           921, 17753],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,  2107,   534,  1204,    11, 44873,  4908,    30,   921,  1365,\n",
            "           407,   307]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([ 0,  4, 22, 41, 32,  5,  8, 27,  7,  3, 35,  5,  2, 33, 21, 14, 17, 43,\n",
            "        24,  1, 11, 31, 16,  7, 18, 37, 42,  9,  2, 33, 29, 26, 39, 17, 34,  9,\n",
            "        23, 20, 25, 44,  3, 21, 14, 17, 14], device='cuda:0')\n",
            "topk 22 tensor([  247,  4908,  4908,   286,   286,   286,   314,   340,   761,   284,\n",
            "          607,   287,    11,    11,   262,   262,   262,   307,   775,  1639,\n",
            "           11,   775,   775,   423,   775,   447,   679,   447,    13,    13,\n",
            "         1913,   775,   338,  2642,   466,   775, 14971, 14971,   836,   257,\n",
            "          257,  2642,  2642,   534,   534], device='cuda:0')\n",
            "topk scores22 tensor([1.0000, 1.0000, 1.0000, 0.8094, 0.6666, 0.6224, 0.5658, 0.4459, 0.4253,\n",
            "        0.3186, 0.2982, 0.2970, 0.2915, 0.2867, 0.2689, 0.2670, 0.2665, 0.2589,\n",
            "        0.2421, 0.2368, 0.2307, 0.2297, 0.2156, 0.2154, 0.2135, 0.2117, 0.1997,\n",
            "        0.1991, 0.1980, 0.1953, 0.1894, 0.1877, 0.1861, 0.1833, 0.1827, 0.1735,\n",
            "        0.1732, 0.1684, 0.1680, 0.1581, 0.1563, 0.1562, 0.1548, 0.1545, 0.1508],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "23 torch.Size([45, 23])\n",
            "depth 23 tensor([[50256,    43,   452,  ...,   836,   447,   247],\n",
            "        [50256,    43,   452,  ...,    11, 44873,  4908],\n",
            "        [50256,    43,   452,  ...,    11, 44873,  4908],\n",
            "        ...,\n",
            "        [50256,     1,    40,  ...,  2061,   338,  2642],\n",
            "        [50256,     1,    40,  ...,  2061,   338,   534],\n",
            "        [50256,     1,    40,  ...,  2061,   338,   534]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([ 0, 25, 27, 23, 33, 41, 42,  6,  7, 38,  5, 39,  8,  5, 38,  1, 14, 16,\n",
            "         2, 15, 32, 43, 30, 22, 15, 24, 16, 28,  1, 31, 29, 32, 30, 44, 17,  9,\n",
            "        43, 44, 35, 18, 21, 24, 21, 11, 18], device='cuda:0')\n",
            "topk 23 tensor([   83,   251,   251,   284,   351,   351,   351,  1612,   467,   447,\n",
            "         4571,  9192,   284,  8761,   470,    13,   966,   966,    13,   966,\n",
            "          262,   966,  1576,   761,  1917,   761,  1917,   775,    30,   761,\n",
            "          775,  2642,   290,  1917,   257,   307,  1917,   966,   761,   836,\n",
            "          836, 17753,   761,   674,   761], device='cuda:0')\n",
            "topk scores23 tensor([1.0000, 1.0000, 1.0000, 0.9369, 0.8959, 0.8881, 0.8796, 0.8287, 0.6532,\n",
            "        0.6285, 0.5202, 0.4844, 0.4549, 0.3763, 0.3715, 0.3672, 0.3325, 0.3258,\n",
            "        0.3193, 0.2775, 0.2670, 0.2497, 0.2449, 0.2432, 0.2428, 0.2368, 0.2363,\n",
            "        0.2320, 0.2235, 0.2174, 0.2126, 0.2027, 0.2011, 0.1978, 0.1954, 0.1921,\n",
            "        0.1873, 0.1859, 0.1796, 0.1771, 0.1705, 0.1672, 0.1652, 0.1630, 0.1628],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "24 torch.Size([45, 24])\n",
            "depth 24 tensor([[50256,    43,   452,  ...,   447,   247,    83],\n",
            "        [50256,    43,   452,  ...,    30,   447,   251],\n",
            "        [50256,    43,   452,  ...,    13,   447,   251],\n",
            "        ...,\n",
            "        [50256,    46,  1860,  ...,    13,   775,   761],\n",
            "        [50256,    46,  1860,  ...,  1487,   287,   674],\n",
            "        [50256,    46,  1860,  ...,    13,   775,   761]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([ 9, 31,  7, 16, 19, 17, 37, 21, 39, 40, 33, 14, 36,  0, 43, 20, 24, 40,\n",
            "        26, 39, 15, 28, 26, 12,  3, 18, 24, 33,  8, 24, 36, 26, 34, 27, 30, 13,\n",
            "        14, 27, 30,  0, 10, 17, 20, 15, 19], device='cuda:0')\n",
            "topk 24 tensor([ 247,  351,   30,  286,  286,  286, 1701, 1701,  470,  470, 1701,  761,\n",
            "        1701,  761, 2842,  966, 1701,  447, 1701,  447,  447,  447,  351,  307,\n",
            "         307,  775,  351,   30,  284,   30,   30,   30,  922,  761,  761,  290,\n",
            "         423,  836,  836,  423,  290, 1701, 1917,  775, 1701], device='cuda:0')\n",
            "topk scores24 tensor([1.0000, 0.9176, 0.8848, 0.7873, 0.7718, 0.7610, 0.6889, 0.6797, 0.6797,\n",
            "        0.6710, 0.5907, 0.5626, 0.5611, 0.4677, 0.4473, 0.3874, 0.3507, 0.3290,\n",
            "        0.3277, 0.3203, 0.2978, 0.2909, 0.2782, 0.2764, 0.2431, 0.2362, 0.2343,\n",
            "        0.2091, 0.2070, 0.2063, 0.2026, 0.2019, 0.1954, 0.1930, 0.1910, 0.1813,\n",
            "        0.1735, 0.1706, 0.1674, 0.1649, 0.1433, 0.1397, 0.1373, 0.1360, 0.1356],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "25 torch.Size([45, 25])\n",
            "depth 25 tensor([[50256,    43,   452,  ...,   836,   447,   247],\n",
            "        [50256,     1,    40,  ...,   338,  2642,   351],\n",
            "        [50256,    43,   452,  ...,   314,  1612,    30],\n",
            "        ...,\n",
            "        [50256,     1,    40,  ...,   338,   262,  1917],\n",
            "        [50256,    43,   452,  ...,  4908,    13,   775],\n",
            "        [50256,     1,    40,  ...,   262,   966,  1701]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([17, 19,  0, 20, 21, 39, 15, 28, 37, 38,  8,  9, 14, 13, 27, 30, 35, 42,\n",
            "        38, 29, 37, 42, 31, 32, 22,  5, 25, 26,  3,  4, 42, 43, 25,  3, 43, 32,\n",
            "        29,  5, 32,  8,  9,  3, 31, 15, 43], device='cuda:0')\n",
            "topk 25 tensor([  247,   247,    83,   251,   251,   284,   286,  7030,   470,   470,\n",
            "          761,   761,   286,   284,   921,   921,  4571,   351,   447,   921,\n",
            "          447,  1701,   921,   582,   852,   852,   761,   852,   852,   852,\n",
            "           30,   836,   836,  2877,   761,  2560,   775,  2282,  1048,   765,\n",
            "          765,  2282,   775,  1701, 17753], device='cuda:0')\n",
            "topk scores25 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9560, 0.8152, 0.7084, 0.6871,\n",
            "        0.6713, 0.5535, 0.5508, 0.5036, 0.4825, 0.4225, 0.3836, 0.3687, 0.3520,\n",
            "        0.3287, 0.3231, 0.3129, 0.3101, 0.3075, 0.2596, 0.2428, 0.2341, 0.2281,\n",
            "        0.2003, 0.1765, 0.1694, 0.1692, 0.1688, 0.1637, 0.1386, 0.1312, 0.1310,\n",
            "        0.1307, 0.1295, 0.1288, 0.1262, 0.1255, 0.1137, 0.1112, 0.1072, 0.1068],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "26 torch.Size([45, 26])\n",
            "depth 26 tensor([[50256,    46,  1860,  ...,   836,   447,   247],\n",
            "        [50256,    46,  1860,  ...,   836,   447,   247],\n",
            "        [50256,    43,   452,  ...,   447,   247,    83],\n",
            "        ...,\n",
            "        [50256,     1,    40,  ...,  1917,    30,   775],\n",
            "        [50256,     1,    40,  ...,   262,   966,  1701],\n",
            "        [50256,    43,   452,  ...,    13,   775, 17753]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([18, 20,  0,  1, 29, 28, 32,  2, 31,  8,  9, 25, 33, 12, 27, 31, 37, 32,\n",
            "        12, 30, 24, 13, 17, 38,  6,  5, 27, 36, 35, 23, 42, 35, 41, 33,  7, 23,\n",
            "         8,  9,  6, 11, 10,  2, 24, 38, 37], device='cuda:0')\n",
            "topk 26 tensor([ 247,  247,   83,   83,  257,  257,  447,  761,  447,  761,  761,  257,\n",
            "        1701, 2877,  257,  470,  326,  470, 1204,  921,  257,  307,  852,  284,\n",
            "         852,  307, 2042,  761,  284,  290,  761,  290,  428,   30,  319,  284,\n",
            "         765,  765, 2282,  257,  257,  423, 2042,  290,  428], device='cuda:0')\n",
            "topk scores26 tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.6612, 0.6433, 0.6361, 0.6288, 0.6003,\n",
            "        0.5901, 0.5895, 0.5526, 0.5451, 0.5089, 0.4018, 0.3997, 0.3878, 0.3639,\n",
            "        0.3434, 0.3229, 0.3148, 0.2901, 0.2774, 0.2656, 0.2639, 0.2432, 0.1901,\n",
            "        0.1774, 0.1764, 0.1684, 0.1637, 0.1548, 0.1529, 0.1485, 0.1382, 0.1252,\n",
            "        0.1241, 0.1226, 0.1192, 0.1185, 0.1183, 0.1161, 0.1142, 0.1012, 0.0991],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "27 torch.Size([45, 27])\n",
            "depth 27 tensor([[50256,    46,  1860,  ...,   836,   447,   247],\n",
            "        [50256,    46,  1860,  ...,   836,   447,   247],\n",
            "        [50256,    46,  1860,  ...,   447,   247,    83],\n",
            "        ...,\n",
            "        [50256,     1,    40,  ...,   351,   852,  2042],\n",
            "        [50256,    46,  1860,  ...,   922,  1048,   290],\n",
            "        [50256,     1,    40,  ...,   286,  2282,   428]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([ 6,  8,  1,  0, 35, 32, 33, 17, 31, 44, 15, 16, 24,  3,  2, 43, 28, 23,\n",
            "        29, 14, 42, 26, 42, 26, 22, 38, 20,  5,  4, 27, 16, 30, 44, 11, 11,  2,\n",
            "         3, 32, 15,  9, 22, 10,  4, 38, 38], device='cuda:0')\n",
            "topk 27 tensor([ 247,  247,   83,   83, 3511, 1701,  921,  761,  407, 1701,  761, 1701,\n",
            "         257,  761,  761,  407, 3511, 3511,  407,  582,   30, 1701, 1701,   30,\n",
            "         257,  326,  582,  582,  582,  257,   30,  257,   30,  582, 2802,  765,\n",
            "         765,   30,  423,  257, 2042,  257, 2802, 1701,  428], device='cuda:0')\n",
            "topk scores27 tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.7443, 0.7116, 0.6529, 0.6452, 0.6333,\n",
            "        0.6175, 0.5915, 0.5874, 0.5787, 0.5415, 0.5381, 0.4955, 0.4881, 0.4443,\n",
            "        0.4080, 0.3991, 0.3173, 0.3122, 0.3022, 0.3000, 0.2917, 0.2872, 0.2801,\n",
            "        0.2610, 0.2610, 0.2329, 0.2178, 0.2131, 0.1689, 0.1586, 0.1541, 0.1514,\n",
            "        0.1513, 0.1504, 0.1456, 0.1311, 0.1293, 0.1287, 0.1242, 0.1157, 0.1155],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "28 torch.Size([45, 28])\n",
            "depth 28 tensor([[50256,    43,   452,  ...,   836,   447,   247],\n",
            "        [50256,    43,   452,  ...,   836,   447,   247],\n",
            "        [50256,    46,  1860,  ...,   447,   247,    83],\n",
            "        ...,\n",
            "        [50256,     1,    40,  ...,   852,   257,  2802],\n",
            "        [50256,     1,    40,  ...,   286,  2282,  1701],\n",
            "        [50256,     1,    40,  ...,   286,  2282,   428]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([ 1,  0, 44, 25,  2,  3, 37, 32, 30, 27, 23, 20, 40, 42, 28, 40, 19, 34,\n",
            "        19, 27, 24, 33, 31, 34, 26, 28, 26, 33, 42, 29,  4, 16, 17, 34, 25,  6,\n",
            "        42, 12, 26, 44,  2,  3, 12, 16, 13], device='cuda:0')\n",
            "topk 28 tensor([   83,    83,  1701,  1701,   761,   761,   921,   921,   921,  1701,\n",
            "          921,   921,  1701,  1701,  1701,    30,  1701,    69,    30,    30,\n",
            "          582,  1701,  3554, 31699,    30,    30,  1701,    30,    69,  3554,\n",
            "          290,   290,   290,  1701,    30, 17753, 31699,   582,   508,    30,\n",
            "          765,   765,  2802,    13,   257], device='cuda:0')\n",
            "topk scores28 tensor([1.0000, 1.0000, 0.6893, 0.6434, 0.5790, 0.5771, 0.5689, 0.4887, 0.4770,\n",
            "        0.4418, 0.3969, 0.3899, 0.3777, 0.3720, 0.3537, 0.3509, 0.3380, 0.3355,\n",
            "        0.3347, 0.3146, 0.3012, 0.2953, 0.2919, 0.2893, 0.2839, 0.2822, 0.2586,\n",
            "        0.2486, 0.2475, 0.2460, 0.2386, 0.2322, 0.2278, 0.2176, 0.2003, 0.1743,\n",
            "        0.1662, 0.1601, 0.1587, 0.1584, 0.1502, 0.1495, 0.1340, 0.1252, 0.1240],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "29 torch.Size([45, 29])\n",
            "depth 29 tensor([[50256,    43,   452,  ...,   447,   247,    83],\n",
            "        [50256,    43,   452,  ...,   447,   247,    83],\n",
            "        [50256,     1,    40,  ...,  2282,   428,  1701],\n",
            "        ...,\n",
            "        [50256,     1,    40,  ...,   852,   257,  2802],\n",
            "        [50256,    46,  1860,  ...,   284,  3511,    13],\n",
            "        [50256,    46,  1860,  ...,    83,   761,   257]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([23, 36, 28, 17, 32, 30,  1, 31,  0, 19, 34, 39, 25, 27, 18, 24, 15, 37,\n",
            "        20, 42, 20, 35, 37, 42, 42,  6, 22, 35, 15,  4,  5,  7, 20, 10, 29,  0,\n",
            "        38, 22, 11, 37, 35,  8, 43, 38, 17], device='cuda:0')\n",
            "topk 29 tensor([  259,   259, 12603, 12603,   407,   407,   761,   407,   761,   921,\n",
            "          921,   921,   921,   921,   921,   921,   921,  1701,    30,  1701,\n",
            "         1701,   307,    30,    69, 31699, 17753,   326,   423, 13732,   257,\n",
            "          257, 17753,   508, 17753,   508,   423,   318,   508, 17753,   618,\n",
            "         2107, 17753,  2094,  1595, 19296], device='cuda:0')\n",
            "topk scores29 tensor([0.9963, 0.9901, 0.9353, 0.9218, 0.7370, 0.6873, 0.6707, 0.6542, 0.6398,\n",
            "        0.5239, 0.5146, 0.5067, 0.4944, 0.4638, 0.4619, 0.4322, 0.3821, 0.3757,\n",
            "        0.3426, 0.3162, 0.3148, 0.3143, 0.2792, 0.2505, 0.2446, 0.1971, 0.1960,\n",
            "        0.1567, 0.1382, 0.1289, 0.1282, 0.1144, 0.1118, 0.1068, 0.1024, 0.0976,\n",
            "        0.0976, 0.0965, 0.0918, 0.0917, 0.0899, 0.0866, 0.0835, 0.0787, 0.0782],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "30 torch.Size([45, 30])\n",
            "depth 30 tensor([[50256,     1,    40,  ...,  2802, 31699,   259],\n",
            "        [50256,     1,    40,  ...,  2802, 31699,   259],\n",
            "        [50256,     1,    40,  ...,  2802,    69, 12603],\n",
            "        ...,\n",
            "        [50256,    46,  1860,  ...,  3511,    13,  2094],\n",
            "        [50256,     1,    40,  ...,   582,   508,  1595],\n",
            "        [50256,     1,    40,  ...,  2802,    69, 19296]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([28, 24,  1,  0, 23, 42, 27,  2, 22,  3, 18, 21, 38, 33, 42, 31, 36, 40,\n",
            "        41,  9,  2,  3, 25, 25, 33, 12, 14, 34, 13, 15, 40, 37, 11,  6, 26, 25,\n",
            "        10, 32, 23, 25, 33,  6, 16, 32, 30], device='cuda:0')\n",
            "topk 30 tensor([ 4908,   259,     6,     6, 12603,   470,   257,  1701,   921,  1701,\n",
            "          921,   257,  1833,  1833,   447,  1833,   407,   329,  1833, 17753,\n",
            "           30,    30,   307,  1833,   307, 17753, 17753,   460, 17753, 17753,\n",
            "          287,   460, 17753,   257,   460,  2461, 17753,   318, 19296,   423,\n",
            "          423,   645, 17753,  1595,  1230], device='cuda:0')\n",
            "topk scores30 tensor([0.9988, 0.9932, 0.9111, 0.9083, 0.9055, 0.7167, 0.5088, 0.4775, 0.4627,\n",
            "        0.4372, 0.4048, 0.3628, 0.3294, 0.3155, 0.2832, 0.2718, 0.2654, 0.2613,\n",
            "        0.2592, 0.2414, 0.2359, 0.2300, 0.1963, 0.1859, 0.1848, 0.1659, 0.1653,\n",
            "        0.1510, 0.1451, 0.1431, 0.1425, 0.1413, 0.1246, 0.1220, 0.1130, 0.1050,\n",
            "        0.0961, 0.0955, 0.0945, 0.0944, 0.0931, 0.0912, 0.0903, 0.0899, 0.0834],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "31 torch.Size([45, 31])\n",
            "depth 31 tensor([[50256,     1,    40,  ...,    30, 13732,  4908],\n",
            "        [50256,     1,    40,  ...,  2802, 31699,   259],\n",
            "        [50256,     1,    40,  ..., 31699,   259,     6],\n",
            "        ...,\n",
            "        [50256,     1,    40,  ...,    30,   921, 17753],\n",
            "        [50256,     1,    40,  ...,   582,   508,  1595],\n",
            "        [50256,    46,  1860,  ...,   761,   257,  1230]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2187,  0.3895, -0.4276,  ...,  0.8284, -0.4827,  0.5920],\n",
            "         [ 0.2180,  0.3888, -0.4263,  ...,  0.8289, -0.4838,  0.5912],\n",
            "         [ 0.2183,  0.3894, -0.4270,  ...,  0.8284, -0.4834,  0.5916],\n",
            "         ...,\n",
            "         [ 0.2181,  0.3892, -0.4269,  ...,  0.8294, -0.4832,  0.5909],\n",
            "         [ 0.2177,  0.3892, -0.4267,  ...,  0.8297, -0.4832,  0.5905],\n",
            "         [ 0.2183,  0.3895, -0.4280,  ...,  0.8288, -0.4826,  0.5919]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([ 0,  1, 20, 30,  4, 21, 40, 19, 25, 11, 15, 42, 12, 39,  6, 26, 37, 22,\n",
            "        32,  4, 36, 17, 29, 26, 35,  5,  8, 19, 25,  6, 34, 10, 30, 27,  6, 35,\n",
            "         2, 27, 34, 31, 26, 17, 11, 31, 39], device='cuda:0')\n",
            "topk 31 tensor([50256,     6,   921,   257,  1701,   921,   257,   307,   307,   582,\n",
            "          326,  1833,   326,   257,  1693,   307,   407,   257,  1833,    30,\n",
            "         1833,  3511,  1833,  1833,   534,  1309, 17753,   423,   423,  4320,\n",
            "         1487, 17753,   428,  1487,   922,  3511,  1200,   466,   466,  1487,\n",
            "          423,  4925,  1692,   466,  2461], device='cuda:0')\n",
            "topk scores31 tensor([1.0000, 0.8753, 0.5057, 0.4909, 0.4835, 0.4773, 0.4427, 0.4395, 0.4059,\n",
            "        0.3714, 0.3617, 0.3498, 0.3303, 0.3153, 0.2987, 0.2816, 0.2692, 0.2493,\n",
            "        0.2462, 0.2407, 0.2390, 0.2147, 0.2118, 0.1833, 0.1644, 0.1639, 0.1502,\n",
            "        0.1484, 0.1456, 0.1442, 0.1429, 0.1388, 0.1350, 0.1332, 0.1263, 0.1227,\n",
            "        0.1213, 0.1208, 0.1202, 0.1197, 0.1189, 0.1150, 0.1146, 0.1123, 0.1122],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "ppppp tensor([[5, 9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.1588830833596715\n",
            "ppppp tensor([[8, 2]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.332204510175204\n",
            "ppppp tensor([[7, 2]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.258096538021482\n",
            "ppppp tensor([[4, 9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.127134385045092\n",
            "ppppp tensor([[7, 2]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.258096538021482\n",
            "ppppp tensor([[9, 1]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.1780538303479458\n",
            "[CLS] i was never gonna stay the same [SEP]\n",
            "<|endoftext|>Killing for freedom, I’m a motherfuckin' slave to my manhood<|endoftext|>\n",
            "<|endoftext|>Dope is the only way that we can live our lives together and not be afraid of it, no matter how many lies you tell us to lie<|endoftext|>\n",
            "<|endoftext|>Odd Future is the only way that you can live it, nigga<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "res = generator(model ,keyword = 'religion , race , gangster')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tri_3 = decoder_tokenizer.encode(\"Odd Future is\")\n",
        "tri_4 = decoder_tokenizer.encode(\"Odd Future is\")\n"
      ],
      "metadata": {
        "id": "bAGBEfE0XG2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_wf(model , keyword , f_s):\n",
        "  lyrics = []\n",
        "  lyrics.append(f_s)\n",
        "  last_tri = None\n",
        "  phoneme = None\n",
        "  keyword = keyword_encoder_tokenizer.encode(keyword)\n",
        "  keyword = torch.LongTensor(keyword)[None,:].to(device)\n",
        "  for i in range(1,4):\n",
        "    print('iiii',i)\n",
        "    last_line = lyrics[i-1]\n",
        "    last_line = last_line[:,1:].squeeze()\n",
        "\n",
        "    if count_tri(tri_1 , last_line) >= 1:\n",
        "      last_tri = tri_3\n",
        "    # elif count_tri(tri_2 , last_line) >= 1:\n",
        "    #   last_tri = tri_2\n",
        "      \n",
        "    string = decoder_tokenizer.decode(last_line)\n",
        "    last_line = sentence_encoder_tokenizer.encode(string)\n",
        "    phoneme = sentence_vowel(last_line)\n",
        "    last_line = torch.LongTensor(last_line)[None,:].to(device)\n",
        "    \n",
        "    next_line = beam_search(model, last_line , keyword , phoneme , last_tri=last_tri )\n",
        "    lyrics.append(next_line)\n",
        "  for i in range(len(lyrics)):\n",
        "    sentence = lyrics[i]\n",
        "    print(decoder_tokenizer.decode(sentence.squeeze()))\n",
        "  return lyrics"
      ],
      "metadata": {
        "id": "2B9bf9WcVNlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res1 = generator_wf(model , 'religion , race , gangster' , f_s=res[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqi4oZz1VfJA",
        "outputId": "dd85007a-18e5-4377-bdf5-bfd94d753d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iiii 1\n",
            "1 torch.Size([50, 1])\n",
            "depth 1 tensor([[50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2117,  0.4020, -0.4399,  ...,  0.8197, -0.4950,  0.5803],\n",
            "         [ 0.2124,  0.4019, -0.4397,  ...,  0.8188, -0.4959,  0.5804],\n",
            "         [ 0.2112,  0.4015, -0.4386,  ...,  0.8200, -0.4961,  0.5795],\n",
            "         ...,\n",
            "         [ 0.2119,  0.4022, -0.4396,  ...,  0.8196, -0.4954,  0.5798],\n",
            "         [ 0.2091,  0.4016, -0.4378,  ...,  0.8228, -0.4953,  0.5781],\n",
            "         [ 0.2107,  0.4018, -0.4396,  ...,  0.8208, -0.4950,  0.5797]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0], device='cuda:0')\n",
            "topk 1 tensor([31, 30, 28, 29, 25, 24, 26, 27, 19, 18, 16, 17, 21, 20, 22, 23,  7,  6,\n",
            "         4,  5,  1,  0,  2,  3, 11, 10,  8,  9, 13, 12, 14, 15, 47, 46, 44, 45,\n",
            "        41, 40, 42, 43, 35, 34, 32, 33, 37, 36, 38, 39, 49, 48],\n",
            "       device='cuda:0')\n",
            "topk scores1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "2 torch.Size([50, 2])\n",
            "depth 2 tensor([[50256,    31],\n",
            "        [50256,    30],\n",
            "        [50256,    28],\n",
            "        [50256,    29],\n",
            "        [50256,    25],\n",
            "        [50256,    24],\n",
            "        [50256,    26],\n",
            "        [50256,    27],\n",
            "        [50256,    19],\n",
            "        [50256,    18],\n",
            "        [50256,    16],\n",
            "        [50256,    17],\n",
            "        [50256,    21],\n",
            "        [50256,    20],\n",
            "        [50256,    22],\n",
            "        [50256,    23],\n",
            "        [50256,     7],\n",
            "        [50256,     6],\n",
            "        [50256,     4],\n",
            "        [50256,     5],\n",
            "        [50256,     1],\n",
            "        [50256,     0],\n",
            "        [50256,     2],\n",
            "        [50256,     3],\n",
            "        [50256,    11],\n",
            "        [50256,    10],\n",
            "        [50256,     8],\n",
            "        [50256,     9],\n",
            "        [50256,    13],\n",
            "        [50256,    12],\n",
            "        [50256,    14],\n",
            "        [50256,    15],\n",
            "        [50256,    47],\n",
            "        [50256,    46],\n",
            "        [50256,    44],\n",
            "        [50256,    45],\n",
            "        [50256,    41],\n",
            "        [50256,    40],\n",
            "        [50256,    42],\n",
            "        [50256,    43],\n",
            "        [50256,    35],\n",
            "        [50256,    34],\n",
            "        [50256,    32],\n",
            "        [50256,    33],\n",
            "        [50256,    37],\n",
            "        [50256,    36],\n",
            "        [50256,    38],\n",
            "        [50256,    39],\n",
            "        [50256,    49],\n",
            "        [50256,    48]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2117,  0.4020, -0.4399,  ...,  0.8197, -0.4950,  0.5803],\n",
            "         [ 0.2124,  0.4019, -0.4397,  ...,  0.8188, -0.4959,  0.5804],\n",
            "         [ 0.2112,  0.4015, -0.4386,  ...,  0.8200, -0.4961,  0.5795],\n",
            "         ...,\n",
            "         [ 0.2119,  0.4022, -0.4396,  ...,  0.8196, -0.4954,  0.5798],\n",
            "         [ 0.2091,  0.4016, -0.4378,  ...,  0.8228, -0.4953,  0.5781],\n",
            "         [ 0.2107,  0.4018, -0.4396,  ...,  0.8208, -0.4950,  0.5797]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([17, 35, 43, 12, 46, 33, 42,  5, 45, 14, 31, 47, 32, 15,  8, 36, 13, 34,\n",
            "        41, 48,  6, 28, 22, 44,  9, 10, 11, 34, 31, 22,  8, 10, 39, 32, 38, 40,\n",
            "        49, 34, 38,  9, 10, 46, 20, 48, 16, 33, 13, 37, 36, 37],\n",
            "       device='cuda:0')\n",
            "topk 2 tensor([42323,  6950,  2007,    13, 12375,  1860,   259,    13,  8505,    13,\n",
            "           13,   436,  2433,    13,    13, 15746,    13,  2002, 10277,  2530,\n",
            "          314,  2231,  1303, 19296,    13,    13,    13,   432,   657,    16,\n",
            "          400,  1510,   452, 14650,  4509,  3008,    25,  1689,   747,    12,\n",
            "          301,  3087,    40, 46629,    16,  1219,   812,  1101,   603,   447],\n",
            "       device='cuda:0')\n",
            "topk scores2 tensor([0.9807, 0.8691, 0.7412, 0.6363, 0.6270, 0.5754, 0.5723, 0.5692, 0.5360,\n",
            "        0.5206, 0.5082, 0.4025, 0.4003, 0.3889, 0.3847, 0.3563, 0.3552, 0.3390,\n",
            "        0.3318, 0.3225, 0.3149, 0.2881, 0.2793, 0.2788, 0.2761, 0.2668, 0.2398,\n",
            "        0.2392, 0.2390, 0.2321, 0.2310, 0.2276, 0.2222, 0.2163, 0.2153, 0.1950,\n",
            "        0.1869, 0.1863, 0.1822, 0.1799, 0.1744, 0.1726, 0.1597, 0.1550, 0.1530,\n",
            "        0.1520, 0.1510, 0.1487, 0.1423, 0.1405], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "3 torch.Size([50, 3])\n",
            "depth 3 tensor([[50256,     6, 42323],\n",
            "        [50256,    45,  6950],\n",
            "        [50256,    33,  2007],\n",
            "        [50256,    21,    13],\n",
            "        [50256,    38, 12375],\n",
            "        [50256,    46,  1860],\n",
            "        [50256,    32,   259],\n",
            "        [50256,    24,    13],\n",
            "        [50256,    36,  8505],\n",
            "        [50256,    22,    13],\n",
            "        [50256,    15,    13],\n",
            "        [50256,    39,   436],\n",
            "        [50256,    47,  2433],\n",
            "        [50256,    23,    13],\n",
            "        [50256,    19,    13],\n",
            "        [50256,    41, 15746],\n",
            "        [50256,    20,    13],\n",
            "        [50256,    44,  2002],\n",
            "        [50256,    34, 10277],\n",
            "        [50256,    49,  2530],\n",
            "        [50256,    26,   314],\n",
            "        [50256,    13,  2231],\n",
            "        [50256,     2,  1303],\n",
            "        [50256,    37, 19296],\n",
            "        [50256,    18,    13],\n",
            "        [50256,    16,    13],\n",
            "        [50256,    17,    13],\n",
            "        [50256,    44,   432],\n",
            "        [50256,    15,   657],\n",
            "        [50256,     2,    16],\n",
            "        [50256,    19,   400],\n",
            "        [50256,    16,  1510],\n",
            "        [50256,    43,   452],\n",
            "        [50256,    47, 14650],\n",
            "        [50256,    42,  4509],\n",
            "        [50256,    35,  3008],\n",
            "        [50256,    48,    25],\n",
            "        [50256,    44,  1689],\n",
            "        [50256,    42,   747],\n",
            "        [50256,    18,    12],\n",
            "        [50256,    16,   301],\n",
            "        [50256,    38,  3087],\n",
            "        [50256,     1,    40],\n",
            "        [50256,    49, 46629],\n",
            "        [50256,     7,    16],\n",
            "        [50256,    46,  1219],\n",
            "        [50256,    20,   812],\n",
            "        [50256,    40,  1101],\n",
            "        [50256,    41,   603],\n",
            "        [50256,    40,   447]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2117,  0.4020, -0.4399,  ...,  0.8197, -0.4950,  0.5803],\n",
            "         [ 0.2124,  0.4019, -0.4397,  ...,  0.8188, -0.4959,  0.5804],\n",
            "         [ 0.2112,  0.4015, -0.4386,  ...,  0.8200, -0.4961,  0.5795],\n",
            "         ...,\n",
            "         [ 0.2119,  0.4022, -0.4396,  ...,  0.8196, -0.4954,  0.5798],\n",
            "         [ 0.2091,  0.4016, -0.4378,  ...,  0.8228, -0.4953,  0.5781],\n",
            "         [ 0.2107,  0.4018, -0.4396,  ...,  0.8208, -0.4950,  0.5797]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([ 1, 49, 17, 41,  5, 32, 15,  6, 45,  2, 12, 46, 40, 39, 44, 25, 29, 33,\n",
            "        11, 31, 29, 37, 46, 29, 11, 38, 20, 19, 11, 25, 42, 12,  0, 38, 13, 42,\n",
            "        47, 35, 37, 31, 18, 20, 36, 19, 19, 44,  4, 40,  2, 12],\n",
            "       device='cuda:0')\n",
            "topk 3 tensor([  292,   247,    64,   259, 10898,   259,    88,   470,    11,    11,\n",
            "          278,  1468,   290,    19,    11,   362,    13, 44873,   293,    11,\n",
            "           11,  1297,  2084,    25,  8116,   502,  1101,   351,  2815,    17,\n",
            "         1101,   284,   314,   616,    24,   836,   257,    11,   531,   661,\n",
            "          314,   447,  1867,   832,   287,    25,  1394,  4289,   314,   329],\n",
            "       device='cuda:0')\n",
            "topk scores3 tensor([1.0000, 1.0000, 0.9999, 0.9999, 0.9993, 0.9975, 0.9424, 0.9097, 0.7170,\n",
            "        0.6773, 0.5622, 0.4998, 0.4866, 0.4804, 0.4434, 0.4303, 0.3672, 0.3561,\n",
            "        0.3267, 0.3102, 0.2866, 0.2707, 0.2648, 0.2497, 0.2484, 0.2456, 0.2329,\n",
            "        0.2302, 0.2300, 0.2274, 0.2185, 0.2105, 0.2087, 0.2086, 0.2036, 0.1963,\n",
            "        0.1954, 0.1897, 0.1828, 0.1809, 0.1807, 0.1780, 0.1727, 0.1642, 0.1620,\n",
            "        0.1586, 0.1548, 0.1540, 0.1499, 0.1431], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "4 torch.Size([50, 4])\n",
            "depth 4 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,  2282],\n",
            "        [50256,     6, 42323,   314,  1101,   257,  2802,    69, 12603,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   616],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   407,   257],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282],\n",
            "        [50256,    44,  1689,    11,   314,  1101,  2611,   307,   257,  2802],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   356],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   314],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11],\n",
            "        [50256,    19,   400,  9559,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   262],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   530,   326,   460],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   530,   326,   338],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   284,   651],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   616],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257,  2802,    69, 19296],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([45, 34, 43,  0, 17, 24, 46,  9, 39, 14, 10, 27, 12, 36, 23, 42, 30, 28,\n",
            "        21, 29,  8, 49, 21, 33,  3, 18, 22, 26, 31, 11,  4, 35, 38,  1, 44, 19,\n",
            "        42, 16, 36, 46, 30, 25,  5, 37,  7, 31, 49, 30, 20, 33],\n",
            "       device='cuda:0')\n",
            "topk 10 tensor([  247,   259,   546,    76,  1573,    69,    69,    30,  2292,    30,\n",
            "          314,   460,    30,   502,    11,  2282, 44873,   460,    11,   460,\n",
            "          314,  7356,   286,  1479,   921,   340,   287,  3430,   314,   307,\n",
            "          921,   257,  1487,   257,   460,  2802,   910,   651,   345, 31699,\n",
            "         1048,   651,   314,   307,   314, 44873,   262,  2802, 25670,   661],\n",
            "       device='cuda:0')\n",
            "topk scores10 tensor([1.0000, 1.0000, 0.9862, 0.9835, 0.9407, 0.9206, 0.8222, 0.7908, 0.7334,\n",
            "        0.7139, 0.7108, 0.6842, 0.6574, 0.6498, 0.6429, 0.5918, 0.5229, 0.4822,\n",
            "        0.4460, 0.4120, 0.4109, 0.3558, 0.3413, 0.3400, 0.3276, 0.3216, 0.2955,\n",
            "        0.2942, 0.2933, 0.2773, 0.2602, 0.2222, 0.2152, 0.1954, 0.1804, 0.1795,\n",
            "        0.1785, 0.1777, 0.1749, 0.1723, 0.1617, 0.1554, 0.1513, 0.1488, 0.1464,\n",
            "        0.1464, 0.1456, 0.1356, 0.1314, 0.1291], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "11 torch.Size([50, 11])\n",
            "depth 11 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910,\n",
            "           259],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546],\n",
            "        [50256,    47,  2433,   278,   284,  1793,    11,   314,   447,   247,\n",
            "            76],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   616,\n",
            "          1573],\n",
            "        [50256,    44,  1689,    11,   314,  1101,  2611,   307,   257,  2802,\n",
            "            69],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802,\n",
            "            69],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1612,\n",
            "            30],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,  2282,\n",
            "            30],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   257, 25670,    11,\n",
            "           314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460],\n",
            "        [50256,    44,  1689,    11,   345,   760,   644,   314,  1101,  2282,\n",
            "            30],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "          2282],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   356,\n",
            "           460],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285,\n",
            "            11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   314,\n",
            "           460],\n",
            "        [50256,    40,   447,   247,    76,   257,  2802,    69, 12603,    11,\n",
            "           314],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          7356],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285,\n",
            "           286],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "          1479],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1612,    30,\n",
            "           921],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   284,\n",
            "           307],\n",
            "        [50256,    44,  1689,    11,   345,   760,   644,   314,  1612,    30,\n",
            "           921],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   530,   326,   460,\n",
            "          1487],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247,    76,\n",
            "           257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "           910],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460,\n",
            "           651],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802,\n",
            "         31699],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11,\n",
            "           314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "         44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "           262],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   407,   257,\n",
            "         25670],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "           661]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([45, 16, 36, 39,  0,  1,  6,  8,  5, 47, 35, 15, 40,  2, 21, 27, 25, 34,\n",
            "        13, 38, 13,  9, 35, 47, 12, 14,  7, 46, 23, 38, 34, 41, 40, 26, 43, 26,\n",
            "        37, 44, 18, 31, 42, 10, 11, 14, 44, 33,  3, 41, 28, 29],\n",
            "       device='cuda:0')\n",
            "topk 11 tensor([ 4908,  4908,   259,   259,    76, 30960, 12603,   810, 12603,    69,\n",
            "           69,    30,   326,    11,    11,    11,    11,  1487,    11,    11,\n",
            "          290,   921, 31699, 31699,   921,   314,   921,  6483,   290,   290,\n",
            "          466,   340,   284,  7356,   257,   262,   340,  1101,   362,  2802,\n",
            "         1101,   765,   651, 44873,   447,   582,   257,  1978,   655,   257],\n",
            "       device='cuda:0')\n",
            "topk scores11 tensor([1.0000, 1.0000, 1.0000, 0.9975, 0.9428, 0.9294, 0.8889, 0.8630, 0.8503,\n",
            "        0.6813, 0.6705, 0.6609, 0.6270, 0.5429, 0.5063, 0.4661, 0.4402, 0.3980,\n",
            "        0.3485, 0.3364, 0.3289, 0.3202, 0.3197, 0.3111, 0.2809, 0.2638, 0.2631,\n",
            "        0.2455, 0.2392, 0.2373, 0.2341, 0.2313, 0.2271, 0.2224, 0.2178, 0.2170,\n",
            "        0.2085, 0.1971, 0.1965, 0.1939, 0.1842, 0.1822, 0.1796, 0.1746, 0.1576,\n",
            "        0.1569, 0.1560, 0.1551, 0.1545, 0.1538], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "12 torch.Size([50, 12])\n",
            "depth 12 tensor([[50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "         44873,  4908],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802,\n",
            "         31699,   259],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910,\n",
            "           259, 30960],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802,\n",
            "            69, 12603],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810],\n",
            "        [50256,    44,  1689,    11,   314,  1101,  2611,   307,   257,  2802,\n",
            "            69, 12603],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802,    69],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "          2282,    30],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          7356,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,  1487],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,    11],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,   290],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,  2282,\n",
            "            30,   921],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699],\n",
            "        [50256,    44,  1689,    11,   345,   760,   644,   314,  1101,  2282,\n",
            "            30,   921],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1612,\n",
            "            30,   921],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "           262,  6483],\n",
            "        [50256,    11,   314,   447,   247,    76,   257,   582,   286,   262,\n",
            "          1479,   290],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,   290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,  7356],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460,\n",
            "           651,   340],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314,  1101],\n",
            "        [50256,    16,    13,    17,  1510,   661,   287,   262,   717,  1285,\n",
            "            11,   362],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257,  2802],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11,\n",
            "           314,  1101],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   257, 25670,    11,\n",
            "           314,   765],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11, 44873],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314,   447],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247,    76,\n",
            "           257,   582],\n",
            "        [50256,    47,  2433,   278,   284,  1793,    11,   314,   447,   247,\n",
            "            76,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,  1978],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   284,\n",
            "           307,   257]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([44, 43, 22,  9, 10, 23,  2,  3, 39,  1, 41, 15,  7, 12, 33, 27, 45, 17,\n",
            "         5,  4, 16, 14, 13, 11, 36, 47, 47, 30, 48, 32, 48, 42, 29, 45,  4, 18,\n",
            "        35, 31, 39, 31, 15, 14, 34, 16, 35,  4,  7, 34,  1, 34],\n",
            "       device='cuda:0')\n",
            "topk 12 tensor([  247,  4908,   259, 12603, 12603,   259, 30960,     6,    69,   326,\n",
            "          284,   314,   345,   345,    11,    11,   286,    11,   921,  8066,\n",
            "          314,   314,   314,   921,    11,    11,   290,   329,   765,   307,\n",
            "        18869,   340,   534,   326, 17717,   314,  6483,   826, 31699,    11,\n",
            "          345, 44873,   922, 44873, 10162,  1016,   314,   582,   284,  2988],\n",
            "       device='cuda:0')\n",
            "topk scores12 tensor([1.0000, 1.0000, 0.9963, 0.9959, 0.9656, 0.9256, 0.9024, 0.8676, 0.8148,\n",
            "        0.7337, 0.6316, 0.5663, 0.4892, 0.4759, 0.4505, 0.3792, 0.3571, 0.3541,\n",
            "        0.3306, 0.3301, 0.3014, 0.2942, 0.2827, 0.2811, 0.2807, 0.2706, 0.2700,\n",
            "        0.2557, 0.2523, 0.2348, 0.2198, 0.2159, 0.2144, 0.2144, 0.2017, 0.1848,\n",
            "        0.1814, 0.1807, 0.1755, 0.1701, 0.1689, 0.1687, 0.1677, 0.1631, 0.1598,\n",
            "        0.1574, 0.1557, 0.1532, 0.1532, 0.1464], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "13 torch.Size([50, 13])\n",
            "depth 13 tensor([[50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314,   447,   247],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11, 44873,  4908],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699,   259],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802,    69, 12603],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   259],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259, 30960],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257,  2802,\n",
            "         31699,   259,     6],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257,  2802,    69],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   257, 25670,    11,\n",
            "           314,   765,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,  7356,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "           262,  6483,    11],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247,    76,\n",
            "           257,   582,   286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,  1487,    11],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910,\n",
            "           259, 30960,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          7356,    11,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "          2282,    30,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460,\n",
            "           651,   340,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,  1978,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,  1978,   290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655, 18869],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,   290,   534],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247,    76,\n",
            "           257,   582,   326],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262,  6483],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257,  2802, 31699],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          7356,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   922],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262, 10162],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,  2988]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([41, 43, 34,  0,  8,  5,  2, 45, 32,  3, 42, 28, 27, 36, 15, 46,  9, 12,\n",
            "        44, 19,  6, 37, 14, 12, 24, 47,  3, 31, 35, 46, 30, 14, 17, 47, 39, 25,\n",
            "        20, 13, 40, 37, 16, 15, 19, 22, 22, 29, 29, 35, 42, 48],\n",
            "       device='cuda:0')\n",
            "topk 13 tensor([ 4908,  4908,   259,    76, 12603,     6,     6,   284,  1641,   326,\n",
            "          582,   284,   345,    11,   314,  1101,   345,   821,    11,   307,\n",
            "          921,   783, 44873,   460, 44873,   286,   284,    11,   447,   460,\n",
            "          307,   314, 44873,   284, 44873, 44873,   655,   815,   760,   994,\n",
            "          616, 44873,   651,   655,  1101,   257,   287,  2911,  1048,   307],\n",
            "       device='cuda:0')\n",
            "topk scores13 tensor([1.0000, 1.0000, 1.0000, 0.9750, 0.9586, 0.9476, 0.8166, 0.7392, 0.7381,\n",
            "        0.5737, 0.4667, 0.4063, 0.3872, 0.3594, 0.3293, 0.3199, 0.3158, 0.3146,\n",
            "        0.3041, 0.2763, 0.2675, 0.2623, 0.2584, 0.2475, 0.2451, 0.2358, 0.2323,\n",
            "        0.2272, 0.2265, 0.2167, 0.2159, 0.2142, 0.2092, 0.2038, 0.1974, 0.1798,\n",
            "        0.1784, 0.1745, 0.1715, 0.1705, 0.1702, 0.1683, 0.1607, 0.1580, 0.1556,\n",
            "        0.1416, 0.1413, 0.1405, 0.1388, 0.1367], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "14 torch.Size([50, 14])\n",
            "depth 14 tensor([[50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "          7356,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314,   447,   247,    76],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257,  2802,    69, 12603],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802, 31699,   259,     6],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699,   259,     6],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,   290,   534,  1641],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   922,   582],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262,  6483,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "           262,  6483,    11,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,  1101],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262, 10162,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259, 30960,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,  7356,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   460],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460,\n",
            "           651,   340,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,    11],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,   447],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,   460],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655, 18869,   307],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,  7356,    11,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,  1487,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,  1978,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247,    76,\n",
            "           257,   582,   286,   616],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "           262,  6483,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   257],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   922,  1048],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   284,   307]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([24, 34, 41, 22, 32, 35, 28, 40,  2, 25, 15, 17, 39, 21, 38,  8, 37,  7,\n",
            "        43, 47, 43, 44,  9, 27, 13, 21, 36, 42, 36, 48, 18, 12,  7, 19, 13, 11,\n",
            "        10, 18, 12, 33, 16, 46, 49, 47, 46, 44, 30, 48, 39, 38],\n",
            "       device='cuda:0')\n",
            "topk 14 tensor([ 4908,  4908,  4908,  4908,  4908,  4908,   247,  1573,     6,   616,\n",
            "          407,   407,   287,    11,   644,    11,   307,   307, 18869,   345,\n",
            "          765,   655,   345, 44873, 44873,    13, 18869,   340,   765,   284,\n",
            "        44873,   284,   651,   257,   314,   307,    11,   314,    11,  2107,\n",
            "          815,   428,   257,   484,   257,   407,   257,    11,    11,  1701],\n",
            "       device='cuda:0')\n",
            "topk scores14 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9439, 0.8108,\n",
            "        0.7595, 0.7181, 0.6150, 0.5023, 0.4328, 0.4234, 0.4175, 0.4025, 0.3782,\n",
            "        0.3730, 0.3471, 0.3447, 0.3376, 0.3241, 0.3120, 0.2986, 0.2842, 0.2679,\n",
            "        0.2650, 0.2637, 0.2580, 0.2423, 0.2413, 0.2313, 0.2167, 0.2159, 0.2158,\n",
            "        0.2148, 0.2048, 0.1987, 0.1984, 0.1953, 0.1854, 0.1768, 0.1653, 0.1651,\n",
            "        0.1629, 0.1604, 0.1597, 0.1581, 0.1378], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "15 torch.Size([50, 15])\n",
            "depth 15 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460,\n",
            "           651,   340,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345,   287,\n",
            "           262,  6483,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,  7356,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,  1487,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,  1978,    11, 44873,  4908],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,   447,   247],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247,    76,\n",
            "           257,   582,   286,   616,  1573],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,  1101,   407],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821,   407],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           345,   290,   534,  1641,    11],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911,   345],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262,  6483,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655, 18869],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655,   765],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   922,  1048,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262, 10162,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   651],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262,  6483,    11,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   922,   582,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262, 10162,    11,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   284,  2107],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   284,   307,   257],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911,   484],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655, 18869,   307,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   922,  1048,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,  1701]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([ 0, 30, 24,  6, 44, 14,  9, 11, 49, 28, 20, 21, 48,  9, 43, 40, 13, 38,\n",
            "        32, 10, 17, 46, 19, 27,  4, 39, 12,  5, 45,  8, 26, 33, 18, 22, 18, 12,\n",
            "        27, 47, 41, 16, 31,  8, 21,  4, 27, 35, 39,  1, 31, 41],\n",
            "       device='cuda:0')\n",
            "topk 15 tensor([50256,  4908,  4908,    76,  2292,   314,   898,  3142,   366,   284,\n",
            "          284,  2282, 44873,  1573,   836,   307, 44873, 44873,   340,  3142,\n",
            "          257,   582,   836,    11,    13,   616,  2253,    30,   257,  1204,\n",
            "          307,   582,   307,   815,   910,   428,   826, 44873,  2292, 22461,\n",
            "          766,   287,  1561,    30,  1760,   257,  1204,    30,   307,   983],\n",
            "       device='cuda:0')\n",
            "topk scores15 tensor([1.0000, 1.0000, 1.0000, 0.9642, 0.7972, 0.6005, 0.5701, 0.5047, 0.4543,\n",
            "        0.4215, 0.3937, 0.3730, 0.3684, 0.3595, 0.3508, 0.3497, 0.3261, 0.3110,\n",
            "        0.2855, 0.2795, 0.2791, 0.2576, 0.2451, 0.2248, 0.2182, 0.2155, 0.2153,\n",
            "        0.2152, 0.2090, 0.2073, 0.2071, 0.2023, 0.1978, 0.1914, 0.1721, 0.1703,\n",
            "        0.1673, 0.1610, 0.1588, 0.1560, 0.1541, 0.1526, 0.1488, 0.1443, 0.1378,\n",
            "        0.1367, 0.1359, 0.1298, 0.1290, 0.1280], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "16 torch.Size([49, 16])\n",
            "depth 16 tensor([[50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262, 10162,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262,  6483,    11, 44873,  4908],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,   447,   247,    76],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   257,  2292],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616,   898],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821,   407,  3142],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,  1701,   366],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655,   765,   284],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655,  2282],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616,  1573],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911,   484,   836],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   651,   340],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,  1101,   407,  3142],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655, 18869,   307,   257,   582],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911,   345,   836],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,  1487,    11, 44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   284,  2107,   616],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,  2253],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,  1978,    11, 44873,  4908,    30],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,  1204],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655, 18869,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   922,  1048,    11, 44873],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,  2292],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307, 22461],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655,  1561],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,  1487,    11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,  1760],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   284,  2107,  1204],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,    11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([49, 50257])\n",
            "prev word tensor([ 0, 15, 11, 16, 41, 24, 21, 38,  4, 13, 43, 34,  3, 13, 10, 40, 32, 17,\n",
            "        12, 47,  5, 35, 33, 14, 35,  5, 22, 39, 12,  7, 21, 34, 44, 30, 19,  8,\n",
            "         2,  4, 47, 44,  9, 39, 28, 48,  9, 25, 40, 27, 34], device='cuda:0')\n",
            "topk 16 tensor([50256,  4908,  4908,  4908,   259,  1204,   447,   284,  1612,   447,\n",
            "           11,  1499,   810,   470,   326,   428,   307,    11,    11,  3772,\n",
            "         8178,   994,   326, 22461,   783, 22701, 44873,   502,   290,  1639,\n",
            "          470,  3277,   582,   286,   582,   307,   257,  1101,  1479,   636,\n",
            "          307,   832,    11,   351,   910,    11,   257, 25670,   995],\n",
            "       device='cuda:0')\n",
            "topk scores16 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9704, 0.7612, 0.6930, 0.6133,\n",
            "        0.5968, 0.5353, 0.4913, 0.4702, 0.4032, 0.3763, 0.3719, 0.3615, 0.3394,\n",
            "        0.3307, 0.3183, 0.3057, 0.2726, 0.2717, 0.2700, 0.2649, 0.2622, 0.2619,\n",
            "        0.2452, 0.2443, 0.2420, 0.2387, 0.2302, 0.2123, 0.2121, 0.2093, 0.2040,\n",
            "        0.2008, 0.1885, 0.1751, 0.1713, 0.1706, 0.1697, 0.1677, 0.1665, 0.1583,\n",
            "        0.1574, 0.1513, 0.1491, 0.1424], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "17 torch.Size([48, 17])\n",
            "depth 17 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655,  1561,   259],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   284,  2107,   616,  1204],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911,   345,   836,   447],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307, 22461,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911,   484,   836,   447],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,  1760,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  1499],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   257,  2292,   810],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911,   484,   836,   470],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655,  2282,   326],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   651,   340,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616,  1573,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616,   898,  8178],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307, 22461],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   783],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616,   898, 22701],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   502],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616,  1573,   290],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,  1701,   366,  1639],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911,   345,   836,   470],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   582],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655,   765,   284,   307],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,  1204,    11],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,  2253,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257, 25670],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([48, 50257])\n",
            "prev word tensor([ 0, 25,  5, 38,  3, 32, 18, 19, 14, 22, 36, 42, 20,  7, 15,  7, 23, 41,\n",
            "        11, 30, 45, 43, 24,  9,  6, 39, 44, 24, 47, 33, 21,  2, 16, 40,  1, 26,\n",
            "        36, 20, 37, 27, 23, 18,  0,  2,  6, 14,  4, 46], device='cuda:0')\n",
            "topk 17 tensor([50256,  4908,   247,   286,     6,   616,   351,   351,   995,   284,\n",
            "         2282,   645,   287,  1701, 22461,    30,    11, 44873,   345,   286,\n",
            "          995,   326,    11, 44873,  9159,  1598, 44873,    13,   810,   286,\n",
            "          526,    13, 44873,   340,    30,   287,   910,    11,   422,   407,\n",
            "           13,   546,    30,    30,   466,  1204,    11,   526],\n",
            "       device='cuda:0')\n",
            "topk scores17 tensor([1.0000, 1.0000, 1.0000, 0.9969, 0.9576, 0.7798, 0.7185, 0.6619, 0.6183,\n",
            "        0.6050, 0.5775, 0.5721, 0.4865, 0.4831, 0.4722, 0.4405, 0.4152, 0.4054,\n",
            "        0.3995, 0.3495, 0.3336, 0.3171, 0.3149, 0.3124, 0.3106, 0.3103, 0.2924,\n",
            "        0.2898, 0.2856, 0.2668, 0.2597, 0.2568, 0.2427, 0.2401, 0.2293, 0.2258,\n",
            "        0.2244, 0.2137, 0.1997, 0.1955, 0.1947, 0.1920, 0.1880, 0.1730, 0.1634,\n",
            "        0.1553, 0.1521, 0.1507], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "18 torch.Size([47, 18])\n",
            "depth 18 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,    11, 44873,  4908],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911,   345,   836,   447,   247],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655,  1561,   259,     6],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616,   898,  8178,   351],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428,   995],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307, 22461,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612,  1701],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   783,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,  1204,    11, 44873],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   257,  2292,   810,   345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616,   898, 22701,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,  1760,    11, 44873],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307, 22461,   284,  9159],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,  2253,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616,   898, 22701,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   810],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,    11, 44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   651,   340,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,    11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   502,   287],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   286,   616,  1573,   290,   407],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   783,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   546],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,    11, 44873,  4908,    30],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307, 22461,   284,   466],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428,  1204],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           307,   257,   582,   284,  2107,   616,  1204,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257, 25670,   526]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([47, 50257])\n",
            "prev word tensor([ 0, 31, 22, 25, 35,  1, 18, 19, 28,  9, 13, 12,  4,  4, 36, 46, 43, 29,\n",
            "        14,  8, 17,  2, 20, 15,  9, 32, 44, 44, 40,  0, 10,  7, 24, 11, 29,  8,\n",
            "        37,  5, 17,  8, 46, 29,  2, 37,  5, 40, 11], device='cuda:0')\n",
            "topk 18 tensor([50256,  4908,  4908,  4908,   259,    83, 16903,   810,   616,    30,\n",
            "          284,   366,  1573,   898, 44873,   784,   340,   784,   921,  9159,\n",
            "          460,   340,   526, 44873,  1701,    11,   826,    11,    30,    30,\n",
            "         2461,   326,   526,   428,   366,   910,   340,  3511,   821,   466,\n",
            "          851,   851,   262,   428,   502,    11,  2253], device='cuda:0')\n",
            "topk scores18 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998, 0.9907, 0.8126, 0.8054,\n",
            "        0.5960, 0.5846, 0.4893, 0.4530, 0.4449, 0.3692, 0.3656, 0.3535, 0.3415,\n",
            "        0.3387, 0.3296, 0.3293, 0.3278, 0.3276, 0.3246, 0.2946, 0.2540, 0.2415,\n",
            "        0.2412, 0.2266, 0.2250, 0.2213, 0.2085, 0.2012, 0.1984, 0.1953, 0.1900,\n",
            "        0.1899, 0.1884, 0.1877, 0.1863, 0.1801, 0.1786, 0.1758, 0.1736, 0.1736,\n",
            "        0.1647, 0.1603], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "19 torch.Size([46, 19])\n",
            "depth 19 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   651,   340,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,  1760,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,  2253,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910,   259],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262,   661,   326,  1842,\n",
            "           502,    11,   314,  2911,   345,   836,   447,   247,    83],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995,   810],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282,    30],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612,  1701,   366],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,  1573],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257, 25670,   526,   784],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   326,   345,   815,   307, 22461,   284,   466,   340],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   784],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612,    30,   921],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307, 22461,   284,  9159],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   257,  2292,   810,   345,   460],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   783,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282,  1701],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428,  1204,   826],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428,  1204,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   546,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,    11, 44873,  4908,    30],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428,   995,   326],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   366],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307, 22461,   284,   910],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,  3511],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   257,  2292,   810,   345,   821],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307, 22461,   284,   466],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257, 25670,   526,   851],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   851],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   546,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,  2253]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([46, 50257])\n",
            "prev word tensor([ 0, 13, 25, 29,  3, 19,  7, 23, 26, 38, 31,  7, 32, 11,  8, 21, 24, 41,\n",
            "        43, 35,  5, 12,  9, 10,  5,  2, 32, 41, 36,  0, 12,  1, 20, 37, 11, 45,\n",
            "         6, 42,  9, 32, 21, 12,  2, 16,  9, 21], device='cuda:0')\n",
            "topk 19 tensor([50256,  4908,   783,   329, 30960,   470,   898,   366, 44873,   340,\n",
            "          784,  1573,  1499,    11,   921,   784, 44873,  5273,    11,    11,\n",
            "           13,  8178,  9159,  1639,    11,    30,  3277,   983,    11,    30,\n",
            "        22701,    30,   526,  4385,   290,    11,   345, 20041,   910,   995,\n",
            "          851,   826,     0,   366,   466,   366], device='cuda:0')\n",
            "topk scores19 tensor([1.0000, 1.0000, 0.9127, 0.8807, 0.8440, 0.8027, 0.5172, 0.4784, 0.4638,\n",
            "        0.4352, 0.4147, 0.3981, 0.3963, 0.3958, 0.3863, 0.3776, 0.3753, 0.3702,\n",
            "        0.3384, 0.3229, 0.3200, 0.3026, 0.2941, 0.2693, 0.2648, 0.2619, 0.2547,\n",
            "        0.2473, 0.2464, 0.2400, 0.2324, 0.2244, 0.2096, 0.2075, 0.2034, 0.1977,\n",
            "        0.1960, 0.1906, 0.1868, 0.1840, 0.1785, 0.1752, 0.1749, 0.1658, 0.1627,\n",
            "        0.1616], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "20 torch.Size([45, 20])\n",
            "depth 20 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428,  1204,   826,   783],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461,   329],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910,   259, 30960],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   257,  2292,   810,   345,   460,   470],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282,  1701,   366],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428,  1204,    11, 44873],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "         44873,  4908,   326,   345,   815,   307, 22461,   284,   466,   340],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   784],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,  1573],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,  1499],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,  1573,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282,    30,   921],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   784],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262,  5273],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,  8178],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,  9159],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1612,  1701,   366,  1639],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,  2253,    11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,  3277],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262,   983],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,  3511,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   651,   340,    11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898, 22701],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,  1760,    11, 44873,  4908,    30],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   257,  2292,   810,   345,   821,  4385],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,  1573,   290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,  2253,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995,   810,   345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   910],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   851],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,   826],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,  2253,    11, 44873,  4908,     0],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   784,   366],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   466],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   366]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([ 0, 15, 20,  3,  1, 10, 23, 31, 25, 18, 29,  0,  5, 40, 34, 35, 27,  6,\n",
            "        17, 29, 43,  5, 40,  1, 10, 38, 12,  2, 36, 36, 33, 36, 31, 35,  4, 38,\n",
            "        31, 18, 44, 16,  5, 19, 17, 14, 37], device='cuda:0')\n",
            "topk 20 tensor([50256,  4908,   351,   921,    11,    11, 44873,   784,   286, 44873,\n",
            "           11,    30,  8178,    11, 44873,   460, 44873,  1639, 44873,    13,\n",
            "          340,   826,   290,    13,   290,   810, 44873,   345,    13,    11,\n",
            "          407,   290,   851,   836,   772,   286,   366,   582,  1639,   526,\n",
            "        22701,   775,   582,   366,  7510], device='cuda:0')\n",
            "topk scores20 tensor([1.0000, 1.0000, 0.6500, 0.4769, 0.4642, 0.4149, 0.4093, 0.3946, 0.3571,\n",
            "        0.3554, 0.3334, 0.3211, 0.3170, 0.3001, 0.2946, 0.2733, 0.2661, 0.2617,\n",
            "        0.2515, 0.2354, 0.2316, 0.2314, 0.2174, 0.2173, 0.2138, 0.1840, 0.1803,\n",
            "        0.1703, 0.1703, 0.1700, 0.1682, 0.1668, 0.1600, 0.1536, 0.1513, 0.1440,\n",
            "        0.1438, 0.1428, 0.1389, 0.1388, 0.1349, 0.1343, 0.1338, 0.1316, 0.1281],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "21 torch.Size([44, 21])\n",
            "depth 21 tensor([[50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11, 44873,\n",
            "          4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,  8178,\n",
            "           351],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,   910,   259, 30960,\n",
            "           921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428,  1204,   826,   783,\n",
            "            11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,  1573,\n",
            "            11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    11,\n",
            "         44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           784],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,  3277,\n",
            "           286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "         44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898, 22701,\n",
            "            11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,    11, 44873,  4908,\n",
            "            30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "          8178],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,   826,\n",
            "            11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,  2253,    11,\n",
            "         44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995,   810,   345,\n",
            "           460],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,  3511,    11,\n",
            "         44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   345,   760,   644,   314,  1101,  2282,  1701,   366,\n",
            "          1639],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "         44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898, 22701,\n",
            "            13],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   466,\n",
            "           340],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "           826],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,   826,\n",
            "           290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428,  1204,   826,   783,\n",
            "            13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,  1573,\n",
            "           290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,  1573,    11,\n",
            "         44873],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461,   329,\n",
            "           345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,  1573,   290,\n",
            "           407],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "           290],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           851],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995,   810,   345,\n",
            "           836],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   257,  2292,   810,   345,   460,   470,\n",
            "           772],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           286],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           366],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   366,\n",
            "          1639],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262,  5273,\n",
            "           526],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "         22701],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "           582],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   784,\n",
            "           366],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   910,\n",
            "          7510]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([44, 50257])\n",
            "prev word tensor([ 0,  8, 15,  5, 17, 25,  7, 34, 11, 43, 28, 38,  3, 39, 20,  0, 41, 24,\n",
            "        14, 36,  0, 41,  9, 39, 20, 36,  1, 12, 40, 38,  4, 21, 35, 35, 28, 38,\n",
            "        26, 24, 11,  4, 14, 37, 20, 19], device='cuda:0')\n",
            "topk 21 tensor([50256,  4908,  4908,  4908,  4908,  4908, 16903, 16903,   351,   546,\n",
            "        44873,   784, 44873,    11,    11,    13,    30,   345,   470,    30,\n",
            "           30,    13, 44873,    13,   290,    13,  2461, 44873,   836,   851,\n",
            "        44873,   407,  2061,  1639,   582,   366,    13,   356,    11,  2933,\n",
            "         2107, 18959,    30,  3511], device='cuda:0')\n",
            "topk scores21 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9895, 0.7510, 0.6028,\n",
            "        0.5625, 0.3834, 0.3790, 0.3697, 0.3542, 0.3269, 0.3043, 0.2692, 0.2329,\n",
            "        0.2257, 0.2225, 0.2177, 0.2133, 0.2100, 0.2091, 0.2081, 0.1851, 0.1811,\n",
            "        0.1774, 0.1765, 0.1552, 0.1552, 0.1394, 0.1361, 0.1359, 0.1357, 0.1352,\n",
            "        0.1293, 0.1254, 0.1244, 0.1223, 0.1196, 0.1172, 0.1165, 0.1134],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "22 torch.Size([43, 22])\n",
            "depth 22 tensor([[50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "         44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,  3511,    11,\n",
            "         44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    11,\n",
            "         44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "         44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,  1573,    11,\n",
            "         44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,  3277,\n",
            "           286, 16903],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           286, 16903],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "          8178,   351],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   910,\n",
            "          7510,   546],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262,  5273,\n",
            "           526,   784],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   428,  1204,   826,   783,\n",
            "            11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "         22701,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "           826,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11, 44873,\n",
            "          4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "           582,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810,   345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995,   810,   345,\n",
            "           460,   470],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11, 44873,\n",
            "          4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "           582,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898, 22701,\n",
            "            11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "         22701,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "           826,   290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,  8178,\n",
            "           351,  2461],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,   826,\n",
            "            11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262,  5273,\n",
            "           526,   851],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,  1573,\n",
            "            11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,   826,\n",
            "           290,   407],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           366,  2061],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           366,  1639],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11,   582],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262,  5273,\n",
            "           526,   366],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461,   329,\n",
            "           345,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810,   356],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "          8178,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,  1573,\n",
            "            11,  2933],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995,   810,   345,\n",
            "           460,  2107],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   366,\n",
            "          1639, 18959],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "           826,    30],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   466,\n",
            "           340,  3511]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([43, 50257])\n",
            "prev word tensor([ 0, 26, 21,  9, 29, 25, 27,  3,  0,  1, 27, 42,  6,  4,  5,  1,  3, 33,\n",
            "         2,  7,  5, 16, 19,  0,  2,  2, 33, 31, 36, 37, 17,  6, 14, 12, 35, 24,\n",
            "        18, 35,  4, 34, 20, 39, 15], device='cuda:0')\n",
            "topk 22 tensor([50256,  4908,  4908,  4908,  4908,   284,   470,    30,    30,    30,\n",
            "          447,     0,    11,    30,    11,    13,    13,    13,     0,  2461,\n",
            "           13,   460,   921,    13,    30,    13,    30,   338,  2107, 44873,\n",
            "         2107,    13,   921, 44873,   921,   921,   921,   447,    13,  2061,\n",
            "          921,   290,   921], device='cuda:0')\n",
            "topk scores22 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8737, 0.6110, 0.4037, 0.3961,\n",
            "        0.3900, 0.3890, 0.3346, 0.3242, 0.3175, 0.3088, 0.3066, 0.2883, 0.2705,\n",
            "        0.2635, 0.2435, 0.2424, 0.2366, 0.2250, 0.2246, 0.2183, 0.2172, 0.2171,\n",
            "        0.2059, 0.2047, 0.2032, 0.1983, 0.1947, 0.1935, 0.1919, 0.1913, 0.1821,\n",
            "        0.1816, 0.1777, 0.1699, 0.1624, 0.1624, 0.1614, 0.1606],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "23 torch.Size([42, 23])\n",
            "depth 23 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,   826,\n",
            "            11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898, 22701,\n",
            "            11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,  1573,\n",
            "            11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,  8178,\n",
            "           351,  2461,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836,   470],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "         44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "         44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,  3511,    11,\n",
            "         44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836,   447],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   466,\n",
            "           340,  3511,     0],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           286, 16903,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,  1573,    11,\n",
            "         44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,  3277,\n",
            "           286, 16903,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,  3511,    11,\n",
            "         44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "         44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11,   582,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    11,\n",
            "         44873,  4908,     0],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "          8178,   351,  2461],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,  3277,\n",
            "           286, 16903,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810,   345,   460],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11, 44873,\n",
            "          4908,    30,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "         44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    11,\n",
            "         44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    11,\n",
            "         44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11,   582,    30],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           366,  2061,   338],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810,   356,  2107],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "          8178,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995,   810,   345,\n",
            "           460,   470,  2107],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           286, 16903,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11, 44873,\n",
            "          4908,    13,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "         22701,    11, 44873],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461,   329,\n",
            "           345,    13,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    13,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    30,   921],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461,   329,\n",
            "           345,    13,   447],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,  1573,    11,\n",
            "         44873,  4908,    13],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262,  5273,\n",
            "           526,   366,  2061],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "           582,    13,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995,   810,   345,\n",
            "           460,  2107,   290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "           582,    30,   921]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([42, 50257])\n",
            "prev word tensor([ 0, 28, 32, 36,  5, 13,  2, 10,  1,  0, 29,  3, 27, 20, 39, 31, 26, 41,\n",
            "        11, 35, 34, 21,  2,  7, 38, 27,  8, 22, 14,  6, 15,  3,  5, 33, 25,  1,\n",
            "         0, 26,  1, 16, 33, 35], device='cuda:0')\n",
            "topk 23 tensor([50256,  4908,  4908,   251,   761, 44873,    30,   347,    30,    30,\n",
            "         1231,    30,  1909,   470,   760,   760,   262,   760, 44873,   760,\n",
            "          760,   760,    13,   921,   338,   290,   921,   921,   921,   921,\n",
            "          921,    13,   765,   760,   921,    13,    13,   510,     0,   921,\n",
            "          821,   821], device='cuda:0')\n",
            "topk scores23 tensor([1.0000, 1.0000, 1.0000, 0.9999, 0.4528, 0.4470, 0.4402, 0.4379, 0.4321,\n",
            "        0.4272, 0.3496, 0.3127, 0.2949, 0.2877, 0.2805, 0.2796, 0.2663, 0.2614,\n",
            "        0.2587, 0.2575, 0.2439, 0.2415, 0.2249, 0.2138, 0.2124, 0.2027, 0.1954,\n",
            "        0.1817, 0.1808, 0.1786, 0.1727, 0.1724, 0.1722, 0.1662, 0.1642, 0.1597,\n",
            "        0.1570, 0.1551, 0.1534, 0.1523, 0.1517, 0.1439], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "24 torch.Size([41, 24])\n",
            "depth 24 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "          8178,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "         22701,    11, 44873,  4908],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461,   329,\n",
            "           345,    13,   447,   251],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836,   470,   761],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,  3277,\n",
            "           286, 16903,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11, 44873,  4908,    30],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          2802,    69, 12603,   326,   345,   815,   307, 22461,   284,   466,\n",
            "           340,  3511,     0,   347],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898, 22701,\n",
            "            11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,   826,\n",
            "            11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995,   810,   345,\n",
            "           460,   470,  2107,  1231],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,  1573,\n",
            "            11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810,   356,  2107,  1909],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810,   345,   460,   470],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "           582,    13,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11, 44873,\n",
            "          4908,    13,   921,   760],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           366,  2061,   338,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "           582,    30,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           286, 16903,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    30,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    13,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11, 44873,\n",
            "          4908,    30,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11, 44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "         44873,  4908,    30,   921],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262,  5273,\n",
            "           526,   366,  2061,   338],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810,   356,  2107,   290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,  3511,    11,\n",
            "         44873,  4908,    30,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "         44873,  4908,    13,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,  3511,    11,\n",
            "         44873,  4908,    13,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "         44873,  4908,    30,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "         44873,  4908,    13,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,  1573,\n",
            "            11, 44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836,   470,   765],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461,   329,\n",
            "           345,    13,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11,   582,    30,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898, 22701,\n",
            "            11, 44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898,   826,\n",
            "            11, 44873,  4908,    13],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           366,  2061,   338,   510],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   307,   257,   582,   286,   616,   898, 22701,\n",
            "            11, 44873,  4908,     0],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11,   582,    13,   921],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461,   329,\n",
            "           345,    13,   921,   821],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    30,   921,   821]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([41, 50257])\n",
            "prev word tensor([ 0, 17, 19, 13,  1,  0, 18, 14, 16, 32, 20, 15, 31,  9, 36, 38,  3, 29,\n",
            "        33, 11, 11,  3, 23, 28, 27, 26, 22,  2, 25, 15, 11,  5,  3, 21, 40, 33,\n",
            "        22, 40, 38, 36, 23], device='cuda:0')\n",
            "topk 24 tensor([50256,  4908,   644,   644,    30,    30,   644,   644,   644,   644,\n",
            "          644,  1917,   284,   502,  1701,   760,   284,   760,   760,    11,\n",
            "           13,   257,   510,   760,   760,   760,   760,   347,   760,   966,\n",
            "          290,   921,   645,   921,   407,   821,   821,   257,   821,    30,\n",
            "          534], device='cuda:0')\n",
            "topk scores24 tensor([1.0000, 1.0000, 0.5364, 0.5050, 0.4940, 0.4934, 0.4711, 0.4690, 0.4484,\n",
            "        0.4269, 0.4115, 0.3569, 0.3549, 0.2978, 0.2829, 0.2663, 0.2647, 0.2571,\n",
            "        0.2568, 0.2535, 0.2436, 0.2419, 0.2409, 0.2384, 0.2126, 0.2104, 0.2094,\n",
            "        0.2089, 0.2045, 0.2016, 0.1983, 0.1976, 0.1672, 0.1590, 0.1509, 0.1507,\n",
            "        0.1385, 0.1378, 0.1330, 0.1322, 0.1284], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "25 torch.Size([40, 25])\n",
            "depth 25 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           286, 16903,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    13,   921,   760,   644],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "           582,    13,   921,   760,   644],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "         22701,    11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  1016,   284,   307,   257,   582,   286,   616,   898,\n",
            "          8178,    11, 44873,  4908,    30],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    30,   921,   760,   644],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11, 44873,\n",
            "          4908,    13,   921,   760,   644],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "           582,    30,   921,   760,   644],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461,   329,\n",
            "           345,    13,   921,   760,   644],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   766,   832,   340,    11, 44873,\n",
            "          4908,    30,   921,   760,   644],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           366,  2061,   338,   262,  1917],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836,   470,   765,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76, 17717,   259,     6,   287,   257,   995,   810,   345,\n",
            "           460,   470,  2107,  1231,   502],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           366,  2061,   338,   510,  1701],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11,   582,    13,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836,   470,   761,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "         44873,  4908,    13,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11,   582,    30,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810,   356,  2107,  1909,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810,   356,  2107,  1909,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836,   470,   761,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262,  5273,\n",
            "           526,   366,  2061,   338,   510],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,   502,    11,\n",
            "         44873,  4908,    30,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,  3511,    11,\n",
            "         44873,  4908,    13,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "         44873,  4908,    13,   921,   760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "         44873,  4908,    30,   921,   760],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   262,  2099,   286,\n",
            "          1048,   284,   307,   287,   428,   983,   351,   645,  2461,   329,\n",
            "           345,    13,   447,   251,   347],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  3772,   351,  3511,    11,\n",
            "         44873,  4908,    30,   921,   760],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           366,  2061,   338,   262,   966],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   447,\n",
            "           247,    76,  8066,   651,   340,   826,   994,   287,   428,   995,\n",
            "           810,   356,  2107,  1909,   290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11, 44873,  4908,    30,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836,   470,   761,   645],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11, 44873,  4908,    13,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    30,   921,   821,   407],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11,   582,    30,   921,   821],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "         44873,  4908,    30,   921,   821],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   340,    11,\n",
            "           582,    30,   921,   821,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326,   314,\n",
            "           460,   466,   329,   345,   284,   307,  1479,   422,   428, 20041,\n",
            "            11,   582,    13,   921,   821],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   340,   526,\n",
            "           366,  2061,   338,   510,    30],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   636,   286,   262,  5273,\n",
            "           526,   366,  2061,   338,   534]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([40, 50257])\n",
            "prev word tensor([ 0, 24, 23, 16, 17, 25,  0, 27, 22, 10, 21, 12, 39, 38, 11, 15, 18, 32,\n",
            "        10, 10, 30, 39, 28, 36, 35, 37, 33, 35, 34, 34, 30, 11, 37, 11, 21, 32,\n",
            "        20, 12,  0,  0], device='cuda:0')\n",
            "topk 25 tensor([50256,   644,   644,   644,   644,   644,    30,   644,   644,  1701,\n",
            "         1701,    11,   966,   921,   307,   307, 44873,   760,    30,   351,\n",
            "          760,  1917,  1701,  9372,   407,   407,   257,   257,   407,   257,\n",
            "          821,   766,   257,  1309,    30,   821,  1230,    13,    13,     0],\n",
            "       device='cuda:0')\n",
            "topk scores25 tensor([0.5824, 0.5575, 0.5300, 0.5266, 0.5146, 0.4787, 0.4762, 0.4626, 0.4574,\n",
            "        0.3254, 0.3144, 0.3021, 0.2727, 0.2686, 0.2534, 0.2529, 0.2392, 0.2263,\n",
            "        0.2231, 0.2199, 0.2133, 0.2070, 0.1862, 0.1822, 0.1808, 0.1671, 0.1619,\n",
            "        0.1477, 0.1439, 0.1432, 0.1424, 0.1341, 0.1308, 0.1261, 0.1244, 0.1211,\n",
            "        0.1209, 0.1200, 0.1142, 0.1111], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "26 torch.Size([39, 26])\n",
            "depth 26 tensor([[50256,    46,  1860,  ...,   921,   760,   644],\n",
            "        [50256,    46,  1860,  ...,   921,   760,   644],\n",
            "        [50256,    46,  1860,  ...,   921,   760,   644],\n",
            "        ...,\n",
            "        [50256,    46,  1860,  ...,  1231,   502,    13],\n",
            "        [50256,    46,  1860,  ..., 44873,  4908,    13],\n",
            "        [50256,    46,  1860,  ..., 44873,  4908,     0]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([39, 50257])\n",
            "prev word tensor([15, 11, 16, 20, 19, 17, 35, 10, 35, 18, 28, 33, 20, 34, 31, 24, 29, 27,\n",
            "        18, 23, 36, 26, 29, 14, 30, 34, 12, 11,  5, 28, 25, 22, 13, 26, 38, 25,\n",
            "        37, 31, 30], device='cuda:0')\n",
            "topk 26 tensor([ 4908,  1701,   644,  1701,   644,   921,   326, 44873,   284,   852,\n",
            "         9372,   921,    30,   407,  9372,   257,   407,   257,   326,   257,\n",
            "          921,  9372,   257, 22461,  1194,   257, 17753,    30,   921, 46733,\n",
            "          636,  9192,  1364, 46733,   921,  9372,   921, 46733,   257],\n",
            "       device='cuda:0')\n",
            "topk scores26 tensor([1.0000, 0.6549, 0.5877, 0.5660, 0.5076, 0.3240, 0.2981, 0.2839, 0.2784,\n",
            "        0.2480, 0.2430, 0.2163, 0.2160, 0.2037, 0.1952, 0.1882, 0.1771, 0.1770,\n",
            "        0.1755, 0.1667, 0.1644, 0.1636, 0.1551, 0.1531, 0.1502, 0.1412, 0.1368,\n",
            "        0.1280, 0.1172, 0.1131, 0.1081, 0.1056, 0.1027, 0.1020, 0.1006, 0.0962,\n",
            "        0.0961, 0.0929, 0.0926], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "27 torch.Size([39, 27])\n",
            "depth 27 tensor([[50256,    46,  1860,  ...,    11, 44873,  4908],\n",
            "        [50256,     1,    40,  ...,   534,   966,  1701],\n",
            "        [50256,    46,  1860,  ...,   921,   760,   644],\n",
            "        ...,\n",
            "        [50256,    46,  1860,  ...,  4908,    13,   921],\n",
            "        [50256,    46,  1860,  ...,   821,   257, 46733],\n",
            "        [50256,    46,  1860,  ...,   284,   766,   257]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([39, 50257])\n",
            "prev word tensor([ 0, 30, 37, 33, 29, 18,  0,  8, 32, 12, 27, 24, 36, 28, 18,  6, 34, 22,\n",
            "        13, 32, 16, 25, 31, 38, 26, 17, 34, 19, 26, 11, 28, 20, 38, 31, 14, 22,\n",
            "        20, 15, 10], device='cuda:0')\n",
            "topk 27 tensor([50256,   286,     6,     6,     6,  1701,    30,   466,  2157,   921,\n",
            "          921,  1110,   760,   760,    30,   338,   760,  9372,   257,  3436,\n",
            "          257,  9372,   290, 44873,   423,  9372,  1365,   636,  1833, 17753,\n",
            "         1365,   836,   582,     0,  9192, 46733,   760,  9372,  9192],\n",
            "       device='cuda:0')\n",
            "topk scores27 tensor([1.0000, 0.9997, 0.9223, 0.9057, 0.8845, 0.7391, 0.7125, 0.3267, 0.3265,\n",
            "        0.3234, 0.3120, 0.2914, 0.2726, 0.2488, 0.2316, 0.2293, 0.2276, 0.2247,\n",
            "        0.1905, 0.1865, 0.1806, 0.1786, 0.1493, 0.1349, 0.1346, 0.1333, 0.1321,\n",
            "        0.1288, 0.1278, 0.1261, 0.1258, 0.1206, 0.1192, 0.1162, 0.1106, 0.1103,\n",
            "        0.1085, 0.1077, 0.1072], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "28 torch.Size([38, 28])\n",
            "depth 28 tensor([[50256,    46,  1860,  ...,   257,   636,   286],\n",
            "        [50256,    46,  1860,  ...,   257, 46733,     6],\n",
            "        [50256,    46,  1860,  ...,   257, 46733,     6],\n",
            "        ...,\n",
            "        [50256,    46,  1860,  ...,    13,   921,   760],\n",
            "        [50256,    46,  1860,  ...,   407,   257,  9372],\n",
            "        [50256,    46,  1860,  ...,   257,  9372,  9192]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([38, 50257])\n",
            "prev word tensor([22, 26, 34,  0, 15, 11, 12, 13, 18, 35,  7,  6, 31, 25, 29, 37, 33, 28,\n",
            "         5,  9, 27,  8, 37, 31,  7, 28, 20, 24, 35, 14, 19, 19, 36, 10, 16, 17,\n",
            "        19, 29], device='cuda:0')\n",
            "topk 28 tensor([4908,  286,    6,  502,  644,  644,  644,  921,   11,  644,   11,  326,\n",
            "         326,  407,  407,  290,  290,  423,  921,  821,  326,  821,    0,  508,\n",
            "         588, 1833, 9192, 9192,   11,  407,  636,  582, 9192,  588, 9192,  582,\n",
            "        9372,  423], device='cuda:0')\n",
            "topk scores28 tensor([1.0000, 0.9998, 0.7948, 0.7031, 0.5857, 0.5321, 0.4471, 0.4387, 0.3780,\n",
            "        0.3716, 0.2871, 0.2351, 0.1770, 0.1752, 0.1629, 0.1574, 0.1548, 0.1528,\n",
            "        0.1437, 0.1408, 0.1366, 0.1338, 0.1268, 0.1266, 0.1229, 0.1159, 0.1143,\n",
            "        0.1102, 0.1100, 0.1068, 0.1066, 0.1056, 0.1049, 0.1016, 0.1002, 0.0998,\n",
            "        0.0997, 0.0959], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "29 torch.Size([38, 29])\n",
            "depth 29 tensor([[50256,    46,  1860,  ...,   257, 44873,  4908],\n",
            "        [50256,    46,  1860,  ...,   257,   636,   286],\n",
            "        [50256,    46,  1860,  ...,   257, 46733,     6],\n",
            "        ...,\n",
            "        [50256,    46,  1860,  ...,   407,   257,   582],\n",
            "        [50256,    46,  1860,  ...,   407,   257,  9372],\n",
            "        [50256,    46,  1860,  ...,   921,  1365,   423]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([38, 50257])\n",
            "prev word tensor([30,  1, 19, 21, 12, 37, 18, 13, 14, 11, 23,  0, 17, 34, 26, 25, 11, 34,\n",
            "        31, 24,  8, 35, 33, 13, 18, 36, 14,  2,  0, 37, 10,  7, 36, 20, 34, 29,\n",
            "         2,  0], device='cuda:0')\n",
            "topk 29 tensor([  286,   502,   407,   407,   338,   257,   760,  1309,  1309,  7510,\n",
            "          338,  4656,  1257,   290,   290,   326,    11,     0,   286,   326,\n",
            "        44873,   286,   326,   307,   836,   530,   307, 25258,   467,   617,\n",
            "        44873, 17753,  9192,    13,    13,  8030,  9707,   326],\n",
            "       device='cuda:0')\n",
            "topk scores29 tensor([0.9998, 0.6382, 0.4394, 0.3543, 0.3381, 0.3155, 0.2849, 0.2603, 0.2514,\n",
            "        0.2439, 0.1977, 0.1653, 0.1532, 0.1527, 0.1513, 0.1478, 0.1441, 0.1405,\n",
            "        0.1329, 0.1187, 0.1185, 0.1091, 0.0972, 0.0969, 0.0959, 0.0883, 0.0871,\n",
            "        0.0856, 0.0843, 0.0818, 0.0802, 0.0785, 0.0775, 0.0775, 0.0770, 0.0756,\n",
            "        0.0753, 0.0751], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "30 torch.Size([38, 30])\n",
            "depth 30 tensor([[50256,    46,  1860,  ...,   257,   636,   286],\n",
            "        [50256,    46,  1860,  ...,   636,   286,   502],\n",
            "        [50256,     1,    40,  ...,   921,   821,   407],\n",
            "        ...,\n",
            "        [50256,    46,  1860,  ...,   338,   407,  8030],\n",
            "        [50256,    46,  1860,  ..., 46733,     6,  9707],\n",
            "        [50256,    46,  1860,  ..., 44873,  4908,   326]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([38, 50257])\n",
            "prev word tensor([27, 20, 30, 35,  0, 28, 37,  9,  6,  7,  8, 29, 19, 22, 18, 31, 12, 10,\n",
            "        23, 31, 26, 29,  4, 21, 23, 26, 26, 12, 23, 11, 36, 16, 16,  5, 11, 25,\n",
            "        32, 36], device='cuda:0')\n",
            "topk 30 tensor([  578,  4908,  4908,   284,   502,   832,   338,    11,   644,   502,\n",
            "          502, 16336,    11,    11,   616,  1833,   351,   407,   257,   423,\n",
            "          257,   640,   407,   616,   523,  8805,   523,  2474,  8805,  1862,\n",
            "           13,   582, 44873,   922,   329,    13,     0,   290],\n",
            "       device='cuda:0')\n",
            "topk scores30 tensor([1.0000, 1.0000, 1.0000, 0.8752, 0.7851, 0.4611, 0.4594, 0.3802, 0.3571,\n",
            "        0.3528, 0.3277, 0.3229, 0.3084, 0.2685, 0.1926, 0.1883, 0.1810, 0.1704,\n",
            "        0.1661, 0.1527, 0.1507, 0.1409, 0.1376, 0.1371, 0.1289, 0.1283, 0.1220,\n",
            "        0.1184, 0.1168, 0.1012, 0.0993, 0.0940, 0.0909, 0.0897, 0.0895, 0.0861,\n",
            "        0.0836, 0.0834], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "31 torch.Size([38, 31])\n",
            "depth 31 tensor([[50256,    46,  1860,  ...,     6, 25258,   578],\n",
            "        [50256,    46,  1860,  ...,    11, 44873,  4908],\n",
            "        [50256,    46,  1860,  ...,    11, 44873,  4908],\n",
            "        ...,\n",
            "        [50256,    46,  1860,  ...,  9372,   530,    13],\n",
            "        [50256,    46,  1860,  ...,  9372,  9192,     0],\n",
            "        [50256,    46,  1860,  ...,     6,  9707,   290]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2467,  0.3805, -0.4374,  ...,  0.7962, -0.5518,  0.5751],\n",
            "         [ 0.2481,  0.3801, -0.4368,  ...,  0.7933, -0.5534,  0.5755],\n",
            "         [ 0.2462,  0.3794, -0.4354,  ...,  0.7956, -0.5538,  0.5743],\n",
            "         ...,\n",
            "         [ 0.2464,  0.3806, -0.4361,  ...,  0.7979, -0.5524,  0.5735],\n",
            "         [ 0.2436,  0.3797, -0.4347,  ...,  0.8020, -0.5528,  0.5718],\n",
            "         [ 0.2453,  0.3804, -0.4378,  ...,  0.7986, -0.5517,  0.5746]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([38, 50257])\n",
            "prev word tensor([ 0,  3, 14, 23, 11, 18, 20, 25, 28, 21, 33, 10,  9, 16, 31,  5, 21, 13,\n",
            "        16, 29, 12,  2, 26, 25,  6, 29, 21, 28, 24, 34,  1,  7, 22, 33,  0, 12,\n",
            "        29,  5], device='cuda:0')\n",
            "topk 31 tensor([50256,   514,  1573,  1573,   351,  9192,  9192,   379,   379,   284,\n",
            "         3707,   467,   467,   340,     0,   351,   329, 44873,   326,   290,\n",
            "        44873,     0, 13526,   546,   407,  1576,   351,   546, 13526,  2147,\n",
            "            0, 44873,   355,   640,    13,   582,    11,   326],\n",
            "       device='cuda:0')\n",
            "topk scores31 tensor([1.0000, 0.8657, 0.8285, 0.8195, 0.7365, 0.4500, 0.4380, 0.3831, 0.3514,\n",
            "        0.3453, 0.2956, 0.2941, 0.2864, 0.2841, 0.2606, 0.2470, 0.2372, 0.1899,\n",
            "        0.1814, 0.1783, 0.1649, 0.1583, 0.1516, 0.1513, 0.1498, 0.1457, 0.1446,\n",
            "        0.1416, 0.1401, 0.1399, 0.1337, 0.1260, 0.1237, 0.1218, 0.1194, 0.1163,\n",
            "        0.1147, 0.1057], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "ppppp tensor([[6, 9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.189654742026425\n",
            "ppppp tensor([[9, 7]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.0943445622221\n",
            "ppppp tensor([[5, 4]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.5263605246161616\n",
            "ppppp tensor([[6, 9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.189654742026425\n",
            "ppppp tensor([[6, 9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.189654742026425\n",
            "ppppp tensor([[ 5, 10]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.248495242049359\n",
            "ppppp tensor([[14,  9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.406719247264253\n",
            "ppppp tensor([[2, 9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.060443010546419\n",
            "ppppp tensor([[11,  5]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.9512437185814275\n",
            "ppppp tensor([[2, 3]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.091042453358316\n",
            "ppppp tensor([[4, 7]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.912023005428146\n",
            "ppppp tensor([[2, 8]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.9512437185814275\n",
            "ppppp tensor([[13,  2]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.6375861597263857\n",
            "iiii 3\n",
            "1 torch.Size([50, 1])\n",
            "depth 1 tensor([[50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256],\n",
            "        [50256]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0], device='cuda:0')\n",
            "topk 1 tensor([31, 30, 28, 29, 25, 24, 26, 27, 19, 18, 16, 17, 21, 20, 22, 23,  7,  6,\n",
            "         4,  5,  1,  0,  2,  3, 11, 10,  8,  9, 13, 12, 14, 15, 47, 46, 44, 45,\n",
            "        41, 40, 42, 43, 35, 34, 32, 33, 37, 36, 38, 39, 49, 48],\n",
            "       device='cuda:0')\n",
            "topk scores1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "2 torch.Size([50, 2])\n",
            "depth 2 tensor([[50256,    31],\n",
            "        [50256,    30],\n",
            "        [50256,    28],\n",
            "        [50256,    29],\n",
            "        [50256,    25],\n",
            "        [50256,    24],\n",
            "        [50256,    26],\n",
            "        [50256,    27],\n",
            "        [50256,    19],\n",
            "        [50256,    18],\n",
            "        [50256,    16],\n",
            "        [50256,    17],\n",
            "        [50256,    21],\n",
            "        [50256,    20],\n",
            "        [50256,    22],\n",
            "        [50256,    23],\n",
            "        [50256,     7],\n",
            "        [50256,     6],\n",
            "        [50256,     4],\n",
            "        [50256,     5],\n",
            "        [50256,     1],\n",
            "        [50256,     0],\n",
            "        [50256,     2],\n",
            "        [50256,     3],\n",
            "        [50256,    11],\n",
            "        [50256,    10],\n",
            "        [50256,     8],\n",
            "        [50256,     9],\n",
            "        [50256,    13],\n",
            "        [50256,    12],\n",
            "        [50256,    14],\n",
            "        [50256,    15],\n",
            "        [50256,    47],\n",
            "        [50256,    46],\n",
            "        [50256,    44],\n",
            "        [50256,    45],\n",
            "        [50256,    41],\n",
            "        [50256,    40],\n",
            "        [50256,    42],\n",
            "        [50256,    43],\n",
            "        [50256,    35],\n",
            "        [50256,    34],\n",
            "        [50256,    32],\n",
            "        [50256,    33],\n",
            "        [50256,    37],\n",
            "        [50256,    36],\n",
            "        [50256,    38],\n",
            "        [50256,    39],\n",
            "        [50256,    49],\n",
            "        [50256,    48]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([17, 35, 43, 46, 12, 42, 45,  5, 33, 14, 31, 32,  8, 15,  6, 47, 41, 13,\n",
            "        34, 36, 28, 22, 48,  9, 44, 10, 31, 34, 39, 22, 10, 11,  8, 38, 49, 34,\n",
            "        32, 10, 38, 40,  9, 20, 33, 48, 46, 37, 38, 36, 13, 41],\n",
            "       device='cuda:0')\n",
            "topk 2 tensor([42323,  6950,  2007, 12375,    13,   259,  8505,    13,  1860,    13,\n",
            "           13,  2433,    13,    13,   314,   436, 10277,    13,  2002, 15746,\n",
            "         2231,  1303,  2530,    13, 19296,    13,   657,   432,   452,    16,\n",
            "         1510,    13,   400,  4509,    25,  1689, 14650,   301,   747,  3008,\n",
            "           12,    40,  1219, 46629,  3087,  1101,  3605,   603,   812,   963],\n",
            "       device='cuda:0')\n",
            "topk scores2 tensor([0.9804, 0.8574, 0.7496, 0.6540, 0.6116, 0.5986, 0.5764, 0.5764, 0.5714,\n",
            "        0.4978, 0.4977, 0.4747, 0.3922, 0.3814, 0.3788, 0.3763, 0.3567, 0.3479,\n",
            "        0.3416, 0.3266, 0.3111, 0.2961, 0.2805, 0.2691, 0.2549, 0.2520, 0.2502,\n",
            "        0.2434, 0.2374, 0.2330, 0.2276, 0.2246, 0.2150, 0.2102, 0.2040, 0.1907,\n",
            "        0.1844, 0.1839, 0.1835, 0.1832, 0.1830, 0.1795, 0.1714, 0.1707, 0.1528,\n",
            "        0.1503, 0.1499, 0.1481, 0.1411, 0.1407], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "3 torch.Size([50, 3])\n",
            "depth 3 tensor([[50256,     6, 42323],\n",
            "        [50256,    45,  6950],\n",
            "        [50256,    33,  2007],\n",
            "        [50256,    38, 12375],\n",
            "        [50256,    21,    13],\n",
            "        [50256,    32,   259],\n",
            "        [50256,    36,  8505],\n",
            "        [50256,    24,    13],\n",
            "        [50256,    46,  1860],\n",
            "        [50256,    22,    13],\n",
            "        [50256,    15,    13],\n",
            "        [50256,    47,  2433],\n",
            "        [50256,    19,    13],\n",
            "        [50256,    23,    13],\n",
            "        [50256,    26,   314],\n",
            "        [50256,    39,   436],\n",
            "        [50256,    34, 10277],\n",
            "        [50256,    20,    13],\n",
            "        [50256,    44,  2002],\n",
            "        [50256,    41, 15746],\n",
            "        [50256,    13,  2231],\n",
            "        [50256,     2,  1303],\n",
            "        [50256,    49,  2530],\n",
            "        [50256,    18,    13],\n",
            "        [50256,    37, 19296],\n",
            "        [50256,    16,    13],\n",
            "        [50256,    15,   657],\n",
            "        [50256,    44,   432],\n",
            "        [50256,    43,   452],\n",
            "        [50256,     2,    16],\n",
            "        [50256,    16,  1510],\n",
            "        [50256,    17,    13],\n",
            "        [50256,    19,   400],\n",
            "        [50256,    42,  4509],\n",
            "        [50256,    48,    25],\n",
            "        [50256,    44,  1689],\n",
            "        [50256,    47, 14650],\n",
            "        [50256,    16,   301],\n",
            "        [50256,    42,   747],\n",
            "        [50256,    35,  3008],\n",
            "        [50256,    18,    12],\n",
            "        [50256,     1,    40],\n",
            "        [50256,    46,  1219],\n",
            "        [50256,    49, 46629],\n",
            "        [50256,    38,  3087],\n",
            "        [50256,    40,  1101],\n",
            "        [50256,    42,  3605],\n",
            "        [50256,    41,   603],\n",
            "        [50256,    20,   812],\n",
            "        [50256,    34,   963]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([ 1, 18, 44,  8, 28, 19,  5, 42,  2, 49, 37, 15, 48, 11, 40, 25, 30, 29,\n",
            "        29, 36, 48, 46, 38,  0, 11, 14, 22, 13, 29, 25, 16, 35, 41, 38, 15, 45,\n",
            "        39, 11,  2, 41, 34, 22, 49,  3, 35, 30, 14, 35, 15, 22],\n",
            "       device='cuda:0')\n",
            "topk 3 tensor([  292,    64,   259, 10898,   259,    88,   470,    11,    11,   502,\n",
            "          290,   293,  1468,   278,    19,   362,    11,    13,    11, 44873,\n",
            "         2084,   345,   502,   314,   284,  1101,   351,    24,    25,    17,\n",
            "          314,  1297,  1101,   616,  8116,   257,    11,   329,   314,   836,\n",
            "         1867,   287,   257,  1394,   531,   661,   447,    11,  2815,   832],\n",
            "       device='cuda:0')\n",
            "topk scores3 tensor([1.0000, 1.0000, 0.9999, 0.9994, 0.9982, 0.9478, 0.9331, 0.7019, 0.6323,\n",
            "        0.5894, 0.5204, 0.4874, 0.4849, 0.4599, 0.4587, 0.4467, 0.3487, 0.3445,\n",
            "        0.3120, 0.2906, 0.2843, 0.2749, 0.2720, 0.2717, 0.2700, 0.2502, 0.2479,\n",
            "        0.2443, 0.2423, 0.2381, 0.2341, 0.2310, 0.2289, 0.2132, 0.1963, 0.1914,\n",
            "        0.1864, 0.1840, 0.1816, 0.1813, 0.1732, 0.1704, 0.1688, 0.1685, 0.1659,\n",
            "        0.1657, 0.1627, 0.1624, 0.1563, 0.1550], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "4 torch.Size([50, 4])\n",
            "depth 4 tensor([[50256,    45,  6950,   292],\n",
            "        [50256,    44,  2002,    64],\n",
            "        [50256,    38,  3087,   259],\n",
            "        [50256,    46,  1860, 10898],\n",
            "        [50256,    43,   452,   259],\n",
            "        [50256,    41, 15746,    88],\n",
            "        [50256,    32,   259,   470],\n",
            "        [50256,    46,  1219,    11],\n",
            "        [50256,    33,  2007,    11],\n",
            "        [50256,    34,   963,   502],\n",
            "        [50256,    16,   301,   290],\n",
            "        [50256,    39,   436,   293],\n",
            "        [50256,    20,   812,  1468],\n",
            "        [50256,    47,  2433,   278],\n",
            "        [50256,    18,    12,    19],\n",
            "        [50256,    16,    13,   362],\n",
            "        [50256,    16,  1510,    11],\n",
            "        [50256,     2,    16,    13],\n",
            "        [50256,     2,    16,    11],\n",
            "        [50256,    47, 14650, 44873],\n",
            "        [50256,    20,   812,  2084],\n",
            "        [50256,    42,  3605,   345],\n",
            "        [50256,    42,   747,   502],\n",
            "        [50256,     6, 42323,   314],\n",
            "        [50256,    47,  2433,   284],\n",
            "        [50256,    26,   314,  1101],\n",
            "        [50256,    49,  2530,   351],\n",
            "        [50256,    23,    13,    24],\n",
            "        [50256,     2,    16,    25],\n",
            "        [50256,    16,    13,    17],\n",
            "        [50256,    34, 10277,   314],\n",
            "        [50256,    44,  1689,  1297],\n",
            "        [50256,     1,    40,  1101],\n",
            "        [50256,    42,   747,   616],\n",
            "        [50256,    39,   436,  8116],\n",
            "        [50256,    40,  1101,   257],\n",
            "        [50256,    35,  3008,    11],\n",
            "        [50256,    47,  2433,   329],\n",
            "        [50256,    33,  2007,   314],\n",
            "        [50256,     1,    40,   836],\n",
            "        [50256,    48,    25,  1867],\n",
            "        [50256,    49,  2530,   287],\n",
            "        [50256,    34,   963,   257],\n",
            "        [50256,    38, 12375,  1394],\n",
            "        [50256,    44,  1689,   531],\n",
            "        [50256,    16,  1510,   661],\n",
            "        [50256,    26,   314,   447],\n",
            "        [50256,    44,  1689,    11],\n",
            "        [50256,    39,   436,  2815],\n",
            "        [50256,    49,  2530,   832]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([19, 46, 39, 48, 31,  4,  2, 12, 24, 49, 37, 26, 20,  6, 16, 13, 41, 29,\n",
            "        10, 44, 43, 45, 23, 22, 10,  5, 14,  8, 40, 18, 47,  9,  8,  1, 30, 38,\n",
            "         3, 25,  6,  3, 34, 16, 13, 32, 11,  7, 35,  9, 30, 11],\n",
            "       device='cuda:0')\n",
            "topk 4 tensor([4908,  247,  470,    6,  502,    6,    6,   11, 1793,  262,  262,  262,\n",
            "          11,  645,  830,  329,  262, 1510, 1315,   11,  340,  287, 1101,   11,\n",
            "         362,   11,   11,  314,  338,  362,  314,  287,  345,   11, 1101, 1101,\n",
            "         318,  257, 8168,   11,   11,  362,  284,  407,  351,  314, 2802,  319,\n",
            "         447,  287], device='cuda:0')\n",
            "topk scores4 tensor([1.0000, 1.0000, 0.9798, 0.9760, 0.9608, 0.9599, 0.8828, 0.7994, 0.7219,\n",
            "        0.6878, 0.6195, 0.5956, 0.5833, 0.5614, 0.5462, 0.4989, 0.4332, 0.4048,\n",
            "        0.3913, 0.3733, 0.3661, 0.3607, 0.3237, 0.3089, 0.3034, 0.3032, 0.2988,\n",
            "        0.2856, 0.2852, 0.2725, 0.2642, 0.2629, 0.2594, 0.2457, 0.2401, 0.2389,\n",
            "        0.2314, 0.2311, 0.2309, 0.2297, 0.2245, 0.2093, 0.2050, 0.2018, 0.1994,\n",
            "        0.1958, 0.1907, 0.1882, 0.1856, 0.1828], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "5 torch.Size([50, 5])\n",
            "depth 5 tensor([[50256,    47, 14650, 44873,  4908],\n",
            "        [50256,    26,   314,   447,   247],\n",
            "        [50256,     1,    40,   836,   470],\n",
            "        [50256,    39,   436,  2815,     6],\n",
            "        [50256,    44,  1689,  1297,   502],\n",
            "        [50256,    43,   452,   259,     6],\n",
            "        [50256,    38,  3087,   259,     6],\n",
            "        [50256,    20,   812,  1468,    11],\n",
            "        [50256,    47,  2433,   284,  1793],\n",
            "        [50256,    49,  2530,   832,   262],\n",
            "        [50256,    47,  2433,   329,   262],\n",
            "        [50256,    49,  2530,   351,   262],\n",
            "        [50256,    20,   812,  2084,    11],\n",
            "        [50256,    32,   259,   470,   645],\n",
            "        [50256,    16,  1510,    11,   830],\n",
            "        [50256,    47,  2433,   278,   329],\n",
            "        [50256,    49,  2530,   287,   262],\n",
            "        [50256,    16,    13,    17,  1510],\n",
            "        [50256,    16,   301,   290,  1315],\n",
            "        [50256,    44,  1689,   531,    11],\n",
            "        [50256,    38, 12375,  1394,   340],\n",
            "        [50256,    16,  1510,   661,   287],\n",
            "        [50256,     6, 42323,   314,  1101],\n",
            "        [50256,    42,   747,   502,    11],\n",
            "        [50256,    16,   301,   290,   362],\n",
            "        [50256,    41, 15746,    88,    11],\n",
            "        [50256,    18,    12,    19,    11],\n",
            "        [50256,    33,  2007,    11,   314],\n",
            "        [50256,    48,    25,  1867,   338],\n",
            "        [50256,     2,    16,    11,   362],\n",
            "        [50256,    44,  1689,    11,   314],\n",
            "        [50256,    34,   963,   502,   287],\n",
            "        [50256,    33,  2007,    11,   345],\n",
            "        [50256,    44,  2002,    64,    11],\n",
            "        [50256,    34, 10277,   314,  1101],\n",
            "        [50256,    33,  2007,   314,  1101],\n",
            "        [50256,    46,  1860, 10898,   318],\n",
            "        [50256,    26,   314,  1101,   257],\n",
            "        [50256,    32,   259,   470,  8168],\n",
            "        [50256,    46,  1860, 10898,    11],\n",
            "        [50256,    39,   436,  8116,    11],\n",
            "        [50256,    16,  1510,    11,   362],\n",
            "        [50256,    47,  2433,   278,   284],\n",
            "        [50256,     1,    40,  1101,   407],\n",
            "        [50256,    39,   436,   293,   351],\n",
            "        [50256,    46,  1219,    11,   314],\n",
            "        [50256,    40,  1101,   257,  2802],\n",
            "        [50256,    34,   963,   502,   319],\n",
            "        [50256,    34, 10277,   314,   447],\n",
            "        [50256,    39,   436,   293,   287]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([48, 18, 24,  1, 42,  0, 19, 21, 46, 47, 49, 44, 17, 31, 15, 20,  9, 43,\n",
            "         8, 28, 13, 23, 34, 46,  9, 35, 30, 12, 31, 45,  8, 33, 27, 33,  5, 27,\n",
            "        36, 32,  2, 11, 22,  4, 20,  2, 28, 14, 39, 32,  2, 25],\n",
            "       device='cuda:0')\n",
            "topk 5 tensor([  247,   400,   358,    76,  1793,    11,   366,   262,    69,   262,\n",
            "          262,   262,    11,   262,   262,  1103,  1748,   257,    11,   262,\n",
            "          761,   314,   257, 31699,  6483,   257,  1101,   314,   257,   447,\n",
            "          329,   314,   447,   345,   262,  1101,   262,   760,   765,  7706,\n",
            "          257,    11,   402, 18869,   510,   661,   314,  1365,   760,   314],\n",
            "       device='cuda:0')\n",
            "topk scores5 tensor([1.0000, 0.9999, 0.9993, 0.9497, 0.8684, 0.8037, 0.7820, 0.7107, 0.7069,\n",
            "        0.6570, 0.6501, 0.6144, 0.5633, 0.5158, 0.4929, 0.4644, 0.4634, 0.3933,\n",
            "        0.3918, 0.3742, 0.3353, 0.2900, 0.2842, 0.2830, 0.2816, 0.2627, 0.2617,\n",
            "        0.2419, 0.2334, 0.2309, 0.2303, 0.2278, 0.2254, 0.2202, 0.2173, 0.2139,\n",
            "        0.2110, 0.2089, 0.2064, 0.1979, 0.1909, 0.1899, 0.1897, 0.1845, 0.1780,\n",
            "        0.1694, 0.1691, 0.1656, 0.1611, 0.1580], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "6 torch.Size([50, 6])\n",
            "depth 6 tensor([[50256,    34, 10277,   314,   447,   247],\n",
            "        [50256,    16,   301,   290,  1315,   400],\n",
            "        [50256,    16,   301,   290,   362,   358],\n",
            "        [50256,    26,   314,   447,   247,    76],\n",
            "        [50256,    47,  2433,   278,   284,  1793],\n",
            "        [50256,    47, 14650, 44873,  4908,    11],\n",
            "        [50256,    44,  1689,   531,    11,   366],\n",
            "        [50256,    16,  1510,   661,   287,   262],\n",
            "        [50256,    40,  1101,   257,  2802,    69],\n",
            "        [50256,    34,   963,   502,   319,   262],\n",
            "        [50256,    39,   436,   293,   287,   262],\n",
            "        [50256,    39,   436,   293,   351,   262],\n",
            "        [50256,    16,    13,    17,  1510,    11],\n",
            "        [50256,    34,   963,   502,   287,   262],\n",
            "        [50256,    47,  2433,   278,   329,   262],\n",
            "        [50256,    38, 12375,  1394,   340,  1103],\n",
            "        [50256,    49,  2530,   832,   262,  1748],\n",
            "        [50256,     1,    40,  1101,   407,   257],\n",
            "        [50256,    47,  2433,   284,  1793,    11],\n",
            "        [50256,    48,    25,  1867,   338,   262],\n",
            "        [50256,    32,   259,   470,   645,   761],\n",
            "        [50256,    42,   747,   502,    11,   314],\n",
            "        [50256,    34, 10277,   314,  1101,   257],\n",
            "        [50256,    40,  1101,   257,  2802, 31699],\n",
            "        [50256,    49,  2530,   832,   262,  6483],\n",
            "        [50256,    33,  2007,   314,  1101,   257],\n",
            "        [50256,    44,  1689,    11,   314,  1101],\n",
            "        [50256,    20,   812,  2084,    11,   314],\n",
            "        [50256,    34,   963,   502,   287,   257],\n",
            "        [50256,    46,  1219,    11,   314,   447],\n",
            "        [50256,    47,  2433,   284,  1793,   329],\n",
            "        [50256,    44,  2002,    64,    11,   314],\n",
            "        [50256,    33,  2007,    11,   314,   447],\n",
            "        [50256,    44,  2002,    64,    11,   345],\n",
            "        [50256,    43,   452,   259,     6,   262],\n",
            "        [50256,    33,  2007,    11,   314,  1101],\n",
            "        [50256,    46,  1860, 10898,   318,   262],\n",
            "        [50256,    33,  2007,    11,   345,   760],\n",
            "        [50256,     1,    40,   836,   470,   765],\n",
            "        [50256,    49,  2530,   351,   262,  7706],\n",
            "        [50256,     6, 42323,   314,  1101,   257],\n",
            "        [50256,    44,  1689,  1297,   502,    11],\n",
            "        [50256,    38, 12375,  1394,   340,   402],\n",
            "        [50256,     1,    40,   836,   470, 18869],\n",
            "        [50256,    48,    25,  1867,   338,   510],\n",
            "        [50256,    16,  1510,    11,   830,   661],\n",
            "        [50256,    46,  1860, 10898,    11,   314],\n",
            "        [50256,    33,  2007,    11,   345,  1365],\n",
            "        [50256,     1,    40,   836,   470,   760],\n",
            "        [50256,    41, 15746,    88,    11,   314]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([32, 29, 23,  0, 20, 41,  8, 15, 39, 12, 42, 19, 43, 37, 24,  4, 16, 30,\n",
            "        44, 48, 38, 44,  9,  8,  1, 21, 26, 35,  6,  5,  3,  2, 36, 33, 46, 34,\n",
            "        45, 13, 24, 31, 45, 16, 27, 47, 40, 20, 31, 38, 35, 18],\n",
            "       device='cuda:0')\n",
            "topk 6 tensor([  247,   247,   259,    76,   284,   366, 12603,    11,    11,   362,\n",
            "           11,   966,   307,   644,   351,    11,   351,   262,   351,   644,\n",
            "          284,    11,  2975, 19296,    11,   447,  2611,   257,    40,   314,\n",
            "          257,    11,   691,   760,  1101,   995,   257,  6483,    11,  1101,\n",
            "          287,    11,   373,   651,  2802,   329,   447,   345,   407,   314],\n",
            "       device='cuda:0')\n",
            "topk scores6 tensor([1.0000, 1.0000, 0.9959, 0.9950, 0.8176, 0.7280, 0.7163, 0.6526, 0.6395,\n",
            "        0.5573, 0.5424, 0.5294, 0.4751, 0.4214, 0.4154, 0.4065, 0.3861, 0.3802,\n",
            "        0.3656, 0.3414, 0.3395, 0.3334, 0.3088, 0.2836, 0.2793, 0.2663, 0.2563,\n",
            "        0.2506, 0.2366, 0.2358, 0.2350, 0.2334, 0.2231, 0.2213, 0.2179, 0.2121,\n",
            "        0.2051, 0.2035, 0.2025, 0.1991, 0.1953, 0.1939, 0.1897, 0.1891, 0.1778,\n",
            "        0.1749, 0.1727, 0.1654, 0.1606, 0.1565], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "7 torch.Size([50, 7])\n",
            "depth 7 tensor([[50256,    33,  2007,    11,   314,   447,   247],\n",
            "        [50256,    46,  1219,    11,   314,   447,   247],\n",
            "        [50256,    40,  1101,   257,  2802, 31699,   259],\n",
            "        [50256,    34, 10277,   314,   447,   247,    76],\n",
            "        [50256,    32,   259,   470,   645,   761,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366],\n",
            "        [50256,    40,  1101,   257,  2802,    69, 12603],\n",
            "        [50256,    38, 12375,  1394,   340,  1103,    11],\n",
            "        [50256,    49,  2530,   351,   262,  7706,    11],\n",
            "        [50256,    16,    13,    17,  1510,    11,   362],\n",
            "        [50256,    38, 12375,  1394,   340,   402,    11],\n",
            "        [50256,    48,    25,  1867,   338,   262,   966],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644],\n",
            "        [50256,    49,  2530,   832,   262,  6483,   351],\n",
            "        [50256,    47,  2433,   278,   284,  1793,    11],\n",
            "        [50256,    49,  2530,   832,   262,  1748,   351],\n",
            "        [50256,    47,  2433,   284,  1793,   329,   262],\n",
            "        [50256,    48,    25,  1867,   338,   510,   351],\n",
            "        [50256,     1,    40,   836,   470,   760,   644],\n",
            "        [50256,     1,    40,   836,   470,   765,   284],\n",
            "        [50256,    48,    25,  1867,   338,   510,    11],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975],\n",
            "        [50256,    40,  1101,   257,  2802,    69, 19296],\n",
            "        [50256,    16,   301,   290,  1315,   400,    11],\n",
            "        [50256,    42,   747,   502,    11,   314,   447],\n",
            "        [50256,    44,  1689,    11,   314,  1101,  2611],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257],\n",
            "        [50256,    44,  1689,   531,    11,   366,    40],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314],\n",
            "        [50256,    26,   314,   447,   247,    76,   257],\n",
            "        [50256,    16,   301,   290,   362,   358,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760],\n",
            "        [50256,    46,  1860, 10898,    11,   314,  1101],\n",
            "        [50256,    43,   452,   259,     6,   262,   995],\n",
            "        [50256,    16,  1510,    11,   830,   661,   257],\n",
            "        [50256,    34,   963,   502,   287,   262,  6483],\n",
            "        [50256,    49,  2530,   832,   262,  6483,    11],\n",
            "        [50256,    44,  2002,    64,    11,   314,  1101],\n",
            "        [50256,    16,  1510,    11,   830,   661,   287],\n",
            "        [50256,    49,  2530,   832,   262,  1748,    11],\n",
            "        [50256,    20,   812,  2084,    11,   314,   373],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651],\n",
            "        [50256,     6, 42323,   314,  1101,   257,  2802],\n",
            "        [50256,    32,   259,   470,   645,   761,   329],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447],\n",
            "        [50256,     1,    40,   836,   470,   765,   345],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([46, 25,  1,  0,  2, 11, 47, 13,  6, 40, 22, 31, 44, 36, 19, 33, 44, 37,\n",
            "        48, 20, 45, 37, 35, 36, 18, 12, 20,  3, 21, 43, 16, 29, 45, 32, 49, 32,\n",
            "        43, 26, 15,  5, 34, 32, 22,  4, 29, 27, 42, 35, 39, 49],\n",
            "       device='cuda:0')\n",
            "topk 7 tensor([  247,   247,    76,    76,     6,   286,   284,   314,    11,   262,\n",
            "          284,   513,    69,  1110,   345,   644, 31699,    11,   257,   307,\n",
            "          257,   290,    11,   614,   262,   257,   766,   257,   644,   534,\n",
            "          257,  1101,   262,   835,   447,  1517,   257,   307,   314,    40,\n",
            "          257,   530,    11,   307,   447,  2802,   257,   257,  2611, 12472],\n",
            "       device='cuda:0')\n",
            "topk scores7 tensor([1.0000, 1.0000, 0.9936, 0.9772, 0.9621, 0.9370, 0.8108, 0.7766, 0.7220,\n",
            "        0.6951, 0.6683, 0.6290, 0.5759, 0.5744, 0.4551, 0.4221, 0.4153, 0.3573,\n",
            "        0.3465, 0.3085, 0.2892, 0.2779, 0.2770, 0.2770, 0.2604, 0.2597, 0.2522,\n",
            "        0.2474, 0.2395, 0.2378, 0.2320, 0.2221, 0.2180, 0.2131, 0.2086, 0.2046,\n",
            "        0.1969, 0.1918, 0.1917, 0.1856, 0.1840, 0.1713, 0.1662, 0.1612, 0.1602,\n",
            "        0.1600, 0.1596, 0.1572, 0.1538, 0.1444], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "8 torch.Size([50, 8])\n",
            "depth 8 tensor([[50256,    44,  2002,    64,    11,   314,   447,   247],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247],\n",
            "        [50256,    46,  1219,    11,   314,   447,   247,    76],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76],\n",
            "        [50256,    40,  1101,   257,  2802, 31699,   259,     6],\n",
            "        [50256,    48,    25,  1867,   338,   262,   966,   286],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314],\n",
            "        [50256,    40,  1101,   257,  2802,    69, 12603,    11],\n",
            "        [50256,    16,  1510,    11,   830,   661,   287,   262],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284],\n",
            "        [50256,    16,   301,   290,   362,   358,    11,   513],\n",
            "        [50256,     6, 42323,   314,  1101,   257,  2802,    69],\n",
            "        [50256,    16,  1510,    11,   830,   661,   257,  1110],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644],\n",
            "        [50256,     6, 42323,   314,  1101,   257,  2802, 31699],\n",
            "        [50256,    34,   963,   502,   287,   262,  6483,    11],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   407,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307],\n",
            "        [50256,    32,   259,   470,   645,   761,   329,   257],\n",
            "        [50256,    34,   963,   502,   287,   262,  6483,   290],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11],\n",
            "        [50256,    16,  1510,    11,   830,   661,   257,   614],\n",
            "        [50256,    48,    25,  1867,   338,   510,   351,   262],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766],\n",
            "        [50256,    34, 10277,   314,   447,   247,    76,   257],\n",
            "        [50256,    48,    25,  1867,   338,   510,    11,   644],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534],\n",
            "        [50256,    49,  2530,   832,   262,  1748,   351,   257],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314,  1101],\n",
            "        [50256,    32,   259,   470,   645,   761,   329,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   257],\n",
            "        [50256,    44,  1689,    11,   314,  1101,  2611,   307],\n",
            "        [50256,    47,  2433,   278,   284,  1793,    11,   314],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40],\n",
            "        [50256,    46,  1860, 10898,    11,   314,  1101,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   530],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,    11],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314,   447],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257,  2802],\n",
            "        [50256,    20,   812,  2084,    11,   314,   373,   257],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257],\n",
            "        [50256,    44,  2002,    64,    11,   314,  1101,  2611],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314, 12472]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([34, 44, 16, 11,  1,  0, 12, 45, 35, 15, 41, 10,  7,  8, 29, 36, 14, 47,\n",
            "        26, 43,  3, 47,  7, 31, 25,  6, 33, 37, 19, 33, 19, 22, 38, 48, 33,  2,\n",
            "         2, 14,  6, 49, 42,  3, 45, 39, 33, 31, 39, 33, 39, 43],\n",
            "       device='cuda:0')\n",
            "topk 8 tensor([  247,   247,   259,  4372,    76,    76, 12603,    69,   326,   314,\n",
            "          326, 35000,  1612,   314,   840,  1693,   821,  6486,   345,   257,\n",
            "          257,  4320,  1101,   257, 25670,   766,   356,   257,   257,   284,\n",
            "          287,   314,   447,   307,   326,   655,   257,   892,   760,   329,\n",
            "          314,   407, 31699,   836,   314,   407, 18959,   345,  1101, 22461],\n",
            "       device='cuda:0')\n",
            "topk scores8 tensor([1.0000, 1.0000, 0.9967, 0.9955, 0.9584, 0.9386, 0.8894, 0.8429, 0.7311,\n",
            "        0.7137, 0.6919, 0.6214, 0.6146, 0.4333, 0.4106, 0.4072, 0.3429, 0.2984,\n",
            "        0.2953, 0.2884, 0.2610, 0.2586, 0.2552, 0.2412, 0.2388, 0.2223, 0.2191,\n",
            "        0.2061, 0.1952, 0.1873, 0.1837, 0.1816, 0.1797, 0.1789, 0.1708, 0.1700,\n",
            "        0.1694, 0.1659, 0.1626, 0.1562, 0.1549, 0.1545, 0.1518, 0.1499, 0.1483,\n",
            "        0.1480, 0.1461, 0.1417, 0.1368, 0.1332], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "9 torch.Size([50, 9])\n",
            "depth 9 tensor([[50256,    47,  2433,   284,  1793,    11,   314,   447,   247],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314,   447,   247],\n",
            "        [50256,     6, 42323,   314,  1101,   257,  2802, 31699,   259],\n",
            "        [50256,    16,   301,   290,   362,   358,    11,   513,  4372],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76],\n",
            "        [50256,     6, 42323,   314,  1101,   257,  2802,    69, 12603],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257,  2802,    69],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,  1517,   326],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   530,   326],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1612],\n",
            "        [50256,    40,  1101,   257,  2802,    69, 12603,    11,   314],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   257,  1693],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   766,   345],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307,   257],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314,  1101,   257],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   257, 25670],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356],\n",
            "        [50256,    44,  1689,    11,   314,  1101,  2611,   307,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314],\n",
            "        [50256,    47,  2433,   278,   284,  1793,    11,   314,   447],\n",
            "        [50256,    44,  2002,    64,    11,   314,  1101,  2611,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326],\n",
            "        [50256,    46,  1219,    11,   314,   447,   247,    76,   655],\n",
            "        [50256,    46,  1219,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314, 12472,   329],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,    11,   314],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   407],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257,  2802, 31699],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314,  1101,   407],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,  1101],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([32, 42, 43, 46,  0,  1,  7, 17, 12, 21, 24,  9, 11,  6, 49, 47, 22, 37,\n",
            "        45,  4, 39, 34, 41, 14, 30, 27, 26, 25, 22, 35, 34, 37, 10, 16, 11, 31,\n",
            "        34, 38, 30,  9, 29, 16, 39, 44, 40, 10, 40, 38,  5, 15],\n",
            "       device='cuda:0')\n",
            "topk 9 tensor([  247,   259,   470,   470,    76,    76, 12603,    11,    30,    11,\n",
            "           11,  1612,    11,    11,   286,   460,  2282,   546,   257,   257,\n",
            "          345,   356,   257,   503,   262,  2802,   460,   502,   910,   257,\n",
            "          345,    11,   460,  2282,   290,  1101,   314,   644,   257,  1101,\n",
            "          651,  3375,   262,   460,   447,   338,  1101,   326,   257,   621],\n",
            "       device='cuda:0')\n",
            "topk scores9 tensor([1.0000, 0.9976, 0.9900, 0.9885, 0.9700, 0.9354, 0.8867, 0.7704, 0.7552,\n",
            "        0.7508, 0.6985, 0.6793, 0.6189, 0.6166, 0.5903, 0.5247, 0.5092, 0.4388,\n",
            "        0.4027, 0.3721, 0.3509, 0.3452, 0.3348, 0.3259, 0.3093, 0.2855, 0.2828,\n",
            "        0.2805, 0.2802, 0.2520, 0.2357, 0.2322, 0.2309, 0.2250, 0.2212, 0.2162,\n",
            "        0.2046, 0.2003, 0.1977, 0.1934, 0.1924, 0.1873, 0.1872, 0.1863, 0.1787,\n",
            "        0.1784, 0.1752, 0.1737, 0.1695, 0.1687], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "10 torch.Size([50, 10])\n",
            "depth 10 tensor([[50256,    47,  2433,   278,   284,  1793,    11,   314,   447,   247],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257,  2802, 31699,   259],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247,    76],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314,   447,   247,    76],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257,  2802,    69, 12603],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1612,    30],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   257, 25670,    11],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1612],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11],\n",
            "        [50256,     6, 42323,   314,  1101,   257,  2802,    69, 12603,    11],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,  2282],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314,  1101,   407,   257],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314, 12472,   329,   345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   356],\n",
            "        [50256,    33,  2007,    11,   314,   447,   247,    76,   407,   257],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262],\n",
            "        [50256,    44,  1689,    11,   314,  1101,  2611,   307,   257,  2802],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910],\n",
            "        [50256,    46,  1219,    11,   314,   447,   247,    76,   655,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   530,   326,   460],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,   290],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   284,   651],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314, 12472,   329,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,    11,   314,   447],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   530,   326,   338],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,    11,   314,  1101],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   257,  1693,   621]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([28, 44, 41,  0,  1, 25, 38, 11, 10, 30, 33, 23, 16, 47, 48, 39, 21, 37,\n",
            "        36, 17, 49, 29,  2,  8, 31, 27,  5, 24,  2, 35, 39, 46,  4, 32, 14, 15,\n",
            "        23, 19, 26, 12, 42,  5, 49,  2, 31, 14,  7, 43,  3,  9],\n",
            "       device='cuda:0')\n",
            "topk 10 tensor([  259,   247,   546,    76,     6,    69,  2292,    30,   314,   460,\n",
            "           11,   262,    30,   314,  2802,  2282,   460,   314,   460,   340,\n",
            "          428,  1862,   765,   921,   314,   287,   257,  3430,   761,   257,\n",
            "          910,   257,   257,  1487,   340,   651,   286,  2802,   651,   314,\n",
            "         4453,   407,   326, 18869, 44873,   852,   314,   651, 17753,   314],\n",
            "       device='cuda:0')\n",
            "topk scores10 tensor([1.0000, 1.0000, 0.9817, 0.9813, 0.9669, 0.9063, 0.7209, 0.7128, 0.7097,\n",
            "        0.6788, 0.6330, 0.6113, 0.6065, 0.5100, 0.4850, 0.4849, 0.4640, 0.4286,\n",
            "        0.4024, 0.3719, 0.3441, 0.3251, 0.3009, 0.2925, 0.2863, 0.2787, 0.2619,\n",
            "        0.2520, 0.2375, 0.2327, 0.2264, 0.2171, 0.2049, 0.1935, 0.1865, 0.1857,\n",
            "        0.1795, 0.1732, 0.1644, 0.1561, 0.1552, 0.1467, 0.1456, 0.1398, 0.1361,\n",
            "        0.1338, 0.1336, 0.1335, 0.1330, 0.1303], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "11 torch.Size([50, 11])\n",
            "depth 11 tensor([[50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910,\n",
            "           259],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,    11,   314,   447,\n",
            "           247],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546],\n",
            "        [50256,    47,  2433,   278,   284,  1793,    11,   314,   447,   247,\n",
            "            76],\n",
            "        [50256,    33,  2007,    11,   314,  1101,   257,  2802, 31699,   259,\n",
            "             6],\n",
            "        [50256,    44,  1689,    11,   314,  1101,  2611,   307,   257,  2802,\n",
            "            69],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1612,\n",
            "            30],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   257, 25670,    11,\n",
            "           314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,  2282,\n",
            "            30],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "          2282],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   356,\n",
            "           460],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   314,\n",
            "           460],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   257,  1693,   621,\n",
            "           428],\n",
            "        [50256,    46,  1219,    11,   314,   447,   247,    76,   655,   257,\n",
            "          1862],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1612,    30,\n",
            "           921],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314,   447,   247,    76,\n",
            "           257],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "           910],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,    11,   314,  1101,\n",
            "           257],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314,   447,   247,    76,\n",
            "           257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   530,   326,   460,\n",
            "          1487],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460,\n",
            "           651],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           286],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314],\n",
            "        [50256,    47,  2433,   284,  1793,    11,   314, 12472,   329,   262,\n",
            "          4453],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314,   447,   247,    76,\n",
            "           407],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   257,  1693,   621,\n",
            "           326],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "         44873],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           852],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11,\n",
            "           314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           651],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([44, 30,  1,  5,  0,  6, 14, 34, 45, 21, 37, 15,  2, 22, 27, 19, 17, 41,\n",
            "        37, 43, 36, 28, 12, 38, 10, 17, 20, 25, 35,  7, 13, 11, 36, 39, 28, 47,\n",
            "        28, 14, 49, 25, 39,  8,  9, 48, 46, 29, 48, 10, 24,  3],\n",
            "       device='cuda:0')\n",
            "topk 11 tensor([ 4908,   259,    76, 12603, 30960,   810,    69,    11,   257, 44873,\n",
            "           69,    30,    11,   284,    11,    11,  1612,   257, 31699,   307,\n",
            "         7356,   645,   921,   340,   314,  1101,     0,   262,   340,   921,\n",
            "         1101, 48083,   262, 18959,   284,   340,   257, 31699,  1101,  7356,\n",
            "          447,   765,   651,   307,  1101,  2802,  5490, 44873,   655,   257],\n",
            "       device='cuda:0')\n",
            "topk scores11 tensor([1.0000, 1.0000, 0.9720, 0.8958, 0.8797, 0.8455, 0.7957, 0.7910, 0.7028,\n",
            "        0.6981, 0.6355, 0.5760, 0.5666, 0.5459, 0.4959, 0.4449, 0.4129, 0.4013,\n",
            "        0.3540, 0.3333, 0.2977, 0.2794, 0.2775, 0.2763, 0.2617, 0.2506, 0.2491,\n",
            "        0.2481, 0.2464, 0.2414, 0.2341, 0.2171, 0.2105, 0.2093, 0.2089, 0.2067,\n",
            "        0.2042, 0.1968, 0.1959, 0.1957, 0.1939, 0.1921, 0.1876, 0.1852, 0.1823,\n",
            "        0.1759, 0.1635, 0.1611, 0.1570, 0.1538], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "12 torch.Size([50, 12])\n",
            "depth 12 tensor([[50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "         44873,  4908],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,    11,   314,   447,\n",
            "           247,    76],\n",
            "        [50256,    44,  1689,    11,   314,  1101,  2611,   307,   257,  2802,\n",
            "            69, 12603],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910,\n",
            "           259, 30960],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802,    69],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           852,   257],\n",
            "        [50256,    46,  1219,    11,   314,   447,   247,    76,   655,   257,\n",
            "          1862, 44873],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802,    69],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "          2282,    30],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1612],\n",
            "        [50256,    47, 14650, 44873,  4908,    11,   314,   447,   247,    76,\n",
            "           407,   257],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           286,  7356],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   645],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,  2282,\n",
            "            30,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   257,  1693,   621,\n",
            "           428,     0],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460,\n",
            "           651,   340],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1612,\n",
            "            30,   921],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           286,   262],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           651,   340],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  4320,    11,\n",
            "           314,  1101],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,  7356],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314,   447],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   257, 25670,    11,\n",
            "           314,   765],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,   257,  6486,    11,\n",
            "           314,  1101],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257,  2802],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,  5490],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655],\n",
            "        [50256,    47,  2433,   278,   284,  1793,    11,   314,   447,   247,\n",
            "            76,   257]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([ 9, 47, 40, 18, 37, 10,  6, 46,  1, 45, 33, 41, 14, 39,  5, 30, 31, 13,\n",
            "        28, 15,  4, 30, 12, 33, 26, 34, 42, 26, 25, 11, 48, 19, 36, 27, 35, 48,\n",
            "         7, 43, 23, 16, 45,  2, 19, 27, 23, 23, 25, 35, 15,  5],\n",
            "       device='cuda:0')\n",
            "topk 12 tensor([ 4908,  4908,   247,   259,   259, 12603, 12603,   546, 30960,    69,\n",
            "          470,   284,   314,    11,   345,   407,   290,   307,    11,   314,\n",
            "          921,   257,   314,   447,   921,   307,   340,   347,  2282,   921,\n",
            "          765,   257,  2802,  6483,    11, 18869,   314,   257,    11,    11,\n",
            "        31699,   257,   994, 10162,  1760,   826,   910,  1760, 44873,   314],\n",
            "       device='cuda:0')\n",
            "topk scores12 tensor([1.0000, 1.0000, 1.0000, 0.9961, 0.9941, 0.9791, 0.9728, 0.8329, 0.8276,\n",
            "        0.8035, 0.7398, 0.6622, 0.5725, 0.4893, 0.4740, 0.3748, 0.3557, 0.3213,\n",
            "        0.3111, 0.2999, 0.2954, 0.2949, 0.2771, 0.2602, 0.2568, 0.2556, 0.2544,\n",
            "        0.2528, 0.2499, 0.2487, 0.2316, 0.2255, 0.2252, 0.2242, 0.2166, 0.2043,\n",
            "        0.1980, 0.1943, 0.1936, 0.1904, 0.1866, 0.1809, 0.1683, 0.1654, 0.1554,\n",
            "        0.1553, 0.1528, 0.1499, 0.1417, 0.1409], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "13 torch.Size([50, 13])\n",
            "depth 13 tensor([[50256,    46,  1219,    11,   314,   447,   247,    76,   655,   257,\n",
            "          1862, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  2282,\n",
            "            11, 44873,  4908],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314,   447,   247],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699,   259],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699,   259],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802,    69, 12603],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802,    69, 12603],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,  5490,   546],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259, 30960],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257,  2802,    69],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470],\n",
            "        [50256,     1,    40,   836,   470, 18869,   307,   257, 25670,    11,\n",
            "           314,   765,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   262,\n",
            "          3430,    11,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,  7356,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765,   284,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460,\n",
            "           651,   340,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314],\n",
            "        [50256,    33,  2007,    11,   345,   760,   644,   314,  1101,   910,\n",
            "           259, 30960,   921],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   257,  1693,   621,\n",
            "           428,     0,   921],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   284,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   257,  1693,   621,\n",
            "           428,     0,   347],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,  2282],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "          2282,    30,   921],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262,  6483],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           651,   340,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655, 18869],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1612,    11],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257,  2802, 31699],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,    11,   314,   447,\n",
            "           247,    76,   257],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   994],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262, 10162],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,  1760],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,   910],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           651,   340,  1760],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([48, 23, 46,  2,  9, 32,  3,  4, 47, 44, 15, 28, 30, 33, 49, 43, 14, 14,\n",
            "        13, 26, 36,  7,  8, 18, 49,  0, 35, 13, 45, 36, 25, 17, 38, 19, 10, 34,\n",
            "         7,  0, 16, 22, 22, 17, 39, 45, 31, 45, 37, 16, 15, 26],\n",
            "       device='cuda:0')\n",
            "topk 13 tensor([ 4908,   247,   259,    76, 12603,    69,     6,     6,    11,    11,\n",
            "          257,    11,   284,    11,  1101,    11,   460,   821, 44873,    11,\n",
            "          655,   340,   921, 44873,   460,   326,   307,   314,   783,  1101,\n",
            "          257,   257, 44873,   655,  1392, 44873,   262,   351,  2666,   655,\n",
            "         1101,   994, 44873,    11, 25670,   994,  2802,  1234,   262,   826],\n",
            "       device='cuda:0')\n",
            "topk scores13 tensor([1.0000, 1.0000, 1.0000, 0.9773, 0.9697, 0.9405, 0.7787, 0.6913, 0.5493,\n",
            "        0.5234, 0.4429, 0.4232, 0.4225, 0.4004, 0.3313, 0.3250, 0.2847, 0.2777,\n",
            "        0.2638, 0.2519, 0.2481, 0.2407, 0.2396, 0.2345, 0.2240, 0.2211, 0.2126,\n",
            "        0.2083, 0.2013, 0.1956, 0.1925, 0.1856, 0.1817, 0.1804, 0.1792, 0.1782,\n",
            "        0.1670, 0.1624, 0.1618, 0.1615, 0.1559, 0.1521, 0.1499, 0.1444, 0.1421,\n",
            "        0.1402, 0.1322, 0.1204, 0.1153, 0.1137], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "14 torch.Size([50, 14])\n",
            "depth 14 tensor([[50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11, 44873,  4908],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,   910,   259],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314,   447,   247,    76],\n",
            "        [50256,    43,   452,   259,     6,   262,   995,    11,   314,  1101,\n",
            "           257,  2802,    69, 12603],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69],\n",
            "        [50256,    42,   747,   502,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699,   259,     6],\n",
            "        [50256,    44,  2002,    64,    11,   314,   447,   247,    76,   257,\n",
            "          2802, 31699,   259,     6],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           651,   340,  1760,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,  1760,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,  2282,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262,  6483,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,  1101],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262, 10162,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   460],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,  7356,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,    11],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,   655],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,  5490,   546,   340],\n",
            "        [50256,    44,  2002,    64,    11,   345,   760,   644,   314,  1101,\n",
            "           910,   259, 30960,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460,\n",
            "           651,   340,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,   460],\n",
            "        [50256,    46,  1219,    11,   314,   447,   247,    76,   655,   257,\n",
            "          1862, 44873,  4908,   326],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655, 18869,   307],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,  7356,    11,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,  1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   284,   307,   257],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765,   284,   307,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           651,   340,    11, 44873],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,  5490,   546,   262],\n",
            "        [50256,    46,  1219,    11,   314,   447,   247,    76,   655,   257,\n",
            "          1862, 44873,  4908,   351],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  2666],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765,   284,   307,   994],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1612,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,    11],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  1234],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([35, 32, 18, 23, 42,  1,  5, 46, 14,  2, 34, 38, 17, 45, 48, 28, 39, 40,\n",
            "        39, 19, 13, 20, 20, 29, 33, 47, 44, 11, 33, 15,  9,  8,  3, 28, 12, 13,\n",
            "        48, 15, 43, 36, 29, 49, 45, 26, 10,  2, 31, 40, 29, 10],\n",
            "       device='cuda:0')\n",
            "topk 14 tensor([ 4908,  4908,  4908,  4908,  4908,    83, 12603,    69,   407,     6,\n",
            "          645,   340,   407,   287,  2099,    11, 18869,   655,   765, 44873,\n",
            "        44873, 18869,   765,   655, 18869,   340,     1, 44873,   765, 44873,\n",
            "        44873, 44873,   257,    13,   307,   314,   530,   314, 44873,  3988,\n",
            "          257,   783,    11,   257,  9811, 30960, 11778,   407,    64,  4302],\n",
            "       device='cuda:0')\n",
            "topk scores14 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9047, 0.9042, 0.6819,\n",
            "        0.6663, 0.6361, 0.6200, 0.5943, 0.5058, 0.4978, 0.4887, 0.3703, 0.3408,\n",
            "        0.3223, 0.3014, 0.2988, 0.2873, 0.2700, 0.2675, 0.2563, 0.2533, 0.2511,\n",
            "        0.2507, 0.2414, 0.2364, 0.2300, 0.2263, 0.2243, 0.2217, 0.2155, 0.2093,\n",
            "        0.2089, 0.2013, 0.1963, 0.1879, 0.1816, 0.1800, 0.1707, 0.1638, 0.1561,\n",
            "        0.1539, 0.1487, 0.1449, 0.1368, 0.1293], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "15 torch.Size([50, 15])\n",
            "depth 15 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           651,   340,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,  7356,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   345,   460,\n",
            "           651,   340,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1612,    11, 44873,  4908],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,  1101,   407],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,   910,   259,     6],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  2666,   340],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821,   407],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   262,  2099],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262,  6483,    11, 44873],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,   655, 18869],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,   655,   765],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,  1101,   655],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655, 18869],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  1234,   340],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,  2282,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655,   765],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262, 10162,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,  1760,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           651,   340,  1760,    11, 44873],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314,   447,   247,    76,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262,  6483,    11,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   262,   530],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262, 10162,    11,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,    11, 44873],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,  5490,   546,   262,  3988],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,  1101,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655, 18869,   307,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,   910,   259, 30960],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           765,   284,   307,   257, 11778],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,  1101,    64],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  4302]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([50, 50257])\n",
            "prev word tensor([ 0, 29, 31, 38, 30, 20, 27,  7, 25, 12, 41, 28, 45, 11, 18,  6, 49, 42,\n",
            "        26, 23, 17, 15,  8, 44,  9, 10, 43, 14, 22, 24, 47, 16, 25, 41, 11, 26,\n",
            "        10,  5, 13, 13, 17,  6, 34, 33, 16,  5, 44, 44, 47, 34],\n",
            "       device='cuda:0')\n",
            "topk 15 tensor([50256,  4908,  4908,  4908,  4908,  4908,  4908, 12603,   287,  3142,\n",
            "           11,   284,   921,  3436,   284,   284,    11, 44873,   475,   257,\n",
            "         2282, 44873,  3142,    11,   546,   640,   582,   286,   345,   307,\n",
            "          257,   307,   319,    13,   287,   290, 18437,  1392,  2253,   428,\n",
            "         1561,   588,   257,   775,   910,   772,   526, 44873,  2282,   262],\n",
            "       device='cuda:0')\n",
            "topk scores15 tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9738, 0.6603,\n",
            "        0.5274, 0.4490, 0.4365, 0.4199, 0.4156, 0.4131, 0.4083, 0.3961, 0.3898,\n",
            "        0.3550, 0.3523, 0.3458, 0.3225, 0.2917, 0.2829, 0.2645, 0.2568, 0.2445,\n",
            "        0.2370, 0.2343, 0.2153, 0.2084, 0.2064, 0.2011, 0.2010, 0.1966, 0.1925,\n",
            "        0.1871, 0.1833, 0.1767, 0.1742, 0.1636, 0.1447, 0.1428, 0.1360, 0.1352,\n",
            "        0.1098, 0.1084, 0.1073, 0.1065, 0.0955], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "16 torch.Size([49, 16])\n",
            "depth 16 tensor([[50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262, 10162,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   314,   460,\n",
            "           651,   340,  1760,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,    11, 44873,  4908],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,  1760,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   766,   502,\n",
            "           287,   262,  6483,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,  2282,    11, 44873,  4908],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  1234,   340,   287],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   345,   821,   407,  3142],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655,   765,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,   910,   259, 30960,   921],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  2666,   340,  3436],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   284],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  4302,    11],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,    11, 44873],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   475],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,  1101,   655,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655,  2282],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    11, 44873],\n",
            "        [50256,     1,    40,   836,   470,   765,   284,   307,   287,   257,\n",
            "          2292,   810,   314,  1101,   407,  3142],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,   910,   259,     6,   546],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645,   640],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655, 18869,   307,   257,   582],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   262,  2099,   286],\n",
            "        [50256,    32,   259,   470,   645,   761,   284,   307, 22461,   286,\n",
            "           340,    11,   314,   655,   765,   345],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655, 18869,   307],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  1234,   340,   319],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  2666,   340,   287],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,  1392],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,  2253],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655,  1561],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,   526],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,  2282],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   262]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([49, 50257])\n",
            "prev word tensor([ 0, 16, 46, 39, 24, 36, 17, 45, 26, 48,  7,  9, 38, 35, 19, 34, 48, 38,\n",
            "        43,  6, 10, 47, 44, 41,  0, 37, 13, 40, 44, 38, 33, 40, 44, 28, 45, 22,\n",
            "        34, 26, 13, 29, 42, 45, 11, 35, 30,  3, 32, 29, 12], device='cuda:0')\n",
            "topk 16 tensor([50256,  4908,  4908,   259,   329,   645,   314,   784, 44873,  1266,\n",
            "          257, 44873,  1499,   644,   326,   314,   530,  3277,   326,     1,\n",
            "          307,   326,   760,   582, 50256,    11,   307,   345,  1949,   995,\n",
            "          257,   514, 17753,   257,   851, 44873,   788,   582,   910, 25670,\n",
            "          836,   366, 17753,   546,   257,    13,   314,  4336,     0],\n",
            "       device='cuda:0')\n",
            "topk scores16 tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.7693, 0.6393, 0.5445, 0.5209, 0.5163,\n",
            "        0.4459, 0.4364, 0.4143, 0.4113, 0.3671, 0.3558, 0.3087, 0.2757, 0.2641,\n",
            "        0.2535, 0.2222, 0.2102, 0.2045, 0.2027, 0.2002, 0.1986, 0.1779, 0.1739,\n",
            "        0.1693, 0.1663, 0.1545, 0.1482, 0.1443, 0.1401, 0.1398, 0.1393, 0.1384,\n",
            "        0.1356, 0.1350, 0.1337, 0.1336, 0.1237, 0.1204, 0.1173, 0.1154, 0.1135,\n",
            "        0.1127, 0.1097, 0.1089, 0.1079], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "17 torch.Size([47, 17])\n",
            "depth 17 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655,  1561,   259],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645,   640,   329],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,  1392,   645],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   475,   314],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,   526,   784],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   262,  2099,   286, 44873],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   262,  1266],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  1234,   340,   287,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  1499],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437,   644],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655,  2282,   326],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   262,   530],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655,   765,   284,   307],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,  2282,   326],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,   760],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   257,   582],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,  2253,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,  1949],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  2666,   340,   287,   257],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655, 18869,   307,   257],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,   526,   851],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   788],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   262,  2099,   286,   582],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257, 25670],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,   526,   366],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,   910,   259, 30960,   921, 17753],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437,   546],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,  1760,    11, 44873,  4908,    13],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  2666,   340,  3436,     0]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([47, 50257])\n",
            "prev word tensor([ 0, 33,  7,  2, 45, 26, 15, 38, 40, 34, 16, 38, 29, 46, 23, 27, 36, 24,\n",
            "        39, 25, 17,  4,  1, 21, 25, 31,  4, 30, 14, 41, 34, 42, 25, 27, 18, 26,\n",
            "        18, 29,  5, 21, 46, 11, 44,  0, 42,  9, 15], device='cuda:0')\n",
            "topk 17 tensor([50256,  4908,  4908,     6,   286,   259,   326,   470,  1833,   314,\n",
            "          286,   447,     1,   347, 44873,   810,   326,  1598,  1639,     1,\n",
            "          526,   640,    11,   703,   526,   582, 18437,  3068,  1101,   340,\n",
            "          673,   582,   466,   286,  2728,  2616,   475,   526,  1101,   644,\n",
            "          921,   810,  1101,    30,   636,  3338,   508], device='cuda:0')\n",
            "topk scores17 tensor([1.0000, 1.0000, 1.0000, 0.9545, 0.9252, 0.7684, 0.7032, 0.5815, 0.4643,\n",
            "        0.4435, 0.4186, 0.4185, 0.3712, 0.3703, 0.3249, 0.3021, 0.2976, 0.2950,\n",
            "        0.2744, 0.2450, 0.2396, 0.2263, 0.2188, 0.2157, 0.2143, 0.2139, 0.1973,\n",
            "        0.1957, 0.1946, 0.1931, 0.1898, 0.1885, 0.1813, 0.1804, 0.1777, 0.1763,\n",
            "        0.1692, 0.1689, 0.1636, 0.1604, 0.1542, 0.1530, 0.1529, 0.1480, 0.1477,\n",
            "        0.1434, 0.1416], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "18 torch.Size([46, 18])\n",
            "depth 18 tensor([[50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   262,  2099,   286, 44873,  4908],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   655,  1561,   259,     6],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,  1949,   259],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   262,   530,   326],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,   910,   259, 30960,   921, 17753,  1833],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   788,   314],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  2666,   340,  3436,     0,   347],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,  2253,    11, 44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   810],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,   526,   366,  1639],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,  1392,   645,   640],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811, 44873,  4908,    11],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,   760,   703],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,   546,\n",
            "           340,    11,   314,   655, 18869,   307,   257,   582],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,  1392,   645, 18437],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753,  3068],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437,   546,   340],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   788,   673],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   582],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   286],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1,  2728],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,  1949,  2616],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1,   475],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,   526],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   475,   314,  1101],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,   760,   644],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  2666,   340,  3436,     0,   921],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  1499,   810],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,    11, 44873,  4908,    30],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  1234,   340,   287,   257,  3338],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   892,    11,\n",
            "           314,   655,   765,   284,   307,   262,   530,   508]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([46, 50257])\n",
            "prev word tensor([ 0, 10, 43,  9, 32, 20, 44, 27,  6, 35, 26, 25, 31, 37, 19, 15,  7, 28,\n",
            "        29, 23, 33,  4, 36, 33, 11, 11, 19, 31, 26, 18, 18, 41, 16, 19,  6, 17,\n",
            "         0,  7,  3, 27, 38, 41,  3,  0, 11, 18], device='cuda:0')\n",
            "topk 18 tensor([50256,   247,   286, 16903, 16903,   329,  1295,  2611,   761,   314,\n",
            "          340,   644,   284,   407,   784,   526,   326,  1865,   531,   314,\n",
            "          673,     6,   314,   314,   290,   284,   366,     1,   326,   290,\n",
            "          284,   407,   526,   851,   765,   821,    13,    11,   262,    64,\n",
            "          340,   655,   340,   526,   475,   475], device='cuda:0')\n",
            "topk scores18 tensor([1.0000, 1.0000, 0.9960, 0.9918, 0.7914, 0.7779, 0.6015, 0.5716, 0.4754,\n",
            "        0.4443, 0.4194, 0.3802, 0.3738, 0.3559, 0.3418, 0.3046, 0.3029, 0.2911,\n",
            "        0.2901, 0.2734, 0.2708, 0.2617, 0.2498, 0.2297, 0.1977, 0.1969, 0.1965,\n",
            "        0.1963, 0.1945, 0.1930, 0.1809, 0.1761, 0.1754, 0.1724, 0.1722, 0.1712,\n",
            "        0.1690, 0.1654, 0.1491, 0.1490, 0.1473, 0.1467, 0.1328, 0.1272, 0.1208,\n",
            "        0.1188], device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "19 torch.Size([45, 19])\n",
            "depth 19 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   286, 16903],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,  1392,   645,   640,   329],\n",
            "        [50256,    33,  2007,    11,   345,  1365,   651,   534,   840,   503,\n",
            "           262, 48083,   290,  1234,   340,   287,   257,  3338,  1295],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,  2611],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   761],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1,   475,   314],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753,  3068,   340],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,  1392,   645, 18437,   644],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   475,   314,  1101,   407],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   784],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,   910,   259, 30960,   921, 17753,  1833,   326],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437,   546,   340,  1865],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   788,   673,   531],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1,  2728,   673],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,  1949,   259,     6],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,   526,   314],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1,  2728,   314],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   284],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   366],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466,     1],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753,  3068,   326],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   290],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101,   407],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   851],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   765],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,   526,   366,  1639,   821],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908,    13],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   644,\n",
            "           314,  1101,   910,   259, 30960,   921, 17753,  1833,    11],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   262],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,    64],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,   760,   644,   340],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101,   655],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908,   526],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   475],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([ 0,  1, 42, 35, 29, 44, 31, 33, 43, 14,  3, 24,  2, 22, 19,  7,  2, 39,\n",
            "         3, 16,  9, 21, 18,  7, 28,  8, 40, 23, 26, 44, 42, 38, 41, 43, 16, 14,\n",
            "        26, 30, 14, 13, 30, 31,  1, 39, 31], device='cuda:0')\n",
            "topk 19 tensor([  83,  340,  784,  921,  307,  314,  784,  284,  314,  784,   11,  307,\n",
            "          11, 1101,  338,  284,   13,  318,   13,   13,  287, 1101, 1101,  257,\n",
            "         314, 1101, 2282,  314,  284,  673,  851, 1560,  526,  673,    0,  851,\n",
            "         290,  257,  366,  366, 2282,  366,  262,  338,  851], device='cuda:0')\n",
            "topk scores19 tensor([1.0000, 0.4889, 0.4250, 0.4150, 0.3990, 0.3943, 0.3913, 0.3732, 0.3688,\n",
            "        0.3535, 0.3260, 0.3244, 0.3109, 0.3098, 0.2997, 0.2951, 0.2946, 0.2840,\n",
            "        0.2673, 0.2506, 0.2389, 0.2369, 0.2345, 0.2321, 0.2291, 0.2254, 0.2112,\n",
            "        0.2077, 0.2067, 0.2006, 0.2003, 0.1968, 0.1890, 0.1882, 0.1827, 0.1790,\n",
            "        0.1765, 0.1753, 0.1731, 0.1716, 0.1631, 0.1551, 0.1526, 0.1510, 0.1471],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "20 torch.Size([45, 20])\n",
            "depth 20 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247,    83],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   340],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908,   526,   784],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908,    13,   921],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   284,   307],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   784],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   765,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   475,   314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   784],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   286, 16903,    11],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   284,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    11],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1,  2728,   314,  1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1,  2728,   673,   338],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   761,   284],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,   760,   644,   340,   318],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   286, 16903,    13],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437,   546,   340,  1865,    13],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753,  3068,   340,   287],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,   526,   314,  1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314,  1101],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   761,   257],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   290,   314],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1,   475,   314,  1101],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101,   655,  2282],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290,   314],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466,     1,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   673],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908,   526,   851],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,    64,  1560],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   475,   673],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437,   546,   340,  1865,     0],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   851],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466,     1,   290],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101,   407,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   910,   326,   526,   366],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   910,   326,   526,   784,   366],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101,   407,  2282],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,   760,   644,   340,   338],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   851]],\n",
            "       device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([43, 31, 42, 34, 12,  0, 22, 28, 25, 21, 32, 27, 10, 26, 24, 13, 28,  5,\n",
            "         8, 33,  0, 19, 36, 32, 21, 29, 17, 22, 42, 32, 33, 20, 31,  7,  1, 15,\n",
            "        20,  2, 16, 37, 41, 41, 29,  7, 43], device='cuda:0')\n",
            "topk 20 tensor([  588,   345,  5273,   447, 44873,   761,  2611,   307,   407,  2611,\n",
            "          784,  1101, 44873,    11,  1101,   407,   651,  1101,  1101, 18959,\n",
            "          765,   447,   314,   366,   407, 18959,   588,   407,   983,   851,\n",
            "          338,   616, 21349,   307,   526,   307,   428,   366,   775,  4336,\n",
            "         1639,  2061,   338,   766,   546], device='cuda:0')\n",
            "topk scores20 tensor([0.7001, 0.5467, 0.4899, 0.4556, 0.4312, 0.4263, 0.3986, 0.3845, 0.3834,\n",
            "        0.3751, 0.3531, 0.3072, 0.3012, 0.3002, 0.2936, 0.2691, 0.2441, 0.2263,\n",
            "        0.2232, 0.2125, 0.2105, 0.2016, 0.2011, 0.1878, 0.1836, 0.1795, 0.1766,\n",
            "        0.1751, 0.1750, 0.1714, 0.1712, 0.1702, 0.1688, 0.1671, 0.1669, 0.1652,\n",
            "        0.1642, 0.1591, 0.1518, 0.1518, 0.1510, 0.1502, 0.1497, 0.1484, 0.1483],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "21 torch.Size([45, 21])\n",
            "depth 21 tensor([[50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,   760,   644,   340,   338,\n",
            "           588],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,    64,  1560,\n",
            "           345],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262,\n",
            "          5273],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437,   546,   340,  1865,     0,\n",
            "           447],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    11,\n",
            "         44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247,    83,\n",
            "           761],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314,  1101,\n",
            "          2611],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466,     1,   284,\n",
            "           307],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1,   475,   314,  1101,\n",
            "           407],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,   526,   314,  1101,\n",
            "          2611],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526,\n",
            "           784],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290,   314,\n",
            "          1101],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   286, 16903,    11,\n",
            "         44873],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101,   655,  2282,\n",
            "            11],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   290,   314,\n",
            "          1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40, 18959,   470,\n",
            "         17753,   307,   257,  2802,    69, 12603,     1,  2728,   314,  1101,\n",
            "           407],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466,     1,   284,\n",
            "           651],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   314,\n",
            "          1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   475,   314,\n",
            "          1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   475,   673,\n",
            "         18959],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247,    83,\n",
            "           765],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437,   546,   340,  1865,    13,\n",
            "           447],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466,     1,   290,\n",
            "           314],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526,\n",
            "           366],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,   526,   314,  1101,\n",
            "           407],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   673,\n",
            "         18959],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,   760,   644,   340,   318,\n",
            "           588],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314,  1101,\n",
            "           407],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262,\n",
            "           983],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526,\n",
            "           851],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   475,   673,\n",
            "           338],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753,  3068,   340,   287,\n",
            "           616],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,    64,  1560,\n",
            "         21349],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   765,   284,\n",
            "           307],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   340,\n",
            "           526],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   761,   284,\n",
            "           307],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753,  3068,   340,   287,\n",
            "           428],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908,   526,   784,\n",
            "           366],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101,   407,   257,\n",
            "          4336],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366,\n",
            "          1639],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366,\n",
            "          2061],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   673,\n",
            "           338],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   765,   284,\n",
            "           766],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,   760,   644,   340,   338,\n",
            "           546]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([45, 50257])\n",
            "prev word tensor([ 0, 12, 21,  3, 39, 13, 14, 11, 36, 20, 34, 18,  5, 17, 22,  1,  5, 41,\n",
            "        44, 30, 34,  6, 10,  9, 42, 37, 38, 23, 23, 34, 32,  9, 43,  1,  6,  2,\n",
            "        29, 17, 36,  5, 35, 28, 34, 11, 37], device='cuda:0')\n",
            "topk 21 tensor([50256,  4908,   251,   251,   286,   366,  2611,  2611,  3496,   284,\n",
            "          784,   407,   284,   407,  1101,   326,   257,   338,     0,   407,\n",
            "          366,  1560,   366,  1560,   407,  1639,   836,  1639,  2061,   851,\n",
            "          326,   307,  1194,   644,   307,   526,   366,  2611, 10651,   645,\n",
            "        12008,   526,   532,    64,  2061], device='cuda:0')\n",
            "topk scores21 tensor([1.0000, 1.0000, 0.9999, 0.9999, 0.9830, 0.7237, 0.6670, 0.5931, 0.5558,\n",
            "        0.3822, 0.3554, 0.3341, 0.3309, 0.2965, 0.2758, 0.2494, 0.2326, 0.2143,\n",
            "        0.2047, 0.2038, 0.1919, 0.1876, 0.1860, 0.1833, 0.1825, 0.1784, 0.1736,\n",
            "        0.1643, 0.1624, 0.1598, 0.1573, 0.1493, 0.1463, 0.1448, 0.1434, 0.1095,\n",
            "        0.1093, 0.1063, 0.1059, 0.1050, 0.1040, 0.0999, 0.0986, 0.0980, 0.0958],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "22 torch.Size([44, 22])\n",
            "depth 22 tensor([[50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   286, 16903,    11,\n",
            "         44873,  4908],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437,   546,   340,  1865,    13,\n",
            "           447,   251],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   470,  1392,   645, 18437,   546,   340,  1865,     0,\n",
            "           447,   251],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101,   407,   257,\n",
            "          4336,   286],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101,   655,  2282,\n",
            "            11,   366],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   290,   314,\n",
            "          1101,  2611],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290,   314,\n",
            "          1101,  2611],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753,  3068,   340,   287,\n",
            "           428,  3496],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247,    83,\n",
            "           765,   284],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   340,\n",
            "           526,   784],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   475,   314,\n",
            "          1101,   407],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247,    83,\n",
            "           761,   284],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   314,\n",
            "          1101,   407],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466,     1,   290,\n",
            "           314,  1101],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,    64,  1560,\n",
            "           345,   326],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247,    83,\n",
            "           761,   257],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366,\n",
            "          2061,   338],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772,   760,   644,   340,   338,\n",
            "           546,     0],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   475,   673,\n",
            "           338,   407],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   340,\n",
            "           526,   366],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314,  1101,\n",
            "          2611,  1560],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526,\n",
            "           784,   366],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,   526,   314,  1101,\n",
            "          2611,  1560],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   673,\n",
            "           338,   407],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908,   526,   784,\n",
            "           366,  1639],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526,\n",
            "           366,  1639],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526,\n",
            "           366,  2061],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   340,\n",
            "           526,   851],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,    64,  1560,\n",
            "         21349,   326],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,   526,   314,  1101,\n",
            "          2611,   307],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   765,   284,\n",
            "           766,  1194],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,    64,  1560,\n",
            "           345,   644],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   526,   314,  1101,\n",
            "          2611,   307],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262,\n",
            "          5273,   526],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526,\n",
            "           851,   366],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   314,\n",
            "          1101,  2611],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753,  3068,   340,   287,\n",
            "           428, 10651],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247,    83,\n",
            "           761,   645],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   761,   284,\n",
            "           307, 12008],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262,\n",
            "           983,   526],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   340,\n",
            "           526,   532],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290,   314,\n",
            "          1101,    64],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908,   526,   784,\n",
            "           366,  2061]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([44, 50257])\n",
            "prev word tensor([ 0, 22, 25, 13, 31, 25, 29, 34, 40,  4, 27, 37, 16,  0, 14,  0, 32, 35,\n",
            "        40,  0, 18, 19, 23, 42, 34, 34,  8, 40,  9, 43, 11, 24, 21, 16, 16, 31,\n",
            "         5, 36, 19,  8, 36, 22,  6, 37], device='cuda:0')\n",
            "topk 22 tensor([50256,   345,   470,  2611,  1110,   447,   338,   784,   784, 34094,\n",
            "          338,     0,   262,    30,   338,     0,   284,  2061,   851,    13,\n",
            "          262,  1639,   262,  1560,   851,   366,   307,   366,   366,   338,\n",
            "          307,   821,  2061,   534,  2642,   530,  1560,   466,  2061,   766,\n",
            "          307,   607,  1560,   996], device='cuda:0')\n",
            "topk scores22 tensor([0.7245, 0.6929, 0.6079, 0.5956, 0.4338, 0.3921, 0.3850, 0.3630, 0.3572,\n",
            "        0.3158, 0.2638, 0.2562, 0.2552, 0.2097, 0.2095, 0.2072, 0.2019, 0.1895,\n",
            "        0.1817, 0.1777, 0.1773, 0.1741, 0.1702, 0.1672, 0.1630, 0.1589, 0.1581,\n",
            "        0.1581, 0.1559, 0.1557, 0.1513, 0.1500, 0.1489, 0.1487, 0.1457, 0.1399,\n",
            "        0.1394, 0.1373, 0.1369, 0.1361, 0.1361, 0.1309, 0.1259, 0.1216],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "23 torch.Size([43, 23])\n",
            "depth 23 tensor([[50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,   526,   314,  1101,\n",
            "          2611,  1560,   345],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836,   470],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,   466,     1,   290,\n",
            "           314,  1101,  2611],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   765,   284,\n",
            "           766,  1194,  1110],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,  3277,   286, 16903,    13,\n",
            "           775,   836,   447],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,    64,  1560,\n",
            "         21349,   326,   338],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262,\n",
            "          5273,   526,   784],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262,\n",
            "           983,   526,   784],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   326,   345,\n",
            "           460,   651,   340,   826,   783,    13,   314,  1101,   655,  2282,\n",
            "            11,   366, 34094],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526,\n",
            "           366,  2061,   338],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753,  3068,   340,   287,\n",
            "           428, 10651,     0],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366,\n",
            "          2061,   338,   262],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   286, 16903,    11,\n",
            "         44873,  4908,    30],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,    64,  1560,\n",
            "           345,   326,   338],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   286, 16903,    11,\n",
            "         44873,  4908,     0],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "         18869,   307,   257, 25670,     1,   290,   314,  1101,    64,  1560,\n",
            "           345,   644,   284],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526,\n",
            "           851,   366,  2061],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262,\n",
            "           983,   526,   851],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   994,   287,   428,   995,   286, 16903,    11,\n",
            "         44873,  4908,    13],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   475,   673,\n",
            "           338,   407,   262],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   340,\n",
            "           526,   366,  1639],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   673,\n",
            "           338,   407,   262],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290,   314,\n",
            "          1101,    64,  1560],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262,\n",
            "          5273,   526,   851],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262,\n",
            "          5273,   526,   366],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247,    83,\n",
            "           765,   284,   307],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   262,\n",
            "           983,   526,   366],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   340,\n",
            "           526,   784,   366],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908,   526,   784,\n",
            "           366,  2061,   338],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247,    83,\n",
            "           761,   284,   307],\n",
            "        [50256,     1,    40,   836,   470,   765,   345,   284,   760,   326,\n",
            "           314,  1101,   407,   257,  9811,    11, 44873,  4908,   526,   784,\n",
            "           366,  1639,   821],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,  1101,   407,   257,  4336,   286,   340,   526,\n",
            "           784,   366,  2061],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366,\n",
            "          2061,   338,   534],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655,   765,   284,   307,  1598,   526,   366,\n",
            "          2061,   338,  2642],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   470,   765,   284,\n",
            "           766,  1194,   530],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   290,   314,\n",
            "          1101,  2611,  1560],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   314,\n",
            "          1101,  2611,   466],\n",
            "        [50256,     1,    40,   836,   470,   760,   644,   345,   821,  3375,\n",
            "           546,    11,   314,   655, 18869,   307,   257,   636,   286,   340,\n",
            "           526,   366,  2061],\n",
            "        [50256,    46,  1860, 10898,   318,   262,   691,   835,   356,   460,\n",
            "           651,   340,   826,   783,    13,   775,   836,   447,   247,    83,\n",
            "           765,   284,   766],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   345,     1,   475,   314,\n",
            "          1101,  2611,   307],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,   526,   314,  1101,\n",
            "          2611,  1560,   607],\n",
            "        [50256,    44,  1689,  1297,   502,    11,   366,    40,   836,   470,\n",
            "           761,   257,  2802,    69, 12603,   588,   514,     1,   290,   314,\n",
            "          1101,  2611,  1560],\n",
            "        [50256,    34,   963,   502,   319,   262,  2975,   284, 35000,    11,\n",
            "           314, 18959,   447,   247,    83,   772, 17753,  3068,   340,   287,\n",
            "           428, 10651,   996]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([43, 50257])\n",
            "prev word tensor([ 4, 15, 33, 36,  1, 22, 41, 19, 28, 21, 11, 21,  9, 41, 22, 19, 32, 11,\n",
            "        37, 32,  0,  3, 35, 16, 31, 26, 13,  5, 34, 24,  2,  1,  5, 13, 24, 35,\n",
            "        41,  9, 38, 18,  9, 22,  3], device='cuda:0')\n",
            "topk 23 tensor([  247,   466,   351,   340,   761,   345,   345,   530,   262,   530,\n",
            "          966,  2099,   262,   607,   607,  2099,   966,  1917,   338,  1917,\n",
            "          644,   286,   607,   338,   338,  1639,  1521,   644,   467,  2061,\n",
            "         1560,   765,  1521,   644,  1639, 21349, 21349,   326,  1194,   775,\n",
            "          534, 21349,   618], device='cuda:0')\n",
            "topk scores23 tensor([1.0000, 0.8884, 0.8755, 0.7204, 0.4899, 0.4874, 0.4781, 0.3573, 0.3550,\n",
            "        0.3549, 0.2912, 0.2853, 0.2755, 0.2735, 0.2726, 0.2592, 0.2483, 0.2452,\n",
            "        0.2391, 0.2152, 0.2126, 0.2115, 0.1960, 0.1916, 0.1915, 0.1854, 0.1733,\n",
            "        0.1721, 0.1639, 0.1602, 0.1601, 0.1591, 0.1562, 0.1521, 0.1484, 0.1477,\n",
            "        0.1473, 0.1465, 0.1438, 0.1356, 0.1339, 0.1333, 0.1314],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "24 torch.Size([43, 24])\n",
            "depth 24 tensor([[50256,    46,  1860,  ...,   836,   447,   247],\n",
            "        [50256,    44,  1689,  ...,   644,   284,   466],\n",
            "        [50256,     1,    40,  ...,   338,  2642,   351],\n",
            "        ...,\n",
            "        [50256,     1,    40,  ...,  2061,   338,   534],\n",
            "        [50256,    44,  1689,  ...,    64,  1560, 21349],\n",
            "        [50256,    46,  1860,  ...,  1194,  1110,   618]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([43, 50257])\n",
            "prev word tensor([ 0, 11, 15, 16, 28, 10, 19, 37,  8, 38,  1,  3, 31, 17,  9,  7, 12, 18,\n",
            "        40, 21, 24, 23, 12, 17,  4, 40,  4,  6, 29, 19, 17, 10,  5, 27,  8,  4,\n",
            "        15,  8, 39, 30,  1, 11,  2], device='cuda:0')\n",
            "topk 24 tensor([  83,  284,  284, 1701, 2642,  286, 1701, 1701,  966, 1110,  618,  329,\n",
            "         284, 1701,  326,  326, 1917,  262,  966,  428,  262,  262,  966,  351,\n",
            "         284, 1917,  257,  326,  338,   30,   30, 1701,  326,  345, 1917,  645,\n",
            "         286, 5089,  836,  607,  611,  286,  326], device='cuda:0')\n",
            "topk scores24 tensor([1.0000, 0.7079, 0.6953, 0.6722, 0.6484, 0.6359, 0.5464, 0.4925, 0.4146,\n",
            "        0.3830, 0.3688, 0.3634, 0.3459, 0.3249, 0.3006, 0.2951, 0.2900, 0.2713,\n",
            "        0.2698, 0.2694, 0.2672, 0.2646, 0.2487, 0.2458, 0.2443, 0.2391, 0.2359,\n",
            "        0.2312, 0.2310, 0.2098, 0.2067, 0.2067, 0.2029, 0.1982, 0.1978, 0.1950,\n",
            "        0.1891, 0.1847, 0.1839, 0.1788, 0.1757, 0.1666, 0.1643],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "25 torch.Size([43, 25])\n",
            "depth 25 tensor([[50256,    46,  1860,  ...,   447,   247,    83],\n",
            "        [50256,    44,  1689,  ...,   262,  2099,   284],\n",
            "        [50256,    44,  1689,  ...,   262,  2099,   284],\n",
            "        ...,\n",
            "        [50256,    44,  1689,  ...,   284,   466,   611],\n",
            "        [50256,    44,  1689,  ...,   262,  2099,   286],\n",
            "        [50256,     1,    40,  ...,  2642,   351,   326]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([43, 50257])\n",
            "prev word tensor([ 8, 42, 18, 38, 25,  0, 38,  4, 29, 34, 33, 17, 19, 14, 30, 16, 10, 27,\n",
            "        21, 15, 20, 32, 16,  1,  0,  2, 34, 24, 12,  9, 25,  5, 16, 28, 17, 21,\n",
            "        34,  0, 21, 28,  5, 42, 18], device='cuda:0')\n",
            "topk 25 tensor([  286,  1701,  1701,   470,  1701,   761,   447,    11,   921,  1701,\n",
            "        17753,  1917,  1204,   314,   921,  1701,   340,   356,   966,   314,\n",
            "          966,   356,   351,   307, 50256,   307,   351,   307,   307,   286,\n",
            "           30,   852,    30,   510,   966,  5089,    30,   765,  1917,   326,\n",
            "         2282,    30,    30], device='cuda:0')\n",
            "topk scores25 tensor([0.8068, 0.7795, 0.6471, 0.6107, 0.5197, 0.4533, 0.3892, 0.3878, 0.3681,\n",
            "        0.3522, 0.3416, 0.3174, 0.3161, 0.3053, 0.3032, 0.2979, 0.2863, 0.2831,\n",
            "        0.2812, 0.2801, 0.2787, 0.2703, 0.2696, 0.2590, 0.2556, 0.2514, 0.2502,\n",
            "        0.2486, 0.2456, 0.2408, 0.2362, 0.2244, 0.2141, 0.2138, 0.2113, 0.2084,\n",
            "        0.1901, 0.1853, 0.1715, 0.1659, 0.1607, 0.1586, 0.1572],\n",
            "       device='cuda:0', grad_fn=<TopkBackward0>)\n",
            "26 torch.Size([42, 26])\n",
            "depth 26 tensor([[50256,     1,    40,  ...,   262,   966,   286],\n",
            "        [50256,     1,    40,  ...,   351,   326,  1701],\n",
            "        [50256,     1,    40,  ...,   534,   966,  1701],\n",
            "        ...,\n",
            "        [50256,     1,    40,  ...,   966,   286,  2282],\n",
            "        [50256,     1,    40,  ...,   351,   326,    30],\n",
            "        [50256,     1,    40,  ...,   534,   966,    30]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([42, 50257])\n",
            "prev word tensor([ 6, 10, 30,  3, 38, 40, 16, 29, 36, 41, 39, 37, 11, 31, 35, 32, 16,  5,\n",
            "        37, 28,  5, 11,  0, 11, 25, 12, 35, 33, 37, 13, 22, 19,  5,  3, 17, 26,\n",
            "        18, 20, 13, 12, 34, 19], device='cuda:0')\n",
            "topk 26 tensor([  247,   466,   257,   761,  1701,   921,   338,   921,   284,   921,\n",
            "          326,  1701,  1701,   921,   921,  1701,  2058,   284,   351,   428,\n",
            "          257,   351,   852,    30,   852,    11, 13732,  1701,    30,   765,\n",
            "          326,   765,   645,   765,  1842, 22461,  1701,  1701,  1101,   326,\n",
            "         1701,  1101], device='cuda:0')\n",
            "topk scores26 tensor([1.0000, 0.8601, 0.5848, 0.5547, 0.5389, 0.5082, 0.4035, 0.3811, 0.3622,\n",
            "        0.3469, 0.3371, 0.3221, 0.3105, 0.3056, 0.3035, 0.2919, 0.2830, 0.2824,\n",
            "        0.2815, 0.2789, 0.2459, 0.2381, 0.2329, 0.2203, 0.2156, 0.2026, 0.1918,\n",
            "        0.1849, 0.1811, 0.1795, 0.1782, 0.1759, 0.1543, 0.1489, 0.1405, 0.1399,\n",
            "        0.1347, 0.1344, 0.1327, 0.1275, 0.1270, 0.1224], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "27 torch.Size([42, 27])\n",
            "depth 27 tensor([[50256,    46,  1860,  ...,   836,   447,   247],\n",
            "        [50256,    44,  1689,  ...,   345, 17753,   466],\n",
            "        [50256,     1,    40,  ...,   286,   852,   257],\n",
            "        ...,\n",
            "        [50256,    46,  1860,  ...,   428,  1204,   326],\n",
            "        [50256,     1,    40,  ...,   262,  5089,  1701],\n",
            "        [50256,    44,  1689,  ...,   326,   314,  1101]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([42, 50257])\n",
            "prev word tensor([ 0, 30, 10,  6, 31, 29, 33, 28, 23, 41, 19,  3, 39, 38, 10, 30, 17, 25,\n",
            "         8, 24, 34,  1,  3, 14,  2,  1, 21, 24,  3, 19,  2, 21, 29, 31,  8,  8,\n",
            "         2, 18, 18, 22, 22, 18], device='cuda:0')\n",
            "topk 27 tensor([   83,  1701,  1701,   640,   284,   284,   284,   921,   921,  4385,\n",
            "         1204,   284,   338,  4385,    30,    30,   307, 44873,   307,  2042,\n",
            "          607,   284,   257, 17753,   582,   618,   852,  2330,   645,  7510,\n",
            "        44873,   326,   607,   607,   766,  1309,  2802,   852,   326,  2330,\n",
            "         2042,   428], device='cuda:0')\n",
            "topk scores27 tensor([1.0000, 0.7241, 0.6202, 0.4641, 0.4416, 0.4341, 0.3873, 0.3411, 0.3408,\n",
            "        0.3293, 0.3145, 0.3062, 0.2957, 0.2555, 0.2455, 0.2428, 0.2388, 0.2379,\n",
            "        0.2251, 0.2240, 0.2232, 0.2091, 0.2083, 0.2069, 0.2003, 0.1997, 0.1970,\n",
            "        0.1766, 0.1670, 0.1586, 0.1539, 0.1516, 0.1418, 0.1398, 0.1295, 0.1281,\n",
            "        0.1212, 0.1060, 0.1046, 0.1000, 0.0990, 0.0970], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "28 torch.Size([42, 28])\n",
            "depth 28 tensor([[50256,    46,  1860,  ...,   447,   247,    83],\n",
            "        [50256,     1,    40,  ...,   351,   326,  1701],\n",
            "        [50256,     1,    40,  ...,  2282,   326,  1701],\n",
            "        ...,\n",
            "        [50256,     1,    40,  ...,   286,   852,  2330],\n",
            "        [50256,     1,    40,  ...,   286,   852,  2042],\n",
            "        [50256,     1,    40,  ...,  1917,   351,   428]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([42, 50257])\n",
            "prev word tensor([17,  9, 13, 30, 33, 32, 38, 31, 41,  0, 36, 14, 40, 23, 15, 36, 39, 27,\n",
            "        40, 19, 19, 25, 24, 24,  4, 39, 21, 31,  5,  6, 11, 38,  3, 27, 10, 37,\n",
            "         0,  8, 16, 20, 41, 34], device='cuda:0')\n",
            "topk 28 tensor([ 4908,   284,   284,  4908,   284,   284,  1701,  1701,  1701,   761,\n",
            "           69,   921,  1701,  1833,   921, 31699,    30,    30,    30,    30,\n",
            "         1701,   340,  1701,    30,   307,  1701,   651,    30,   307,   307,\n",
            "          307,    30,   329,  1701,    11,  2042,   765, 17753, 22461,    13,\n",
            "           30,  1194], device='cuda:0')\n",
            "topk scores28 tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.9914, 0.9912, 0.7441, 0.7229, 0.5370,\n",
            "        0.5111, 0.4929, 0.4773, 0.4460, 0.4419, 0.4160, 0.3928, 0.3841, 0.3840,\n",
            "        0.3781, 0.3466, 0.3430, 0.3242, 0.3098, 0.3054, 0.3011, 0.2491, 0.2440,\n",
            "        0.2437, 0.2424, 0.2308, 0.2270, 0.2228, 0.2069, 0.2029, 0.1903, 0.1790,\n",
            "        0.1772, 0.1564, 0.1543, 0.1537, 0.1347, 0.1266], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "29 torch.Size([42, 29])\n",
            "depth 29 tensor([[50256,    46,  1860,  ...,    11, 44873,  4908],\n",
            "        [50256,    44,  1689,  ...,  1101,  4385,   284],\n",
            "        [50256,    44,  1689,  ...,  1101,  4385,   284],\n",
            "        ...,\n",
            "        [50256,    44,  1689,  ...,  1842,   607,    13],\n",
            "        [50256,     1,    40,  ...,   351,   428,    30],\n",
            "        [50256,    46,  1860,  ...,   284,   766,  1194]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([42, 50257])\n",
            "prev word tensor([ 0, 10, 21,  4, 23, 40, 31, 27,  5, 16, 18, 36,  1,  0, 17, 35, 19,  9,\n",
            "        35, 17, 19,  2, 26, 18, 16, 41, 28, 24,  3, 34,  3,  9, 37, 39, 37,  9,\n",
            "         1, 10,  2, 32, 24, 28], device='cuda:0')\n",
            "topk 29 tensor([50256, 12603,  2058,   307,   921,   921,   921,   921,   307,   921,\n",
            "          921,   284,   307,     0,   921,    30,   921,   284,  1701, 13732,\n",
            "        13732,   307,   340, 13732, 13732,  1110,   351,   351,    30, 44873,\n",
            "         1701,   257,  1833,  1375,   423,   645,  1842, 19296,  1842,   514,\n",
            "           13,    13], device='cuda:0')\n",
            "topk scores29 tensor([0.9985, 0.8778, 0.5695, 0.5003, 0.4776, 0.4566, 0.4544, 0.4504, 0.4377,\n",
            "        0.4319, 0.4061, 0.4050, 0.3947, 0.3904, 0.3876, 0.3776, 0.3641, 0.3625,\n",
            "        0.3327, 0.3316, 0.3279, 0.3128, 0.2983, 0.2917, 0.2817, 0.2587, 0.2554,\n",
            "        0.2542, 0.2340, 0.2200, 0.2182, 0.2031, 0.1794, 0.1526, 0.1504, 0.1378,\n",
            "        0.1323, 0.1222, 0.1179, 0.1127, 0.1070, 0.1017], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "30 torch.Size([41, 30])\n",
            "depth 30 tensor([[50256,     1,    40,  ...,  2802,    69, 12603],\n",
            "        [50256,    44,  1689,  ...,   618,   340,  2058],\n",
            "        [50256,    44,  1689,  ...,   607,   284,   307],\n",
            "        ...,\n",
            "        [50256,    44,  1689,  ...,   640,   329,   514],\n",
            "        [50256,    44,  1689,  ...,   284,   307,    13],\n",
            "        [50256,    44,  1689,  ...,   284,   307,    13]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([41, 50257])\n",
            "prev word tensor([28,  1, 27,  0, 14, 14, 39, 40, 32,  0, 20, 31, 15, 11, 16, 13, 10,  9,\n",
            "         8, 26,  7,  2,  3,  2,  7, 11, 35, 20,  6, 35, 10,  7,  8, 10, 21, 24,\n",
            "        24,  9, 21,  2, 10], device='cuda:0')\n",
            "topk 30 tensor([ 4908,   284,   921,  1701, 13732,   921,  1375,  1375,   531,    30,\n",
            "          351,   326, 17753,   351,   307, 17753,   307, 17753, 17753,   345,\n",
            "          526,   526, 17753,    13,    13,  6613,   345,  6613, 17753,   607,\n",
            "         2107,   351,   821,   766,    13,   467,   588,   821,   826,   351,\n",
            "         1309], device='cuda:0')\n",
            "topk scores30 tensor([1.0000, 0.9969, 0.5264, 0.4218, 0.3595, 0.3574, 0.3313, 0.3299, 0.3203,\n",
            "        0.2579, 0.2494, 0.2473, 0.2339, 0.2291, 0.2264, 0.2223, 0.2205, 0.2093,\n",
            "        0.2083, 0.1947, 0.1852, 0.1770, 0.1678, 0.1515, 0.1471, 0.1254, 0.1221,\n",
            "        0.1161, 0.1121, 0.1083, 0.1072, 0.1057, 0.1052, 0.1046, 0.1017, 0.1016,\n",
            "        0.1001, 0.0963, 0.0933, 0.0897, 0.0895], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "31 torch.Size([41, 31])\n",
            "depth 31 tensor([[50256,    46,  1860,  ...,    11, 44873,  4908],\n",
            "        [50256,    44,  1689,  ...,   340,  2058,   284],\n",
            "        [50256,     1,    40,  ...,  4908,    30,   921],\n",
            "        ...,\n",
            "        [50256,    44,  1689,  ...,   651,   340,   826],\n",
            "        [50256,    44,  1689,  ...,   284,   307,   351],\n",
            "        [50256,    46,  1860,  ...,   765,   284,  1309]], device='cuda:0')\n",
            "hidden tensor([[[ 0.2379,  0.3954, -0.4381,  ...,  0.8083, -0.5219,  0.5767],\n",
            "         [ 0.2393,  0.3952, -0.4379,  ...,  0.8061, -0.5230,  0.5773],\n",
            "         [ 0.2374,  0.3946, -0.4366,  ...,  0.8084, -0.5231,  0.5757],\n",
            "         ...,\n",
            "         [ 0.2372,  0.3953, -0.4366,  ...,  0.8098, -0.5222,  0.5752],\n",
            "         [ 0.2360,  0.3949, -0.4356,  ...,  0.8115, -0.5224,  0.5741],\n",
            "         [ 0.2367,  0.3952, -0.4378,  ...,  0.8099, -0.5217,  0.5760]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "next torch.Size([41, 50257])\n",
            "prev word tensor([ 0, 35, 25, 27,  9, 23, 24,  0, 18, 17, 12, 15, 20, 21,  6,  7, 15, 18,\n",
            "        29, 38, 17, 12,  2, 28, 19, 19, 28,  0, 34, 33, 36, 26,  1,  5, 17, 22,\n",
            "        10, 18, 13, 39, 31], device='cuda:0')\n",
            "topk 31 tensor([50256,   416,   286,   286,   921,  1375,  1375,     0,   307,   307,\n",
            "         1833,  1833,  1375,  1375,   655,   655,   307,  1833,    13,    13,\n",
            "         1833,   307, 17753,  1833,    13,   526,   423,    30,   921,  1194,\n",
            "          326,    13,   262, 17753,   423,   423,    13,   423,    13,    13,\n",
            "           13], device='cuda:0')\n",
            "topk scores31 tensor([0.9998, 0.9832, 0.7681, 0.7667, 0.4758, 0.3578, 0.3569, 0.3405, 0.3225,\n",
            "        0.3158, 0.3157, 0.3133, 0.2651, 0.2622, 0.2480, 0.2349, 0.2134, 0.2089,\n",
            "        0.2075, 0.2037, 0.2007, 0.2003, 0.1811, 0.1775, 0.1440, 0.1402, 0.1323,\n",
            "        0.1295, 0.1293, 0.1192, 0.1170, 0.1093, 0.1043, 0.1021, 0.1001, 0.0998,\n",
            "        0.0938, 0.0933, 0.0922, 0.0891, 0.0891], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "ppppp tensor([[6, 9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.189654742026425\n",
            "ppppp tensor([[9, 7]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.0943445622221\n",
            "ppppp tensor([[9, 7]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.0943445622221\n",
            "ppppp tensor([[ 5, 10]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.248495242049359\n",
            "ppppp tensor([[8, 9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.248495242049359\n",
            "ppppp tensor([[2, 9]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.060443010546419\n",
            "ppppp tensor([[4, 7]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.912023005428146\n",
            "ppppp tensor([[ 7, 10]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 4.30406509320417\n",
            "ppppp tensor([[9, 5]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.871201010907891\n",
            "ppppp tensor([[9, 5]], device='cuda:0')\n",
            "ppppp tensor([[2, 6]], device='cuda:0')\n",
            "ppppp 3.871201010907891\n",
            "<|endoftext|>Odd Future is the only way that you can live it, nigga<|endoftext|>\n",
            "<|endoftext|>Odd Future is the only way that I can get it back to you, nigga<|endoftext|>\n",
            "<|endoftext|>Odd Future is the only thing that I can do for you to see through it, nigga<|endoftext|>\n",
            "<|endoftext|>\"I don't want you to know that I'm not a racist, nigga<|endoftext|>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Lyrics (nophoneme_Pretrained_Bert2GPT2)",
      "provenance": [],
      "background_execution": "on",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05c24692ad814124a4b9e4220d56d282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31d0b3f05091410c8bb9c80398572f09",
              "IPY_MODEL_19477efc6acf47798a38b16a051ea7ad",
              "IPY_MODEL_1a849eb139284f489ccf6ae47c48f04b"
            ],
            "layout": "IPY_MODEL_0af31a7196bb4f8a86143254e3db4410"
          }
        },
        "31d0b3f05091410c8bb9c80398572f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb5c71ada824451d877cfb36dfde1884",
            "placeholder": "​",
            "style": "IPY_MODEL_d73f93a9c7b64c23a632c9c0693bea95",
            "value": "100%"
          }
        },
        "19477efc6acf47798a38b16a051ea7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_754b32c5f52c4ef4b371823fe7978ec1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a10b8f339aa74beaa652ecb3b64aa33e",
            "value": 1
          }
        },
        "1a849eb139284f489ccf6ae47c48f04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93c25f025e4a495caefcb9ec70fc9537",
            "placeholder": "​",
            "style": "IPY_MODEL_029436f4ae634ee9a66d5cad6ded90cc",
            "value": " 1/1 [00:01&lt;00:00,  1.29s/it]"
          }
        },
        "0af31a7196bb4f8a86143254e3db4410": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb5c71ada824451d877cfb36dfde1884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d73f93a9c7b64c23a632c9c0693bea95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "754b32c5f52c4ef4b371823fe7978ec1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a10b8f339aa74beaa652ecb3b64aa33e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93c25f025e4a495caefcb9ec70fc9537": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "029436f4ae634ee9a66d5cad6ded90cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "116d9d65860240bb8ee48721894541fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c57ff0763b9e47f18ed3d8e6d5a2c17d",
              "IPY_MODEL_075d01101b5d4f1a8047b5fcbaddae73",
              "IPY_MODEL_bb4853d1be2e48bf95fb1b9f6249b598"
            ],
            "layout": "IPY_MODEL_8deee92328654395a460304bb74d1d44"
          }
        },
        "c57ff0763b9e47f18ed3d8e6d5a2c17d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_704be0a3ca0847c2a05a5827a0541315",
            "placeholder": "​",
            "style": "IPY_MODEL_d4d3aaf8e00c40d0a9b41d7a769d7d0f",
            "value": "100%"
          }
        },
        "075d01101b5d4f1a8047b5fcbaddae73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22f1b1258a66481fa00921a023b8a9b5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d38dc8a855d8476dbf2573d7e89052d8",
            "value": 1
          }
        },
        "bb4853d1be2e48bf95fb1b9f6249b598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb067beed977416e813fdd7b05b54a5b",
            "placeholder": "​",
            "style": "IPY_MODEL_79804d9d9e2344d58bdbd53ba50f4919",
            "value": " 1/1 [00:00&lt;00:00,  1.46it/s]"
          }
        },
        "8deee92328654395a460304bb74d1d44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "704be0a3ca0847c2a05a5827a0541315": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d3aaf8e00c40d0a9b41d7a769d7d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22f1b1258a66481fa00921a023b8a9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d38dc8a855d8476dbf2573d7e89052d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb067beed977416e813fdd7b05b54a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79804d9d9e2344d58bdbd53ba50f4919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e69348bab9ec4130abaa7eecdea663a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d62476f229ef4a69bb721f1a0b318a9e",
              "IPY_MODEL_924e24f5da33414ea1f217e2859c3e36",
              "IPY_MODEL_8ec0dfe2e43340e5aa494fbc2f698c2e"
            ],
            "layout": "IPY_MODEL_1d8aee1a81124f2e9a8de9dbf227ebaf"
          }
        },
        "d62476f229ef4a69bb721f1a0b318a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d937f2878047d68d8ba6f2050e9d6a",
            "placeholder": "​",
            "style": "IPY_MODEL_0fd169cbb4124204bb5368eec32d31c6",
            "value": "100%"
          }
        },
        "924e24f5da33414ea1f217e2859c3e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9f2515923e6414c872beae3571b202e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32daeea075404702988492fbc054d111",
            "value": 1
          }
        },
        "8ec0dfe2e43340e5aa494fbc2f698c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36a517552ff14fd5b0e8fb0f75726278",
            "placeholder": "​",
            "style": "IPY_MODEL_f27ea7edd3e14660b94153ae5f13deb7",
            "value": " 1/1 [00:00&lt;00:00,  2.67it/s]"
          }
        },
        "1d8aee1a81124f2e9a8de9dbf227ebaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36d937f2878047d68d8ba6f2050e9d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fd169cbb4124204bb5368eec32d31c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9f2515923e6414c872beae3571b202e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32daeea075404702988492fbc054d111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36a517552ff14fd5b0e8fb0f75726278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f27ea7edd3e14660b94153ae5f13deb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}